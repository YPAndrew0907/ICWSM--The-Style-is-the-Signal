{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b13d5e",
   "metadata": {},
   "source": [
    "# MBFC URL-masked TAG2RISK tag-field ablations (v6)\n",
    "\n",
    "This notebook runs **tag-field ablations** for the TAG2RISK (tag-only) logistic regression model, using the **same URL-masked (domain-disjoint) evaluation protocol as `mbfc_url_masked_logreg_v6`**.\n",
    "\n",
    "We keep everything fixed (splits, hyperparameter search, threshold tuning, metrics) and only change which **tag fields** are included in the multi-hot vector:\n",
    "\n",
    "- `Theme` (`theme=...`)\n",
    "- `Claim/Framing` (`claim=...`)\n",
    "- `CTA` (`cta=...`)\n",
    "- `Evidence` (`evid=...`)\n",
    "\n",
    "Note: this matches **v6** behavior (drops `Link/URL` evidence tags from the style vector to avoid URL leakage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    brier_score_loss,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "# Resolve dataset path by searching upward from CWD.\n",
    "def _resolve_data_path(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        candidate = (\n",
    "            p\n",
    "            / \"mbfc_channel_masked_logreg_fullpackage_v2_MBFC_C\"\n",
    "            / \"MBFC \"\n",
    "            / \"mega_samples_dedup_qwen_mbfc.csv\"\n",
    "        )\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate mega_samples_dedup_qwen_mbfc.csv from current working directory\"\n",
    "    )\n",
    "\n",
    "DATA_PATH = _resolve_data_path(Path.cwd().resolve())\n",
    "print({\"data_path\": str(DATA_PATH)})\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "\n",
    "# Fix legacy header quirk in the v2 MBFC CSV where the first column name was mangled.\n",
    "if \"source\" not in df.columns:\n",
    "    first_col = df.columns[0]\n",
    "    df = df.rename(columns={first_col: \"source\"})\n",
    "\n",
    "# Strip URLs from message text at load time so the pipeline never sees raw URLs.\n",
    "df[\"message\"] = (\n",
    "    df[\"message\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"(https?://|http://|www\\.[^\\s]*|t\\.me/[^\\s]*)\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "# Drop rows whose message becomes empty after URL stripping.\n",
    "df = df[df[\"message\"] != \"\"].copy()\n",
    "\n",
    "# Use MBFC-derived binary risk label as y (1 = higher-risk / lower-credibility)\n",
    "df = df.dropna(subset=[\"risk_label\"]).copy()\n",
    "df[\"y\"] = df[\"risk_label\"].astype(int)\n",
    "\n",
    "# For URL-masked evaluation we only keep rows that have a resolved normalized_domain.\n",
    "df_eval = df.dropna(subset=[\"normalized_domain\"]).copy()\n",
    "print(\n",
    "    {\n",
    "        \"rows\": int(len(df)),\n",
    "        \"eval_rows\": int(len(df_eval)),\n",
    "        \"unique_domains\": int(df_eval[\"normalized_domain\"].nunique()),\n",
    "        \"pos_rate\": float(df_eval[\"y\"].mean()),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "# v6: remove Qwen 'Link/URL' labels from style features\n",
    "DROP_LINK_URL_LABEL = True\n",
    "_LINK_URL_LABEL_NORM = \"link/url\"\n",
    "\n",
    "\n",
    "def tokenize_multi(value: object) -> List[str]:\n",
    "    \"\"\"Split a comma- or plus-separated label string into atomic pieces.\"\"\"\n",
    "    if not isinstance(value, str):\n",
    "        return []\n",
    "    value = value.replace(\"+\", \",\")\n",
    "    parts = [part.strip() for part in value.split(\",\") if part.strip()]\n",
    "    if not DROP_LINK_URL_LABEL:\n",
    "        return parts\n",
    "    # Normalize by lowercasing and removing whitespace so 'Link / URL' matches too.\n",
    "    return [p for p in parts if \"\".join(p.lower().split()) != _LINK_URL_LABEL_NORM]\n",
    "\n",
    "\n",
    "THEME_BUCKETS = [\n",
    "    \"Finance/Crypto\",\n",
    "    \"Public health & medicine\",\n",
    "    \"Politics\",\n",
    "    \"Lifestyle & well-being\",\n",
    "    \"Crime & public safety\",\n",
    "    \"Gaming/Gambling\",\n",
    "    \"News/Information\",\n",
    "    \"Sports\",\n",
    "    \"Technology\",\n",
    "    \"Conversation/Chat/Other\",\n",
    "    \"Other theme\",\n",
    "]\n",
    "\n",
    "CLAIM_BUCKETS = [\n",
    "    \"Verifiable factual statement\",\n",
    "    \"Rumour / unverified report\",\n",
    "    \"Announcement\",\n",
    "    \"Opinion / subjective statement\",\n",
    "    \"Misleading context / cherry-picking\",\n",
    "    \"Promotional hype / exaggerated profit guarantee\",\n",
    "    \"Emotional appeal / fear-mongering\",\n",
    "    \"Scarcity/FOMO tactic\",\n",
    "    \"Statistics\",\n",
    "    \"Other claim type\",\n",
    "    \"No substantive claim\",\n",
    "    \"Fake content\",\n",
    "    \"Speculative forecast / prediction\",\n",
    "    \"None / assertion only\",\n",
    "]\n",
    "\n",
    "CTA_BUCKETS = [\n",
    "    \"Visit external link / watch video\",\n",
    "    \"Engage/Ask questions\",\n",
    "    \"Join/Subscribe\",\n",
    "    \"Buy / invest / donate\",\n",
    "    \"Attend event / livestream\",\n",
    "    \"Share / repost / like\",\n",
    "    \"No CTA\",\n",
    "    \"Other CTA\",\n",
    "]\n",
    "\n",
    "EVID_BUCKETS = [\n",
    "    \"Link/URL\",\n",
    "    \"Statistics\",\n",
    "    \"Quotes/Testimony\",\n",
    "    \"Chart / price graph / TA diagram\",\n",
    "    \"Other (Evidence)\",\n",
    "    \"None / assertion only\",\n",
    "]\n",
    "\n",
    "\n",
    "def _norm_theme(raw: object) -> Optional[str]:\n",
    "    if not isinstance(raw, str):\n",
    "        return None\n",
    "    t = raw.strip()\n",
    "    if not t:\n",
    "        return None\n",
    "    tl = t.lower()\n",
    "\n",
    "    if t in THEME_BUCKETS:\n",
    "        return t\n",
    "\n",
    "    if any(\n",
    "        k in tl\n",
    "        for k in [\n",
    "            \"crypto\",\n",
    "            \"token\",\n",
    "            \"coin\",\n",
    "            \"airdrop\",\n",
    "            \"ido\",\n",
    "            \"staking\",\n",
    "            \"defi\",\n",
    "            \"exchange\",\n",
    "            \"market\",\n",
    "            \"finance\",\n",
    "            \"econom\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Finance/Crypto\"\n",
    "\n",
    "    if any(\n",
    "        k in tl\n",
    "        for k in [\n",
    "            \"health\",\n",
    "            \"covid\",\n",
    "            \"vaccine\",\n",
    "            \"vaccination\",\n",
    "            \"medicine\",\n",
    "            \"medical\",\n",
    "            \"clinical\",\n",
    "            \"disease\",\n",
    "            \"pandemic\",\n",
    "            \"public health\",\n",
    "            \"hospital\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Public health & medicine\"\n",
    "\n",
    "    if any(\n",
    "        k in tl\n",
    "        for k in [\n",
    "            \"politic\",\n",
    "            \"election\",\n",
    "            \"parliament\",\n",
    "            \"congress\",\n",
    "            \"senate\",\n",
    "            \"government\",\n",
    "            \"president\",\n",
    "            \"minister\",\n",
    "            \"policy\",\n",
    "            \"war\",\n",
    "            \"conflict\",\n",
    "            \"ukraine\",\n",
    "            \"russia\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Politics\"\n",
    "\n",
    "    if any(\n",
    "        k in tl\n",
    "        for k in [\n",
    "            \"crime\",\n",
    "            \"criminal\",\n",
    "            \"terror\",\n",
    "            \"shooting\",\n",
    "            \"police\",\n",
    "            \"public safety\",\n",
    "            \"fraud\",\n",
    "            \"scam\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Crime & public safety\"\n",
    "\n",
    "    if any(k in tl for k in [\"gaming\", \"gambling\", \"casino\", \"betting\", \"lottery\", \"poker\"]):\n",
    "        return \"Gaming/Gambling\"\n",
    "\n",
    "    if any(k in tl for k in [\"sport\", \"football\", \"soccer\", \"basketball\", \"tennis\", \"nba\", \"nfl\"]):\n",
    "        return \"Sports\"\n",
    "\n",
    "    if any(\n",
    "        k in tl\n",
    "        for k in [\n",
    "            \"technology\",\n",
    "            \"tech\",\n",
    "            \"software\",\n",
    "            \"app \",\n",
    "            \"platform\",\n",
    "            \"ai \",\n",
    "            \" a.i.\",\n",
    "            \"machine learning\",\n",
    "            \"blockchain\",\n",
    "            \"internet\",\n",
    "            \"social media\",\n",
    "            \"algorithm\",\n",
    "            \"science\",\n",
    "            \"research\",\n",
    "            \"study\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Technology\"\n",
    "\n",
    "    if any(\n",
    "        k in tl\n",
    "        for k in [\n",
    "            \"lifestyle\",\n",
    "            \"well-being\",\n",
    "            \"wellbeing\",\n",
    "            \"culture\",\n",
    "            \"entertainment\",\n",
    "            \"media\",\n",
    "            \"celebrity\",\n",
    "            \"social issues\",\n",
    "            \"society\",\n",
    "            \"family\",\n",
    "            \"community\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Lifestyle & well-being\"\n",
    "\n",
    "    if any(k in tl for k in [\"news\", \"headline\", \"breaking\", \"coverage\", \"roundup\", \"update\"]):\n",
    "        return \"News/Information\"\n",
    "\n",
    "    if any(k in tl for k in [\"comment\", \"conversation\", \"chat\", \"q&a\", \"ama\", \"ask me anything\"]):\n",
    "        return \"Conversation/Chat/Other\"\n",
    "\n",
    "    return \"Other theme\"\n",
    "\n",
    "\n",
    "def _norm_claim_labels(raw: object) -> List[str]:\n",
    "    labels = tokenize_multi(raw)\n",
    "    out: List[str] = []\n",
    "    for lbl in labels:\n",
    "        base = lbl.strip()\n",
    "        if not base:\n",
    "            continue\n",
    "        low = base.lower()\n",
    "\n",
    "        if base in CLAIM_BUCKETS:\n",
    "            out.append(base)\n",
    "            continue\n",
    "\n",
    "        if \"verifiable\" in low or \"factual\" in low:\n",
    "            out.append(\"Verifiable factual statement\")\n",
    "        elif \"rumour\" in low or \"unverified\" in low:\n",
    "            out.append(\"Rumour / unverified report\")\n",
    "        elif \"misleading context\" in low or \"cherry\" in low:\n",
    "            out.append(\"Misleading context / cherry-picking\")\n",
    "        elif \"promotional hype\" in low or \"exaggerated profit\" in low:\n",
    "            out.append(\"Promotional hype / exaggerated profit guarantee\")\n",
    "        elif \"emotional appeal\" in low or \"fear-mongering\" in low or \"fear mongering\" in low:\n",
    "            out.append(\"Emotional appeal / fear-mongering\")\n",
    "        elif \"scarcity\" in low or \"fomo\" in low:\n",
    "            out.append(\"Scarcity/FOMO tactic\")\n",
    "        elif \"statistic\" in low:\n",
    "            out.append(\"Statistics\")\n",
    "        elif \"fake content\" in low or \"fabricated\" in low:\n",
    "            out.append(\"Fake content\")\n",
    "        elif \"predict\" in low or \"forecast\" in low:\n",
    "            out.append(\"Speculative forecast / prediction\")\n",
    "        elif \"announcement\" in low:\n",
    "            out.append(\"Announcement\")\n",
    "        elif \"opinion\" in low or \"interpretive\" in low or \"analysis\" in low or \"review\" in low:\n",
    "            out.append(\"Opinion / subjective statement\")\n",
    "        elif \"none / assertion only\" in low or \"assertion only\" in low:\n",
    "            out.append(\"None / assertion only\")\n",
    "        else:\n",
    "            out.append(\"Other claim type\")\n",
    "\n",
    "    seen = set()\n",
    "    result: List[str] = []\n",
    "    for v in out:\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            result.append(v)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _norm_cta_labels(raw: object) -> List[str]:\n",
    "    labels = tokenize_multi(raw)\n",
    "    out: List[str] = []\n",
    "    for lbl in labels:\n",
    "        base = lbl.strip()\n",
    "        if not base:\n",
    "            continue\n",
    "        low = base.lower()\n",
    "\n",
    "        if base in CTA_BUCKETS:\n",
    "            out.append(base)\n",
    "            continue\n",
    "\n",
    "        if base in {\"None\", \"No CTA\"} or \"no cta\" in low:\n",
    "            out.append(\"No CTA\")\n",
    "        elif \"engage\" in low or \"ask\" in low or \"anything\" in low:\n",
    "            out.append(\"Engage/Ask questions\")\n",
    "        elif \"attend\" in low or \"event\" in low or \"livestream\" in low or \"live stream\" in low:\n",
    "            out.append(\"Attend event / livestream\")\n",
    "        elif \"join\" in low or \"subscribe\" in low or \"follow\" in low or \"whitelist\" in low:\n",
    "            out.append(\"Join/Subscribe\")\n",
    "        elif \"buy\" in low or \"invest\" in low or \"donate\" in low or \"stake\" in low or \"swap\" in low:\n",
    "            out.append(\"Buy / invest / donate\")\n",
    "        elif \"share\" in low or \"repost\" in low or \"like\" in low:\n",
    "            out.append(\"Share / repost / like\")\n",
    "        elif (\n",
    "            \"visit\" in low\n",
    "            or \"read\" in low\n",
    "            or \"watch\" in low\n",
    "            or \"link\" in low\n",
    "            or \"website\" in low\n",
    "            or \"check\" in low\n",
    "            or \"view charts\" in low\n",
    "        ):\n",
    "            out.append(\"Visit external link / watch video\")\n",
    "        else:\n",
    "            out.append(\"Other CTA\")\n",
    "\n",
    "    seen = set()\n",
    "    result: List[str] = []\n",
    "    for v in out:\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            result.append(v)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _norm_evidence_labels(raw: object) -> List[str]:\n",
    "    labels = tokenize_multi(raw)\n",
    "    out: List[str] = []\n",
    "    for lbl in labels:\n",
    "        base = lbl.strip()\n",
    "        if not base:\n",
    "            continue\n",
    "        low = base.lower()\n",
    "\n",
    "        if base in EVID_BUCKETS:\n",
    "            if base != \"Link/URL\":\n",
    "                out.append(base)\n",
    "            continue\n",
    "\n",
    "        if \"link/url\" in low or \"link\" in low or \"url\" in low:\n",
    "            # v6: drop Link/URL evidence tag entirely\n",
    "            continue\n",
    "        elif \"statistic\" in low:\n",
    "            out.append(\"Statistics\")\n",
    "        elif \"quote\" in low or \"testimony\" in low:\n",
    "            out.append(\"Quotes/Testimony\")\n",
    "        elif \"chart\" in low or \"graph\" in low or \"diagram\" in low:\n",
    "            out.append(\"Chart / price graph / TA diagram\")\n",
    "        elif \"none / assertion only\" in low or \"assertion only\" in low:\n",
    "            out.append(\"None / assertion only\")\n",
    "        else:\n",
    "            out.append(\"Other (Evidence)\")\n",
    "\n",
    "    seen = set()\n",
    "    result: List[str] = []\n",
    "    for v in out:\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            result.append(v)\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_style_tokens(row: pd.Series, include_fields: set[str]) -> List[str]:\n",
    "    tokens: List[str] = []\n",
    "\n",
    "    if \"theme\" in include_fields:\n",
    "        theme = _norm_theme(row.get(\"theme\"))\n",
    "        if theme is not None:\n",
    "            tokens.append(f\"theme={theme}\")\n",
    "\n",
    "    if \"claim\" in include_fields:\n",
    "        for label in _norm_claim_labels(row.get(\"claim_types\")):\n",
    "            tokens.append(f\"claim={label}\")\n",
    "\n",
    "    if \"cta\" in include_fields:\n",
    "        for label in _norm_cta_labels(row.get(\"ctas\")):\n",
    "            tokens.append(f\"cta={label}\")\n",
    "\n",
    "    if \"evidence\" in include_fields:\n",
    "        for label in _norm_evidence_labels(row.get(\"evidence\")):\n",
    "            tokens.append(f\"evid={label}\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_style_features_for_fields(train_df, val_df, test_df, include_fields: set[str]):\n",
    "    train_tokens = train_df.apply(lambda r: build_style_tokens(r, include_fields), axis=1)\n",
    "    val_tokens = val_df.apply(lambda r: build_style_tokens(r, include_fields), axis=1)\n",
    "    test_tokens = test_df.apply(lambda r: build_style_tokens(r, include_fields), axis=1)\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    X_train = mlb.fit_transform(train_tokens)\n",
    "    X_val = mlb.transform(val_tokens)\n",
    "    X_test = mlb.transform(test_tokens)\n",
    "    return X_train, X_val, X_test, mlb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bf859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/eval utilities (copied from v6)\n",
    "\n",
    "RANDOM_STATES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "LR_GRID = [0.001, 0.003, 0.01]\n",
    "THRESH_GRID = [round(t, 2) for t in np.linspace(0.05, 0.95, 19)]\n",
    "PRIMARY_METRIC = \"macro_f1\"\n",
    "\n",
    "\n",
    "class ManualLogisticRegression:\n",
    "    \"\"\"Simple binary logistic regression implemented with gradient descent.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float = 0.1,\n",
    "        max_iter: int = 200,\n",
    "        C: float = 1.0,\n",
    "        class_weight=None,\n",
    "        tol: float | None = 1e-4,\n",
    "        verbose: bool = False,\n",
    "        n_jobs=None,  # kept for API compatibility; not used\n",
    "    ):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.class_weight = class_weight\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def _prepare_X(self, X):\n",
    "        if sparse.issparse(X):\n",
    "            return X.tocsr()\n",
    "        return np.asarray(X, dtype=float)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self._prepare_X(X)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.coef_ = np.zeros(n_features, dtype=float)\n",
    "        self.intercept_ = 0.0\n",
    "        self.loss_history_ = []\n",
    "\n",
    "        if self.class_weight is None:\n",
    "            sample_weights = np.ones_like(y)\n",
    "        elif self.class_weight == \"balanced\":\n",
    "            classes, counts = np.unique(y, return_counts=True)\n",
    "            n_classes = len(classes)\n",
    "            class_weight_values = {\n",
    "                cls: n_samples / (n_classes * count) for cls, count in zip(classes, counts)\n",
    "            }\n",
    "            sample_weights = np.array([class_weight_values[yi] for yi in y], dtype=float)\n",
    "        elif isinstance(self.class_weight, dict):\n",
    "            sample_weights = np.array([self.class_weight.get(yi, 1.0) for yi in y], dtype=float)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported class_weight specification\")\n",
    "\n",
    "        prev_loss = None\n",
    "        for i in range(self.max_iter):\n",
    "            z = X.dot(self.coef_) + self.intercept_\n",
    "            p = expit(z)\n",
    "            residual = (p - y) * sample_weights\n",
    "\n",
    "            if sparse.issparse(X):\n",
    "                grad_w = X.T.dot(residual) / n_samples\n",
    "            else:\n",
    "                grad_w = (X.T @ residual) / n_samples\n",
    "            grad_w += self.coef_ / (self.C * n_samples)\n",
    "            grad_b = residual.mean()\n",
    "\n",
    "            self.coef_ -= self.lr * grad_w\n",
    "            self.intercept_ -= self.lr * grad_b\n",
    "\n",
    "            if self.tol is not None and (i % 10 == 0 or i == self.max_iter - 1):\n",
    "                z = X.dot(self.coef_) + self.intercept_\n",
    "                p = expit(z)\n",
    "                eps = 1e-15\n",
    "                loss_vec = (-(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))) * sample_weights\n",
    "                loss = loss_vec.mean() + 0.5 * np.sum(self.coef_**2) / (self.C * n_samples)\n",
    "                self.loss_history_.append(float(loss))\n",
    "                if self.verbose:\n",
    "                    print(f\"Iter {i}: loss={loss:.6f}\")\n",
    "                if prev_loss is not None and abs(prev_loss - loss) < self.tol:\n",
    "                    break\n",
    "                prev_loss = loss\n",
    "\n",
    "        self.classes_ = np.array([0.0, 1.0])\n",
    "        return self\n",
    "\n",
    "    def _decision_function(self, X):\n",
    "        X = self._prepare_X(X)\n",
    "        return X.dot(self.coef_) + self.intercept_\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = self._decision_function(X)\n",
    "        p_pos = expit(z)\n",
    "        return np.vstack([1 - p_pos, p_pos]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "def _stack_features(a, b):\n",
    "    if sparse.issparse(a):\n",
    "        return sparse.vstack([a, b])\n",
    "    return np.vstack([a, b])\n",
    "\n",
    "\n",
    "def sweep_thresholds(y_true, proba):\n",
    "    rows = []\n",
    "    for t in THRESH_GRID:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"threshold\": float(t),\n",
    "                \"macro_f1\": f1_score(y_true, pred, average=\"macro\"),\n",
    "                \"macro_recall\": recall_score(y_true, pred, average=\"macro\"),\n",
    "                \"recall_pos\": recall_score(y_true, pred, pos_label=1),\n",
    "            }\n",
    "        )\n",
    "    best = max(rows, key=lambda r: r[PRIMARY_METRIC])\n",
    "    return best\n",
    "\n",
    "\n",
    "def expected_calibration_error(y_true, y_proba, n_bins=10):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    indices = np.digitize(y_proba, bins) - 1\n",
    "    ece = 0.0\n",
    "    n = len(y_true)\n",
    "    for b in range(n_bins):\n",
    "        mask = indices == b\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        p_bin = y_proba[mask].mean()\n",
    "        y_bin = y_true[mask].mean()\n",
    "        weight = mask.sum() / n\n",
    "        ece += weight * abs(p_bin - y_bin)\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "def fit_with_val_search(X_train, y_train, X_val, y_val, X_test, y_test, lr_grid=None):\n",
    "    lr_grid = lr_grid or LR_GRID\n",
    "    best = None\n",
    "    for lr in lr_grid:\n",
    "        clf = ManualLogisticRegression(\n",
    "            max_iter=1000,\n",
    "            lr=lr,\n",
    "            C=1.0,\n",
    "            class_weight=\"balanced\",\n",
    "            tol=None,\n",
    "            n_jobs=-1,\n",
    "            verbose=False,\n",
    "        )\n",
    "        clf.fit(X_train, y_train)\n",
    "        val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "        best_thr = sweep_thresholds(y_val, val_proba)\n",
    "        candidate = {\n",
    "            \"lr\": lr,\n",
    "            \"val_threshold\": best_thr[\"threshold\"],\n",
    "            \"val_macro_f1\": best_thr[\"macro_f1\"],\n",
    "            \"val_macro_recall\": best_thr[\"macro_recall\"],\n",
    "            \"val_recall_pos\": best_thr[\"recall_pos\"],\n",
    "            \"primary_score\": best_thr[PRIMARY_METRIC],\n",
    "        }\n",
    "        if best is None or candidate[\"primary_score\"] > best[\"primary_score\"]:\n",
    "            best = candidate\n",
    "\n",
    "    X_trainval = _stack_features(X_train, X_val)\n",
    "    y_trainval = np.concatenate([y_train, y_val])\n",
    "    final_clf = ManualLogisticRegression(\n",
    "        max_iter=1000,\n",
    "        lr=best[\"lr\"],\n",
    "        C=1.0,\n",
    "        class_weight=\"balanced\",\n",
    "        tol=None,\n",
    "        n_jobs=-1,\n",
    "        verbose=False,\n",
    "    )\n",
    "    final_clf.fit(X_trainval, y_trainval)\n",
    "    test_proba = final_clf.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_proba >= best[\"val_threshold\"]).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"best_lr\": float(best[\"lr\"]),\n",
    "        \"threshold\": float(best[\"val_threshold\"]),\n",
    "        \"test_macro_f1\": float(f1_score(y_test, test_pred, average=\"macro\")),\n",
    "        \"test_macro_recall\": float(recall_score(y_test, test_pred, average=\"macro\")),\n",
    "        \"test_recall_pos\": float(recall_score(y_test, test_pred, pos_label=1)),\n",
    "        \"test_roc_auc\": float(roc_auc_score(y_test, test_proba)),\n",
    "        \"test_accuracy\": float(accuracy_score(y_test, test_pred)),\n",
    "        \"test_brier\": float(brier_score_loss(y_test, test_proba)),\n",
    "        \"test_ece\": float(expected_calibration_error(y_test, test_proba, n_bins=10)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tag-field ablations (tag-only model)\n",
    "\n",
    "ABLATIONS: dict[str, set[str]] = {\n",
    "    \"tags_full\": {\"theme\", \"claim\", \"cta\", \"evidence\"},\n",
    "    \"tags_theme_only\": {\"theme\"},\n",
    "    \"tags_claim_only\": {\"claim\"},\n",
    "    \"tags_cta_only\": {\"cta\"},\n",
    "    \"tags_evidence_only\": {\"evidence\"},\n",
    "    \"tags_style_only_no_theme\": {\"claim\", \"cta\", \"evidence\"},\n",
    "    \"tags_drop_claim\": {\"theme\", \"cta\", \"evidence\"},\n",
    "    \"tags_drop_cta\": {\"theme\", \"claim\", \"evidence\"},\n",
    "    \"tags_drop_evidence\": {\"theme\", \"claim\", \"cta\"},\n",
    "}\n",
    "\n",
    "results_dir = Path(\"mbfc_url_masked_logreg_tag_field_ablations_results_v2\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "rows = []\n",
    "for seed in RANDOM_STATES:\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "    trainval_idx, test_idx = next(gss.split(df_eval, df_eval[\"y\"], df_eval[\"normalized_domain\"]))\n",
    "    df_trainval = df_eval.iloc[trainval_idx].copy()\n",
    "    df_test_split = df_eval.iloc[test_idx].copy()\n",
    "\n",
    "    df_train_split, df_val_split = train_test_split(\n",
    "        df_trainval, test_size=0.125, random_state=100 + seed, stratify=df_trainval[\"y\"]\n",
    "    )\n",
    "    y_train = df_train_split[\"y\"].values\n",
    "    y_val = df_val_split[\"y\"].values\n",
    "    y_test = df_test_split[\"y\"].values\n",
    "\n",
    "    for ablation_name, include_fields in ABLATIONS.items():\n",
    "        X_train, X_val, X_test, mlb = build_style_features_for_fields(\n",
    "            df_train_split, df_val_split, df_test_split, include_fields\n",
    "        )\n",
    "        metrics = fit_with_val_search(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "        metrics.update(\n",
    "            {\n",
    "                \"model\": ablation_name,\n",
    "                \"split_seed\": int(seed),\n",
    "                \"n_features\": int(len(mlb.classes_)),\n",
    "                \"fields\": \"+\".join(sorted(include_fields)),\n",
    "            }\n",
    "        )\n",
    "        rows.append(metrics)\n",
    "        print({\"seed\": int(seed), \"model\": ablation_name, \"roc_auc\": round(metrics[\"test_roc_auc\"], 4)})\n",
    "\n",
    "ablation_df = pd.DataFrame(rows)\n",
    "ablation_csv = results_dir / \"tag_field_ablation_metrics.csv\"\n",
    "ablation_df.to_csv(ablation_csv, index=False)\n",
    "print({\"ablation_metrics_csv\": str(ablation_csv)})\n",
    "\n",
    "summary = (\n",
    "    ablation_df.groupby([\"model\", \"fields\", \"n_features\"])[\n",
    "        [\n",
    "            \"test_accuracy\",\n",
    "            \"test_roc_auc\",\n",
    "            \"test_macro_f1\",\n",
    "            \"test_macro_recall\",\n",
    "            \"test_recall_pos\",\n",
    "            \"test_brier\",\n",
    "            \"test_ece\",\n",
    "        ]\n",
    "    ]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .round(4)\n",
    ")\n",
    "summary_csv = results_dir / \"tag_field_ablation_summary.csv\"\n",
    "summary.to_csv(summary_csv)\n",
    "print({\"ablation_summary_csv\": str(summary_csv)})\n",
    "print(\"\\nTag-field ablation summary (meanÂ±std over seeds):\")\n",
    "display(summary)\n",
    "\n",
    "# Also save a flat (tidy) summary table.\n",
    "metrics_for_tables = [\n",
    "    \"test_accuracy\",\n",
    "    \"test_roc_auc\",\n",
    "    \"test_macro_f1\",\n",
    "    \"test_macro_recall\",\n",
    "    \"test_recall_pos\",\n",
    "    \"test_brier\",\n",
    "    \"test_ece\",\n",
    "]\n",
    "agg_spec = {}\n",
    "for col in metrics_for_tables:\n",
    "    agg_spec[f\"{col}_mean\"] = (col, \"mean\")\n",
    "    agg_spec[f\"{col}_std\"] = (col, \"std\")\n",
    "\n",
    "summary_flat = (\n",
    "    ablation_df.groupby([\"model\", \"fields\", \"n_features\"]).agg(**agg_spec).reset_index().round(4)\n",
    ")\n",
    "summary_flat_csv = results_dir / \"tag_field_ablation_summary_flat.csv\"\n",
    "summary_flat.to_csv(summary_flat_csv, index=False)\n",
    "print({\"ablation_summary_flat_csv\": str(summary_flat_csv)})\n",
    "\n",
    "\n",
    "# Load v6 TF-IDF and Combined baselines for side-by-side comparison.\n",
    "def _resolve_v6_metrics_path(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        candidate = p / \"mbfc_url_masked_logreg_results_v6\" / \"url_masked_val_tuned_metrics.csv\"\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate mbfc_url_masked_logreg_results_v6/url_masked_val_tuned_metrics.csv\"\n",
    "    )\n",
    "\n",
    "v6_metrics_path = _resolve_v6_metrics_path(Path.cwd().resolve())\n",
    "v6_df = pd.read_csv(v6_metrics_path)\n",
    "baseline_df = v6_df[v6_df[\"model\"].isin([\"tfidf\", \"combined\"])].copy()\n",
    "baseline_df[\"fields\"] = \"\"\n",
    "baseline_df[\"n_features\"] = np.nan\n",
    "\n",
    "all_df = pd.concat([baseline_df, ablation_df], ignore_index=True)\n",
    "all_metrics = [\n",
    "    \"test_accuracy\",\n",
    "    \"test_roc_auc\",\n",
    "    \"test_macro_f1\",\n",
    "    \"test_macro_recall\",\n",
    "    \"test_brier\",\n",
    "    \"test_ece\",\n",
    "]\n",
    "\n",
    "all_agg_spec = {}\n",
    "for col in all_metrics:\n",
    "    all_agg_spec[f\"{col}_mean\"] = (col, \"mean\")\n",
    "    all_agg_spec[f\"{col}_std\"] = (col, \"std\")\n",
    "\n",
    "all_summary_flat = all_df.groupby([\"model\"]).agg(**all_agg_spec).reset_index().round(4)\n",
    "all_summary_flat_csv = results_dir / \"all_models_summary_flat.csv\"\n",
    "all_summary_flat.to_csv(all_summary_flat_csv, index=False)\n",
    "print({\"all_models_summary_flat_csv\": str(all_summary_flat_csv)})\n",
    "\n",
    "\n",
    "# Marker-style figures (like TF-IDF vs Style vs TF-IDF+Style example)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_metrics = [\n",
    "    \"test_accuracy\",\n",
    "    \"test_roc_auc\",\n",
    "    \"test_macro_f1\",\n",
    "    \"test_macro_recall\",\n",
    "]\n",
    "means = all_df.groupby(\"model\")[plot_metrics].mean()\n",
    "stds = all_df.groupby(\"model\")[plot_metrics].std()\n",
    "\n",
    "METRIC_SPECS = [\n",
    "    (\"Accuracy\", \"test_accuracy\", \"o\"),\n",
    "    (\"ROC-AUC\", \"test_roc_auc\", \"s\"),\n",
    "    (\"F1 (macro)\", \"test_macro_f1\", \"^\"),\n",
    "    (\"Recall (macro)\", \"test_macro_recall\", \"D\"),\n",
    "]\n",
    "\n",
    "\n",
    "def plot_marker_figure(\n",
    "    model_order: list[str],\n",
    "    model_labels: list[str],\n",
    "    out_png: Path,\n",
    "    out_pdf: Path | None = None,\n",
    "    title: str | None = None,\n",
    "    figsize=(6.0, 3.0),\n",
    "    xtick_rotation: float = 0,\n",
    "):\n",
    "    x = np.arange(len(model_order), dtype=float)\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=300)\n",
    "\n",
    "    y_all = []\n",
    "    for _, col, _ in METRIC_SPECS:\n",
    "        y_all.extend([float(means.loc[m, col]) for m in model_order])\n",
    "    y_min = max(0.0, min(y_all) - 0.06)\n",
    "    y_max = min(1.0, max(y_all) + 0.06)\n",
    "\n",
    "    for label, col, marker in METRIC_SPECS:\n",
    "        y = [float(means.loc[m, col]) for m in model_order]\n",
    "        ax.scatter(\n",
    "            x,\n",
    "            y,\n",
    "            marker=marker,\n",
    "            s=70,\n",
    "            facecolors=\"none\",\n",
    "            edgecolors=\"black\",\n",
    "            linewidths=1.2,\n",
    "            label=label,\n",
    "            zorder=3,\n",
    "        )\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ha = \"right\" if xtick_rotation else \"center\"\n",
    "    ax.set_xticklabels(model_labels, rotation=xtick_rotation, ha=ha)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.grid(axis=\"y\", alpha=0.25, zorder=0)\n",
    "    ax.legend(loc=\"upper left\", frameon=False)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_png, dpi=300)\n",
    "    if out_pdf is not None:\n",
    "        fig.savefig(out_pdf)\n",
    "    plt.close(fig)\n",
    "    print({\"figure_png\": str(out_png), \"figure_pdf\": str(out_pdf) if out_pdf else None})\n",
    "\n",
    "\n",
    "# 1) Baseline plot: TF-IDF vs Style vs TF-IDF+Style\n",
    "baseline_order = [\"tfidf\", \"tags_full\", \"combined\"]\n",
    "baseline_order = [m for m in baseline_order if m in means.index]\n",
    "baseline_label_map = {\"tfidf\": \"TF-IDF\", \"tags_full\": \"Style\", \"combined\": \"TF-IDF\\n+Style\"}\n",
    "baseline_labels = [baseline_label_map[m] for m in baseline_order]\n",
    "plot_marker_figure(\n",
    "    baseline_order,\n",
    "    baseline_labels,\n",
    "    out_png=results_dir / \"marker_metrics_tfidf_style_combined.png\",\n",
    "    out_pdf=results_dir / \"marker_metrics_tfidf_style_combined.pdf\",\n",
    "    figsize=(4.6, 2.8),\n",
    ")\n",
    "\n",
    "\n",
    "# 2) Full plot: single-field + drop-one-field ablations (plus TF-IDF and Combined)\n",
    "ablation_order = [\n",
    "    \"tfidf\",\n",
    "    \"tags_theme_only\",\n",
    "    \"tags_claim_only\",\n",
    "    \"tags_cta_only\",\n",
    "    \"tags_evidence_only\",\n",
    "    \"tags_style_only_no_theme\",\n",
    "    \"tags_drop_claim\",\n",
    "    \"tags_drop_cta\",\n",
    "    \"tags_drop_evidence\",\n",
    "    \"tags_full\",\n",
    "    \"combined\",\n",
    "]\n",
    "ablation_order = [m for m in ablation_order if m in means.index]\n",
    "label_map = {\n",
    "    \"tfidf\": \"TF-IDF\",\n",
    "    \"tags_theme_only\": \"Theme only\",\n",
    "    \"tags_claim_only\": \"Claim only\",\n",
    "    \"tags_cta_only\": \"CTA only\",\n",
    "    \"tags_evidence_only\": \"Evidence only\",\n",
    "    \"tags_style_only_no_theme\": \"Style only\\n(no Theme)\",\n",
    "    \"tags_drop_claim\": \"Full - Claim\",\n",
    "    \"tags_drop_cta\": \"Full - CTA\",\n",
    "    \"tags_drop_evidence\": \"Full - Evidence\",\n",
    "    \"tags_full\": \"Full tags\",\n",
    "    \"combined\": \"TF-IDF\\n+Style\",\n",
    "}\n",
    "ablation_labels = [label_map.get(m, m) for m in ablation_order]\n",
    "plot_marker_figure(\n",
    "    ablation_order,\n",
    "    ablation_labels,\n",
    "    out_png=results_dir / \"marker_metrics_all_ablations.png\",\n",
    "    out_pdf=results_dir / \"marker_metrics_all_ablations.pdf\",\n",
    "    figsize=(9.6, 3.0),\n",
    "    xtick_rotation=20,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
