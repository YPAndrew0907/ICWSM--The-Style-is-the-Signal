{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d0f795",
   "metadata": {},
   "source": [
    "# Telegram Message Risk Scoring: URL‑Masked Evaluation (TF‑IDF grid search, Link/URL tags removed)\n",
    "\n",
    "This notebook implements and evaluates a URL‑independent risk model for Telegram messages. Starting from a deduplicated subset of messages with MBFC‑derived risk labels, we construct three feature representations—(1) TF‑IDF over raw message text, (2) compact style vectors built from LLM‑assigned tags (topic, rhetorical stance, calls to action, evidence), and (3) their concatenation—and train ℓ₂‑regularized logistic regression models on each. To avoid leakage across sources, we use a URL‑masked split: training and test sets contain disjoint sets of normalized domains, with the split chosen to match the global positive rate as closely as possible.\n",
    "\n",
    "The notebook then compares these models on a held‑out test set in terms of accuracy, macro‑F1, ROC‑AUC, and calibration, and inspects their behavior across different channel types (e.g., high‑credibility news vs. high‑risk fringe channels). In contrast to the v3 notebook, we perform an explicit grid search over TF‑IDF hyperparameters (n‑gram range and vocabulary size) so that the text‑only baseline is reasonably tuned when comparing against the style and combined models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08fb98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:32:49.061831Z",
     "iopub.status.busy": "2025-12-07T20:32:49.061508Z",
     "iopub.status.idle": "2025-12-07T20:32:49.406137Z",
     "shell.execute_reply": "2025-12-07T20:32:49.405832Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports and paths\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the MBFC-linkable dataset used for Table 2 (domain-disjoint, URL-masked).\n",
    "# Provide via env var, e.g. MBFC_DATA_PATH=/path/to/messages_with_risk_label_urls_removed_nonempty_no_linkurl_evidence.csv\n",
    "DATA_PATH = Path(os.environ.get(\"MBFC_DATA_PATH\", \"data/messages_with_risk_label_urls_removed_nonempty_no_linkurl_evidence.csv\"))\n",
    "print({\"data_path\": str(DATA_PATH)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ac730",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:32:59.033777Z",
     "iopub.status.busy": "2025-12-07T20:32:59.033661Z",
     "iopub.status.idle": "2025-12-07T20:33:00.812491Z",
     "shell.execute_reply": "2025-12-07T20:33:00.812172Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score, recall_score\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy import sparse\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class ManualLogisticRegression:\n",
    "    \"\"\"Simple binary logistic regression implemented with gradient descent.\n",
    "\n",
    "    Supports sparse or dense feature matrices and an interface similar to\n",
    "    sklearn's LogisticRegression for the methods used in this notebook\n",
    "    (fit, predict, predict_proba).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float = 0.1,\n",
    "        max_iter: int = 200,\n",
    "        C: float = 1.0,\n",
    "        class_weight=None,\n",
    "        tol: float = 1e-4,\n",
    "        verbose: bool = False,\n",
    "        n_jobs=None,  # kept for API compatibility; not used\n",
    "    ):\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.class_weight = class_weight\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def _prepare_X(self, X):\n",
    "        if sparse.issparse(X):\n",
    "            return X.tocsr()\n",
    "        return np.asarray(X, dtype=float)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self._prepare_X(X)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.coef_ = np.zeros(n_features, dtype=float)\n",
    "        self.intercept_ = 0.0\n",
    "        # Track loss over iterations when tol is not None\n",
    "        self.loss_history_ = []\n",
    "\n",
    "        # Per-sample weights according to class_weight\n",
    "        if self.class_weight is None:\n",
    "            sample_weights = np.ones_like(y)\n",
    "        elif self.class_weight == \"balanced\":\n",
    "            classes, counts = np.unique(y, return_counts=True)\n",
    "            n_classes = len(classes)\n",
    "            class_weight_values = {\n",
    "                cls: n_samples / (n_classes * count)\n",
    "                for cls, count in zip(classes, counts)\n",
    "            }\n",
    "            sample_weights = np.array([class_weight_values[yi] for yi in y], dtype=float)\n",
    "        elif isinstance(self.class_weight, dict):\n",
    "            sample_weights = np.array([\n",
    "                self.class_weight.get(yi, 1.0) for yi in y\n",
    "            ], dtype=float)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported class_weight specification\")\n",
    "\n",
    "        prev_loss = None\n",
    "        for i in range(self.max_iter):\n",
    "            z = X.dot(self.coef_) + self.intercept_\n",
    "            p = expit(z)\n",
    "            residual = (p - y) * sample_weights\n",
    "\n",
    "            if sparse.issparse(X):\n",
    "                grad_w = X.T.dot(residual) / n_samples\n",
    "            else:\n",
    "                grad_w = X.T @ residual / n_samples\n",
    "            # L2 regularization on weights (not bias)\n",
    "            grad_w += self.coef_ / (self.C * n_samples)\n",
    "            grad_b = residual.mean()\n",
    "\n",
    "            self.coef_ -= self.lr * grad_w\n",
    "            self.intercept_ -= self.lr * grad_b\n",
    "\n",
    "            if self.tol is not None and (i % 10 == 0 or i == self.max_iter - 1):\n",
    "                z = X.dot(self.coef_) + self.intercept_\n",
    "                p = expit(z)\n",
    "                eps = 1e-15\n",
    "                loss_vec = (\n",
    "                    -(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n",
    "                ) * sample_weights\n",
    "                loss = loss_vec.mean() + 0.5 * np.sum(self.coef_ ** 2) / (self.C * n_samples)\n",
    "                # Store loss for optional diagnostics\n",
    "                self.loss_history_.append(float(loss))\n",
    "                if self.verbose:\n",
    "                    print(f\"Iter {i}: loss={loss:.6f}\")\n",
    "                if prev_loss is not None and abs(prev_loss - loss) < self.tol:\n",
    "                    break\n",
    "                prev_loss = loss\n",
    "\n",
    "        self.classes_ = np.array([0.0, 1.0])\n",
    "        return self\n",
    "\n",
    "    def _decision_function(self, X):\n",
    "        X = self._prepare_X(X)\n",
    "        return X.dot(self.coef_) + self.intercept_\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = self._decision_function(X)\n",
    "        p_pos = expit(z)\n",
    "        return np.vstack([1 - p_pos, p_pos]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n",
    "\n",
    "print({\"data_path\": str(DATA_PATH)})\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Fix legacy header quirk in the v2 MBFC CSV where the first column name was mangled.\n",
    "if \"source\" not in df.columns:\n",
    "    first_col = df.columns[0]\n",
    "    df = df.rename(columns={first_col: \"source\"})\n",
    "\n",
    "# Strip URLs from message text at load time so the pipeline never sees raw URLs.\n",
    "df[\"message\"] = df[\"message\"].astype(str).str.replace(\n",
    "    r\"(https?://|http://|www\\.[^\\s]*|t\\.me/[^\\s]*)\", \" \", regex=True\n",
    ").str.strip()\n",
    "# Drop rows whose message becomes empty after URL stripping.\n",
    "df = df[df[\"message\"] != \"\"].copy()\n",
    "\n",
    "print({\"raw_rows\": len(df)})\n",
    "\n",
    "# Use MBFC-derived binary risk label as y (1 = higher-risk / lower-credibility)\n",
    "df = df.dropna(subset=[\"risk_label\"]).copy()\n",
    "df[\"y\"] = df[\"risk_label\"].astype(int)\n",
    "print({\n",
    "    \"rows_with_label\": len(df),\n",
    "    \"high_risk\": int(df[\"y\"].sum()),\n",
    "    \"high_cred\": int((1 - df[\"y\"]).sum()),\n",
    "})\n",
    "\n",
    "groups = df[\"normalized_domain\"].astype(str).values\n",
    "y = df[\"y\"].values\n",
    "p_global = y.mean()\n",
    "print({\"global_pos_rate\": float(p_global)})\n",
    "\n",
    "# 1) URL-masked train+val vs test (20% test)\n",
    "gss_outer = GroupShuffleSplit(n_splits=50, test_size=0.2, random_state=42)\n",
    "best_score_outer = None\n",
    "best_outer = None\n",
    "\n",
    "for split_id, (trainval_idx, test_idx) in enumerate(gss_outer.split(df, y, groups)):\n",
    "    y_trainval = y[trainval_idx]\n",
    "    y_test = y[test_idx]\n",
    "    p_trainval = y_trainval.mean()\n",
    "    p_test = y_test.mean()\n",
    "    score = max(abs(p_trainval - p_global), abs(p_test - p_global))\n",
    "    if best_score_outer is None or score < best_score_outer:\n",
    "        best_score_outer = score\n",
    "        best_outer = (trainval_idx, test_idx, p_trainval, p_test, split_id)\n",
    "\n",
    "trainval_idx, test_idx, p_trainval, p_test, outer_id = best_outer\n",
    "df_trainval = df.iloc[trainval_idx].copy()\n",
    "df_test = df.iloc[test_idx].copy()\n",
    "\n",
    "# 2) Random train vs val within trainval (~70/10 from 80), stratified by label\n",
    "df_train, df_val = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size=0.125,\n",
    "    random_state=43,\n",
    "    stratify=df_trainval[\"y\"],\n",
    ")\n",
    "p_train = float(df_train[\"y\"].mean())\n",
    "p_val = float(df_val[\"y\"].mean())\n",
    "\n",
    "print({\n",
    "    \"global_pos_rate\": float(p_global),\n",
    "    \"outer_split_id\": int(outer_id),\n",
    "    \"outer_balance_score\": float(best_score_outer),\n",
    "    \"inner_split_id\": 0,\n",
    "    \"inner_balance_score\": float(max(abs(p_train - p_global), abs(p_val - p_global))),\n",
    "    \"train_rows\": len(df_train),\n",
    "    \"val_rows\": len(df_val),\n",
    "    \"test_rows\": len(df_test),\n",
    "    \"train_pos_rate\": float(p_train),\n",
    "    \"val_pos_rate\": float(p_val),\n",
    "    \"test_pos_rate\": float(df_test[\"y\"].mean()),\n",
    "    \"train_label_counts\": df_train[\"y\"].value_counts().to_dict(),\n",
    "    \"val_label_counts\": df_val[\"y\"].value_counts().to_dict(),\n",
    "    \"test_label_counts\": df_test[\"y\"].value_counts().to_dict(),\n",
    "})\n",
    "\n",
    "\n",
    "THRESH_GRID = [round(t, 2) for t in np.linspace(0.05, 0.95, 19)]\n",
    "\n",
    "def sweep_thresholds(y_true, proba, grid=THRESH_GRID):\n",
    "    best = None\n",
    "    for t in grid:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        macro_f1 = f1_score(y_true, pred, average='macro')\n",
    "        macro_recall = recall_score(y_true, pred, average='macro')\n",
    "        rec_pos = recall_score(y_true, pred, pos_label=1)\n",
    "        candidate = {\n",
    "            'threshold': float(t),\n",
    "            'macro_f1': macro_f1,\n",
    "            'macro_recall': macro_recall,\n",
    "            'recall_pos': rec_pos,\n",
    "        }\n",
    "        if best is None or candidate['macro_f1'] > best['macro_f1']:\n",
    "            best = candidate\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef392d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:33:00.814107Z",
     "iopub.status.busy": "2025-12-07T20:33:00.814000Z",
     "iopub.status.idle": "2025-12-07T20:35:21.718981Z",
     "shell.execute_reply": "2025-12-07T20:35:21.718649Z"
    }
   },
   "outputs": [],
   "source": [
    "## TF-IDF text only with grid search over n-grams and max_features\n",
    "ngram_grid = [(1, 1), (1, 2), (1, 3)]\n",
    "max_features_grid = [20000, 50000, 100000]\n",
    "candidate_lrs = [0.0003, 0.001, 0.003]\n",
    "\n",
    "best = None\n",
    "\n",
    "for ngram_range in ngram_grid:\n",
    "    for max_features in max_features_grid:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            ngram_range=ngram_range,\n",
    "            max_features=max_features,\n",
    "            min_df=2,\n",
    "            strip_accents=\"unicode\",\n",
    "        )\n",
    "        X_train_text = vectorizer.fit_transform(df_train['message'].astype(str))\n",
    "        X_val_text = vectorizer.transform(df_val['message'].astype(str))\n",
    "        X_test_text = vectorizer.transform(df_test['message'].astype(str))\n",
    "\n",
    "        for lr in candidate_lrs:\n",
    "            clf = ManualLogisticRegression(\n",
    "                max_iter=1000,\n",
    "                lr=lr,\n",
    "                C=1.0,\n",
    "                class_weight=\"balanced\",\n",
    "                tol=None,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            clf.fit(X_train_text, df_train['y'])\n",
    "            val_proba = clf.predict_proba(X_val_text)[:, 1]\n",
    "            best_thr = sweep_thresholds(df_val['y'].values, val_proba)\n",
    "            candidate = {\n",
    "                'ngram_range': ngram_range,\n",
    "                'max_features': max_features,\n",
    "                'lr': lr,\n",
    "                **best_thr,\n",
    "            }\n",
    "            if best is None or candidate['macro_f1'] > best['macro_f1']:\n",
    "                best = candidate\n",
    "                best_vectorizer = vectorizer\n",
    "                best_X_train_text = X_train_text\n",
    "                best_X_val_text = X_val_text\n",
    "                best_X_test_text = X_test_text\n",
    "\n",
    "# Refit best text model on train only for stacking features\n",
    "X_train_text = best_X_train_text\n",
    "X_val_text = best_X_val_text\n",
    "X_test_text = best_X_test_text\n",
    "\n",
    "clf_text_val = ManualLogisticRegression(\n",
    "    max_iter=1000,\n",
    "    lr=best['lr'],\n",
    "    C=1.0,\n",
    "    class_weight=\"balanced\",\n",
    "    tol=None,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "clf_text_val.fit(X_train_text, df_train['y'])\n",
    "val_proba_text = clf_text_val.predict_proba(X_val_text)[:, 1]\n",
    "\n",
    "# Final text model on train+val for test predictions\n",
    "X_trainval_text = sparse.vstack([X_train_text, X_val_text])\n",
    "y_trainval = pd.concat([df_train['y'], df_val['y']]).values\n",
    "clf_text = ManualLogisticRegression(\n",
    "    max_iter=1000,\n",
    "    lr=best['lr'],\n",
    "    C=1.0,\n",
    "    class_weight=\"balanced\",\n",
    "    tol=None,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "clf_text.fit(X_trainval_text, y_trainval)\n",
    "y_proba_text = clf_text.predict_proba(X_test_text)[:, 1]\n",
    "y_pred_text = (y_proba_text >= best['threshold']).astype(int)\n",
    "\n",
    "print({\n",
    "    'best_ngram_range': best['ngram_range'],\n",
    "    'best_max_features': int(best['max_features']),\n",
    "    'best_lr_text': best['lr'],\n",
    "    'val_macro_f1': float(best['macro_f1']),\n",
    "    'val_threshold': best['threshold'],\n",
    "})\n",
    "print('TF-IDF only ROC AUC (test):', roc_auc_score(df_test['y'], y_proba_text))\n",
    "print('TF-IDF only classification report (test, val-tuned threshold):')\n",
    "print(classification_report(df_test['y'], y_pred_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16cd8bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:35:21.720512Z",
     "iopub.status.busy": "2025-12-07T20:35:21.720417Z",
     "iopub.status.idle": "2025-12-07T20:38:59.796434Z",
     "shell.execute_reply": "2025-12-07T20:38:59.796105Z"
    }
   },
   "outputs": [],
   "source": [
    "## Style-only features (Qwen labels, normalized)\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "# v6: remove Qwen 'Link/URL' labels from style features\n",
    "DROP_LINK_URL_LABEL = True\n",
    "_LINK_URL_LABEL_NORM = 'link/url'\n",
    "\n",
    "def tokenize_multi(value: str) -> List[str]:\n",
    "    \"\"\"Split a comma- or plus-separated label string into atomic pieces.\"\"\"\n",
    "    if not isinstance(value, str):\n",
    "        return []\n",
    "    value = value.replace(\"+\", \",\")\n",
    "    parts = [part.strip() for part in value.split(\",\") if part.strip()]\n",
    "    if not DROP_LINK_URL_LABEL:\n",
    "        return parts\n",
    "    # Normalize by lowercasing and removing whitespace so 'Link / URL' matches too.\n",
    "    return [p for p in parts if ''.join(p.lower().split()) != _LINK_URL_LABEL_NORM]\n",
    "\n",
    "\n",
    "# Canonical buckets used for style features (should correspond to the\n",
    "# dimensions shown in the style diagram: compact theme, claim, CTA, evidence).\n",
    "THEME_BUCKETS = [\n",
    "    \"Finance/Crypto\",\n",
    "    \"Public health & medicine\",\n",
    "    \"Politics\",\n",
    "    \"Lifestyle & well-being\",\n",
    "    \"Crime & public safety\",\n",
    "    \"Gaming/Gambling\",\n",
    "    \"News/Information\",\n",
    "    \"Sports\",\n",
    "    \"Technology\",\n",
    "    \"Conversation/Chat/Other\",\n",
    "    \"Other theme\",\n",
    "]\n",
    "\n",
    "CLAIM_BUCKETS = [\n",
    "    \"Verifiable factual statement\",\n",
    "    \"Rumour / unverified report\",\n",
    "    \"Announcement\",\n",
    "    \"Opinion / subjective statement\",\n",
    "    \"Misleading context / cherry-picking\",\n",
    "    \"Promotional hype / exaggerated profit guarantee\",\n",
    "    \"Emotional appeal / fear-mongering\",\n",
    "    \"Scarcity/FOMO tactic\",\n",
    "    \"Statistics\",\n",
    "    \"Other claim type\",\n",
    "    \"No substantive claim\",\n",
    "    \"Fake content\",\n",
    "    \"Speculative forecast / prediction\",\n",
    "    \"None / assertion only\",\n",
    "]\n",
    "\n",
    "CTA_BUCKETS = [\n",
    "    \"Visit external link / watch video\",\n",
    "    \"Engage/Ask questions\",\n",
    "    \"Join/Subscribe\",\n",
    "    \"Buy / invest / donate\",\n",
    "    \"Attend event / livestream\",\n",
    "    \"Share / repost / like\",\n",
    "    \"No CTA\",\n",
    "    \"Other CTA\",\n",
    "]\n",
    "\n",
    "EVID_BUCKETS = [\n",
    "    \"Link/URL\",\n",
    "    \"Statistics\",\n",
    "    \"Quotes/Testimony\",\n",
    "    \"Chart / price graph / TA diagram\",\n",
    "    \"Other (Evidence)\",\n",
    "    \"None / assertion only\",\n",
    "]\n",
    "\n",
    "\n",
    "def _norm_theme(raw: object) -> Optional[str]:\n",
    "    if not isinstance(raw, str):\n",
    "        return None\n",
    "    t = raw.strip()\n",
    "    if not t:\n",
    "        return None\n",
    "    tl = t.lower()\n",
    "\n",
    "    # Direct match to canonical buckets\n",
    "    if t in THEME_BUCKETS:\n",
    "        return t\n",
    "\n",
    "    # Finance / crypto / markets\n",
    "    if any(k in tl for k in [\n",
    "        \"crypto\",\n",
    "        \"token\",\n",
    "        \"coin\",\n",
    "        \"airdrop\",\n",
    "        \"ido\",\n",
    "        \"staking\",\n",
    "        \"defi\",\n",
    "        \"exchange\",\n",
    "        \"market\",\n",
    "        \"finance\",\n",
    "        \"econom\",\n",
    "    ]):\n",
    "        return \"Finance/Crypto\"\n",
    "\n",
    "    # Health / medicine / public health\n",
    "    if any(k in tl for k in [\n",
    "        \"health\",\n",
    "        \"covid\",\n",
    "        \"vaccine\",\n",
    "        \"vaccination\",\n",
    "        \"medicine\",\n",
    "        \"medical\",\n",
    "        \"clinical\",\n",
    "        \"disease\",\n",
    "        \"pandemic\",\n",
    "        \"public health\",\n",
    "        \"hospital\",\n",
    "    ]):\n",
    "        return \"Public health & medicine\"\n",
    "\n",
    "    # Politics, policy, war, elections\n",
    "    if any(k in tl for k in [\n",
    "        \"politic\",\n",
    "        \"election\",\n",
    "        \"parliament\",\n",
    "        \"congress\",\n",
    "        \"senate\",\n",
    "        \"government\",\n",
    "        \"president\",\n",
    "        \"minister\",\n",
    "        \"policy\",\n",
    "        \"war\",\n",
    "        \"conflict\",\n",
    "        \"ukraine\",\n",
    "        \"russia\",\n",
    "    ]):\n",
    "        return \"Politics\"\n",
    "\n",
    "    # Crime / public safety\n",
    "    if any(k in tl for k in [\n",
    "        \"crime\",\n",
    "        \"criminal\",\n",
    "        \"terror\",\n",
    "        \"shooting\",\n",
    "        \"police\",\n",
    "        \"public safety\",\n",
    "        \"fraud\",\n",
    "        \"scam\",\n",
    "    ]):\n",
    "        return \"Crime & public safety\"\n",
    "\n",
    "    # Gaming / gambling\n",
    "    if any(k in tl for k in [\n",
    "        \"gaming\",\n",
    "        \"gambling\",\n",
    "        \"casino\",\n",
    "        \"betting\",\n",
    "        \"lottery\",\n",
    "        \"poker\",\n",
    "    ]):\n",
    "        return \"Gaming/Gambling\"\n",
    "\n",
    "    # Sports\n",
    "    if any(k in tl for k in [\"sport\", \"football\", \"soccer\", \"basketball\", \"tennis\", \"nba\", \"nfl\"]):\n",
    "        return \"Sports\"\n",
    "\n",
    "    # Technology / science\n",
    "    if any(k in tl for k in [\n",
    "        \"technology\",\n",
    "        \"tech\",\n",
    "        \"software\",\n",
    "        \"app \",\n",
    "        \"platform\",\n",
    "        \"ai \",\n",
    "        \" a.i.\",\n",
    "        \"machine learning\",\n",
    "        \"blockchain\",\n",
    "        \"internet\",\n",
    "        \"social media\",\n",
    "        \"algorithm\",\n",
    "        \"science\",\n",
    "        \"research\",\n",
    "        \"study\",\n",
    "    ]):\n",
    "        return \"Technology\"\n",
    "\n",
    "    # Lifestyle / culture / society\n",
    "    if any(k in tl for k in [\n",
    "        \"lifestyle\",\n",
    "        \"well-being\",\n",
    "        \"wellbeing\",\n",
    "        \"culture\",\n",
    "        \"entertainment\",\n",
    "        \"media\",\n",
    "        \"celebrity\",\n",
    "        \"social issues\",\n",
    "        \"society\",\n",
    "        \"family\",\n",
    "        \"community\",\n",
    "    ]):\n",
    "        return \"Lifestyle & well-being\"\n",
    "\n",
    "    # General news / updates\n",
    "    if any(k in tl for k in [\"news\", \"headline\", \"breaking\", \"coverage\", \"roundup\", \"update\"]):\n",
    "        return \"News/Information\"\n",
    "\n",
    "    # Conversational / chat-style content\n",
    "    if any(k in tl for k in [\"comment\", \"conversation\", \"chat\", \"q&a\", \"ama\", \"ask me anything\"]):\n",
    "        return \"Conversation/Chat/Other\"\n",
    "\n",
    "    return \"Other theme\"\n",
    "\n",
    "\n",
    "def _norm_claim_labels(raw: object) -> List[str]:\n",
    "    labels = tokenize_multi(raw)\n",
    "    out: List[str] = []\n",
    "    for lbl in labels:\n",
    "        base = lbl.strip()\n",
    "        if not base:\n",
    "            continue\n",
    "        low = base.lower()\n",
    "\n",
    "        if base in CLAIM_BUCKETS:\n",
    "            out.append(base)\n",
    "            continue\n",
    "\n",
    "        if \"no substantive claim\" in low:\n",
    "            out.append(\"No substantive claim\")\n",
    "        elif \"verifiable factual statement\" in low or \"reportage\" in low or \"study\" in low:\n",
    "            out.append(\"Verifiable factual statement\")\n",
    "        elif \"rumour\" in low or \"rumor\" in low or \"unverified\" in low:\n",
    "            out.append(\"Rumour / unverified report\")\n",
    "        elif \"misleading context\" in low or \"cherry-picking\" in low:\n",
    "            out.append(\"Misleading context / cherry-picking\")\n",
    "        elif \"promotional hype\" in low or \"exaggerated profit\" in low:\n",
    "            out.append(\"Promotional hype / exaggerated profit guarantee\")\n",
    "        elif \"emotional appeal\" in low or \"fear-mongering\" in low or \"fear mongering\" in low:\n",
    "            out.append(\"Emotional appeal / fear-mongering\")\n",
    "        elif \"scarcity\" in low or \"fomo\" in low:\n",
    "            out.append(\"Scarcity/FOMO tactic\")\n",
    "        elif \"statistic\" in low:\n",
    "            out.append(\"Statistics\")\n",
    "        elif \"fake content\" in low or \"fabricated\" in low:\n",
    "            out.append(\"Fake content\")\n",
    "        elif \"predict\" in low or \"forecast\" in low:\n",
    "            out.append(\"Speculative forecast / prediction\")\n",
    "        elif \"announcement\" in low:\n",
    "            out.append(\"Announcement\")\n",
    "        elif \"opinion\" in low or \"interpretive\" in low or \"analysis\" in low or \"review\" in low:\n",
    "            out.append(\"Opinion / subjective statement\")\n",
    "        elif \"none / assertion only\" in low or \"assertion only\" in low:\n",
    "            out.append(\"None / assertion only\")\n",
    "        else:\n",
    "            out.append(\"Other claim type\")\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    seen = set()\n",
    "    result: List[str] = []\n",
    "    for v in out:\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            result.append(v)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _norm_cta_labels(raw: object) -> List[str]:\n",
    "    labels = tokenize_multi(raw)\n",
    "    out: List[str] = []\n",
    "    for lbl in labels:\n",
    "        base = lbl.strip()\n",
    "        if not base:\n",
    "            continue\n",
    "        low = base.lower()\n",
    "\n",
    "        if base in CTA_BUCKETS:\n",
    "            out.append(base)\n",
    "            continue\n",
    "\n",
    "        if base in {\"None\", \"No CTA\"} or \"no cta\" in low:\n",
    "            out.append(\"No CTA\")\n",
    "        elif \"engage\" in low or \"ask\" in low or \"anything\" in low:\n",
    "            out.append(\"Engage/Ask questions\")\n",
    "        elif \"attend\" in low or \"event\" in low or \"livestream\" in low or \"live stream\" in low:\n",
    "            out.append(\"Attend event / livestream\")\n",
    "        elif \"join\" in low or \"subscribe\" in low or \"follow\" in low or \"whitelist\" in low:\n",
    "            out.append(\"Join/Subscribe\")\n",
    "        elif \"buy\" in low or \"invest\" in low or \"donate\" in low or \"stake\" in low or \"swap\" in low:\n",
    "            out.append(\"Buy / invest / donate\")\n",
    "        elif \"share\" in low or \"repost\" in low or \"like\" in low:\n",
    "            out.append(\"Share / repost / like\")\n",
    "        elif \"visit\" in low or \"read\" in low or \"watch\" in low or \"link\" in low or \"website\" in low or \"check\" in low or \"view charts\" in low:\n",
    "            out.append(\"Visit external link / watch video\")\n",
    "        else:\n",
    "            out.append(\"Other CTA\")\n",
    "\n",
    "    seen = set()\n",
    "    result: List[str] = []\n",
    "    for v in out:\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            result.append(v)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _norm_evidence_labels(raw: object) -> List[str]:\n",
    "    labels = tokenize_multi(raw)\n",
    "    out: List[str] = []\n",
    "    for lbl in labels:\n",
    "        base = lbl.strip()\n",
    "        if not base:\n",
    "            continue\n",
    "        low = base.lower()\n",
    "\n",
    "        if base in EVID_BUCKETS:\n",
    "            if base != \"Link/URL\":\n",
    "                out.append(base)\n",
    "            continue\n",
    "\n",
    "        if \"link/url\" in low or \"link\" in low or \"url\" in low:\n",
    "            # v6: drop Link/URL evidence tag entirely\n",
    "            continue\n",
    "        elif \"statistic\" in low:\n",
    "            out.append(\"Statistics\")\n",
    "        elif \"quote\" in low or \"testimony\" in low:\n",
    "            out.append(\"Quotes/Testimony\")\n",
    "        elif \"chart\" in low or \"graph\" in low or \"diagram\" in low:\n",
    "            out.append(\"Chart / price graph / TA diagram\")\n",
    "        elif \"none / assertion only\" in low or \"assertion only\" in low:\n",
    "            out.append(\"None / assertion only\")\n",
    "        else:\n",
    "            out.append(\"Other (Evidence)\")\n",
    "\n",
    "    seen = set()\n",
    "    result: List[str] = []\n",
    "    for v in out:\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            result.append(v)\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_style_tokens(row) -> list:\n",
    "    tokens: List[str] = []\n",
    "\n",
    "    theme = _norm_theme(row.get(\"theme\"))\n",
    "    if theme is not None:\n",
    "        tokens.append(f\"theme={theme}\")\n",
    "\n",
    "    for label in _norm_claim_labels(row.get(\"claim_types\")):\n",
    "        tokens.append(f\"claim={label}\")\n",
    "\n",
    "    for label in _norm_cta_labels(row.get(\"ctas\")):\n",
    "        tokens.append(f\"cta={label}\")\n",
    "\n",
    "    for label in _norm_evidence_labels(row.get(\"evidence\")):\n",
    "        tokens.append(f\"evid={label}\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "df_train['style_tokens'] = df_train.apply(build_style_tokens, axis=1)\n",
    "df_val['style_tokens'] = df_val.apply(build_style_tokens, axis=1)\n",
    "df_test['style_tokens'] = df_test.apply(build_style_tokens, axis=1)\n",
    "print({'example_style_tokens': df_train['style_tokens'].iloc[0][:40]})\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "X_train_style = mlb.fit_transform(df_train['style_tokens'])\n",
    "X_val_style = mlb.transform(df_val['style_tokens'])\n",
    "X_test_style = mlb.transform(df_test['style_tokens'])\n",
    "print({'style_train_shape': X_train_style.shape, 'style_val_shape': X_val_style.shape, 'style_test_shape': X_test_style.shape, 'style_vocab_size': len(mlb.classes_)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_lrs = [0.0003, 0.001, 0.003, 0.01, 0.03]\n",
    "best = None\n",
    "\n",
    "for lr in candidate_lrs:\n",
    "    clf = ManualLogisticRegression(\n",
    "        max_iter=3000,\n",
    "        lr=lr,\n",
    "        C=1.0,\n",
    "        class_weight=\"balanced\",\n",
    "        tol=None,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    clf.fit(X_train_style, df_train['y'])\n",
    "    val_proba = clf.predict_proba(X_val_style)[:, 1]\n",
    "    best_thr = sweep_thresholds(df_val['y'].values, val_proba)\n",
    "    candidate = {'lr': lr, **best_thr}\n",
    "    if best is None or candidate['macro_f1'] > best['macro_f1']:\n",
    "        best = candidate\n",
    "\n",
    "# Refit best style model on train only for stacking features\n",
    "clf_style_val = ManualLogisticRegression(\n",
    "    max_iter=3000,\n",
    "    lr=best['lr'],\n",
    "    C=1.0,\n",
    "    class_weight=\"balanced\",\n",
    "    tol=None,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "clf_style_val.fit(X_train_style, df_train['y'])\n",
    "val_proba_style = clf_style_val.predict_proba(X_val_style)[:, 1]\n",
    "\n",
    "# Final style model on train+val for test predictions\n",
    "X_trainval_style = sparse.vstack([X_train_style, X_val_style])\n",
    "y_trainval = pd.concat([df_train['y'], df_val['y']]).values\n",
    "clf_style = ManualLogisticRegression(\n",
    "    max_iter=3000,\n",
    "    lr=best['lr'],\n",
    "    C=1.0,\n",
    "    class_weight=\"balanced\",\n",
    "    tol=None,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "clf_style.fit(X_trainval_style, y_trainval)\n",
    "y_proba_style = clf_style.predict_proba(X_test_style)[:, 1]\n",
    "y_pred_style = (y_proba_style >= best['threshold']).astype(int)\n",
    "\n",
    "print({\n",
    "    'best_lr_style': best['lr'],\n",
    "    'val_macro_f1': best['macro_f1'],\n",
    "    'val_threshold': best['threshold'],\n",
    "})\n",
    "print('Style-only ROC AUC (test):', roc_auc_score(df_test['y'], y_proba_style))\n",
    "print('Style-only classification report (test, val-tuned threshold):')\n",
    "print(classification_report(df_test['y'], y_pred_style))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce418e0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:38:59.797757Z",
     "iopub.status.busy": "2025-12-07T20:38:59.797674Z",
     "iopub.status.idle": "2025-12-07T20:41:22.909962Z",
     "shell.execute_reply": "2025-12-07T20:41:22.909654Z"
    }
   },
   "outputs": [],
   "source": [
    "## Combined TF-IDF + style features (stacked ensemble)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Build meta-features from validation and test probabilities\n",
    "meta_X_val = np.stack([\n",
    "    val_proba_text,\n",
    "    val_proba_style,\n",
    "    val_proba_text * val_proba_style,\n",
    "], axis=1)\n",
    "meta_X_test = np.stack([\n",
    "    y_proba_text,\n",
    "    y_proba_style,\n",
    "    y_proba_text * y_proba_style,\n",
    "], axis=1)\n",
    "\n",
    "# Meta-model trained on validation predictions only\n",
    "meta_clf = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    ")\n",
    "meta_clf.fit(meta_X_val, df_val['y'].values)\n",
    "\n",
    "meta_val_proba = meta_clf.predict_proba(meta_X_val)[:, 1]\n",
    "best_thr_meta = sweep_thresholds(df_val['y'].values, meta_val_proba)\n",
    "\n",
    "y_proba_combined = meta_clf.predict_proba(meta_X_test)[:, 1]\n",
    "y_pred_combined = (y_proba_combined >= best_thr_meta['threshold']).astype(int)\n",
    "\n",
    "print({\n",
    "    'meta_model': 'stacked_logreg',\n",
    "    'val_macro_f1': best_thr_meta['macro_f1'],\n",
    "    'val_threshold': best_thr_meta['threshold'],\n",
    "})\n",
    "print('TF-IDF + style ROC AUC (test):', roc_auc_score(df_test['y'], y_proba_combined))\n",
    "print('TF-IDF + style classification report (test, val-tuned threshold):')\n",
    "print(classification_report(df_test['y'], y_pred_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93e0f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:41:22.911335Z",
     "iopub.status.busy": "2025-12-07T20:41:22.911255Z",
     "iopub.status.idle": "2025-12-07T20:41:23.593390Z",
     "shell.execute_reply": "2025-12-07T20:41:23.593124Z"
    }
   },
   "outputs": [],
   "source": [
    "## Save artifacts inputs, predictions, metrics, and graph\n",
    "results_dir = Path(\"mbfc_url_masked_logreg_results_v6\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the subset of the input data actually used (messages with a risk label)\n",
    "input_path = results_dir / \"messages_with_risk_label.csv\"\n",
    "df.to_csv(input_path, index=False)\n",
    "print({\"input_csv\": str(input_path)})\n",
    "\n",
    "# Save test-set predictions for all three models\n",
    "y_true = df_test[\"y\"].values\n",
    "preds_df = df_test[[\"source\", \"msg_id\", \"channel\", \"risk_label\", \"y\"]].copy()\n",
    "preds_df.rename(columns={\"y\": \"y_true\"}, inplace=True)\n",
    "preds_df[\"tfidf_pred\"] = y_pred_text\n",
    "preds_df[\"tfidf_proba\"] = y_proba_text\n",
    "preds_df[\"style_pred\"] = y_pred_style\n",
    "preds_df[\"style_proba\"] = y_proba_style\n",
    "preds_df[\"combined_pred\"] = y_pred_combined\n",
    "preds_df[\"combined_proba\"] = y_proba_combined\n",
    "\n",
    "preds_path = results_dir / \"test_predictions_all_models.csv\"\n",
    "preds_df.to_csv(preds_path, index=False)\n",
    "print({\"predictions_csv\": str(preds_path)})\n",
    "\n",
    "# Build a tidy metrics table for all models\n",
    "metrics_rows = []\n",
    "models = [\n",
    "    (\"tfidf\", y_pred_text, y_proba_text),\n",
    "    (\"style\", y_pred_style, y_proba_style),\n",
    "    (\"combined\", y_pred_combined, y_proba_combined),\n",
    "]\n",
    "\n",
    "for name, y_pred, y_proba in models:\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    roc = roc_auc_score(y_true, y_proba)\n",
    "    acc = report.get(\"accuracy\", accuracy_score(y_true, y_pred))\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"model\": name,\n",
    "        \"label\": \"overall\",\n",
    "        \"metric\": \"roc_auc\",\n",
    "        \"value\": roc,\n",
    "    })\n",
    "    metrics_rows.append({\n",
    "        \"model\": name,\n",
    "        \"label\": \"overall\",\n",
    "        \"metric\": \"accuracy\",\n",
    "        \"value\": acc,\n",
    "    })\n",
    "\n",
    "    for label_key in [\"0\", \"1\", \"macro avg\", \"weighted avg\"]:\n",
    "        if label_key not in report:\n",
    "            continue\n",
    "        stats = report[label_key]\n",
    "        for metric_name in [\"precision\", \"recall\", \"f1-score\", \"support\"]:\n",
    "            metrics_rows.append({\n",
    "                \"model\": name,\n",
    "                \"label\": label_key,\n",
    "                \"metric\": metric_name,\n",
    "                \"value\": stats[metric_name],\n",
    "            })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "metrics_path = results_dir / \"metrics_summary.csv\"\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print({\"metrics_csv\": str(metrics_path)})\n",
    "\n",
    "# Make a simple bar chart for accuracy and ROC AUC per model\n",
    "model_order = [\"tfidf\", \"style\", \"combined\"]\n",
    "labels = [\"TF-IDF\", \"Style\", \"TF-IDF+Style\"]\n",
    "accs = [\n",
    "    float(metrics_df[(metrics_df[\"model\"] == m) & (metrics_df[\"metric\"] == \"accuracy\")][\"value\"].iloc[0])\n",
    "    for m in model_order\n",
    "]\n",
    "aucs = [\n",
    "    float(metrics_df[(metrics_df[\"model\"] == m) & (metrics_df[\"metric\"] == \"roc_auc\")][\"value\"].iloc[0])\n",
    "    for m in model_order\n",
    "]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(x - width / 2, accs, width, label=\"Accuracy\")\n",
    "ax.bar(x + width / 2, aucs, width, label=\"ROC AUC\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=15)\n",
    "ax.set_ylim(0.0, 1.05)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"MBFC URL-masked logistic regression performance\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "fig_path = results_dir / \"mbfc_logreg_metrics.png\"\n",
    "fig.savefig(fig_path, dpi=200)\n",
    "plt.close(fig)\n",
    "print({\"metrics_figure\": str(fig_path)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-metrics-plot",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T07:54:56.125658Z",
     "iopub.status.busy": "2025-12-07T07:54:56.125586Z",
     "iopub.status.idle": "2025-12-07T07:54:56.186743Z",
     "shell.execute_reply": "2025-12-07T07:54:56.186437Z"
    }
   },
   "outputs": [],
   "source": [
    "## Extended metrics plot Accuracy, ROC AUC, macro F1, macro Recall\n",
    "model_order = [\"tfidf\", \"style\", \"combined\"]\n",
    "model_labels = [\"TF-IDF\", \"Style\", \"TF-IDF+Style\"]\n",
    "\n",
    "# Overall accuracy and ROC AUC (already in metrics_df)\n",
    "accs = [\n",
    "    float(\n",
    "        metrics_df[\n",
    "            (metrics_df[\"model\"] == model_key)\n",
    "            & (metrics_df[\"label\"] == \"overall\")\n",
    "            & (metrics_df[\"metric\"] == \"accuracy\")\n",
    "        ][\"value\"].iloc[0]\n",
    "    )\n",
    "    for model_key in model_order\n",
    "]\n",
    "aucs = [\n",
    "    float(\n",
    "        metrics_df[\n",
    "            (metrics_df[\"model\"] == model_key)\n",
    "            & (metrics_df[\"label\"] == \"overall\")\n",
    "            & (metrics_df[\"metric\"] == \"roc_auc\")\n",
    "        ][\"value\"].iloc[0]\n",
    "    )\n",
    "    for model_key in model_order\n",
    "]\n",
    "\n",
    "# Macro-averaged F1 and Recall\n",
    "macro_f1s = [\n",
    "    float(\n",
    "        metrics_df[\n",
    "            (metrics_df[\"model\"] == model_key)\n",
    "            & (metrics_df[\"label\"] == \"macro avg\")\n",
    "            & (metrics_df[\"metric\"] == \"f1-score\")\n",
    "        ][\"value\"].iloc[0]\n",
    "    )\n",
    "    for model_key in model_order\n",
    "]\n",
    "macro_recalls = [\n",
    "    float(\n",
    "        metrics_df[\n",
    "            (metrics_df[\"model\"] == model_key)\n",
    "            & (metrics_df[\"label\"] == \"macro avg\")\n",
    "            & (metrics_df[\"metric\"] == \"recall\")\n",
    "        ][\"value\"].iloc[0]\n",
    "    )\n",
    "    for model_key in model_order\n",
    "]\n",
    "\n",
    "x = np.arange(len(model_order))\n",
    "\n",
    "# Caption-critical note: specify whether ROC-AUC is binary or macro-averaged one-vs-rest for multiclass.\n",
    "pred_path = results_dir / \"test_predictions_all_models.csv\"\n",
    "\n",
    "acc_stds = auc_stds = macro_f1_stds = macro_recall_stds = None\n",
    "if pred_path.exists():\n",
    "    preds_df = pd.read_csv(pred_path)\n",
    "    y_true = preds_df[\"y_true\"].astype(int).to_numpy()\n",
    "\n",
    "    def _bootstrap_metric_stds(model_key, n_boot=1000, seed=0):\n",
    "        pred = preds_df[f\"{model_key}_pred\"].astype(int).to_numpy()\n",
    "        proba = preds_df[f\"{model_key}_proba\"].astype(float).to_numpy()\n",
    "\n",
    "        idx0 = np.flatnonzero(y_true == 0)\n",
    "        idx1 = np.flatnonzero(y_true == 1)\n",
    "        n0, n1 = len(idx0), len(idx1)\n",
    "        if n0 == 0 or n1 == 0:\n",
    "            raise ValueError(\"Bootstrap requires both classes in y_true.\")\n",
    "\n",
    "        rng = np.random.default_rng(seed)\n",
    "        vals = np.empty((n_boot, 4), dtype=float)\n",
    "        for bootstrap_index in range(n_boot):\n",
    "            sample_idx = np.concatenate(\n",
    "                [\n",
    "                    rng.choice(idx0, size=n0, replace=True),\n",
    "                    rng.choice(idx1, size=n1, replace=True),\n",
    "                ]\n",
    "            )\n",
    "            yb = y_true[sample_idx]\n",
    "            pb = pred[sample_idx]\n",
    "            probab = proba[sample_idx]\n",
    "            vals[bootstrap_index, 0] = accuracy_score(yb, pb)\n",
    "            vals[bootstrap_index, 1] = roc_auc_score(yb, probab)\n",
    "            vals[bootstrap_index, 2] = f1_score(yb, pb, average=\"macro\")\n",
    "            vals[bootstrap_index, 3] = recall_score(yb, pb, average=\"macro\")\n",
    "\n",
    "        stds = vals.std(axis=0, ddof=1)\n",
    "        return float(stds[0]), float(stds[1]), float(stds[2]), float(stds[3])\n",
    "\n",
    "    acc_stds, auc_stds, macro_f1_stds, macro_recall_stds = [], [], [], []\n",
    "    for model_index, model_key in enumerate(model_order):\n",
    "        s_acc, s_auc, s_f1, s_rec = _bootstrap_metric_stds(model_key, n_boot=1000, seed=7 + model_index)\n",
    "        acc_stds.append(s_acc)\n",
    "        auc_stds.append(s_auc)\n",
    "        macro_f1_stds.append(s_f1)\n",
    "        macro_recall_stds.append(s_rec)\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "with plt.rc_context(\n",
    "    {\n",
    "        \"font.size\": 9,\n",
    "        \"axes.labelsize\": 9,\n",
    "        \"xtick.labelsize\": 9,\n",
    "        \"ytick.labelsize\": 9,\n",
    "        \"legend.fontsize\": 9,\n",
    "        \"pdf.fonttype\": 42,  # TrueType\n",
    "        \"ps.fonttype\": 42,  # TrueType\n",
    "    }\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(3.3, 2.2), dpi=300)\n",
    "    fig.subplots_adjust(left=0.16, right=0.995, bottom=0.22, top=0.80)\n",
    "\n",
    "    metric_specs = [\n",
    "        (\"Accuracy\", accs, acc_stds, \"o\", \"white\"),\n",
    "        (\"ROC-AUC\", aucs, auc_stds, \"s\", \"0.85\"),\n",
    "        (\"F1 (macro)\", macro_f1s, macro_f1_stds, \"^\", \"0.65\"),\n",
    "        (\"Recall (macro)\", macro_recalls, macro_recall_stds, \"D\", \"0.45\"),\n",
    "    ]\n",
    "    offsets = np.array([-0.24, -0.08, 0.08, 0.24])\n",
    "\n",
    "    for metric_index, (metric_label, metric_values, metric_errors, marker, face) in enumerate(metric_specs):\n",
    "        ax.errorbar(\n",
    "            x + offsets[metric_index],\n",
    "            metric_values,\n",
    "            yerr=metric_errors,\n",
    "            fmt=marker,\n",
    "            linestyle=\"none\",\n",
    "            color=\"black\",\n",
    "            ecolor=\"black\",\n",
    "            elinewidth=0.6,\n",
    "            capsize=2,\n",
    "            capthick=0.6,\n",
    "            markerfacecolor=face,\n",
    "            markeredgecolor=\"black\",\n",
    "            markeredgewidth=0.7,\n",
    "            markersize=5.2,\n",
    "            zorder=3,\n",
    "        )\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([\"TF-IDF\", \"Style\", \"TF-IDF\\n+Style\"])\n",
    "    ax.set_ylabel(\"Score\")\n",
    "\n",
    "    all_vals = np.array([accs, aucs, macro_f1s, macro_recalls], dtype=float).ravel()\n",
    "    y_min = max(0.0, np.floor((all_vals.min() - 0.03) * 20) / 20)\n",
    "    y_max = min(1.0, np.ceil((all_vals.max() + 0.03) * 20) / 20)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(axis=\"y\", which=\"major\", lw=0.5, alpha=0.2)\n",
    "\n",
    "    legend_handles = [\n",
    "        Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=marker,\n",
    "            linestyle=\"none\",\n",
    "            color=\"black\",\n",
    "            markerfacecolor=face,\n",
    "            markeredgecolor=\"black\",\n",
    "            markeredgewidth=0.7,\n",
    "            markersize=5.2,\n",
    "            label=label,\n",
    "        )\n",
    "        for (label, _, _, marker, face) in metric_specs\n",
    "    ]\n",
    "    fig.legend(\n",
    "        handles=legend_handles,\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 0.995),\n",
    "        ncol=2,\n",
    "        frameon=False,\n",
    "        handlelength=1.0,\n",
    "        columnspacing=1.0,\n",
    "        handletextpad=0.4,\n",
    "    )\n",
    "\n",
    "    extended_fig_path = results_dir / \"mbfc_logreg_metrics_extended.png\"\n",
    "    extended_pdf_path = results_dir / \"mbfc_logreg_metrics_extended.pdf\"\n",
    "    fig.savefig(extended_fig_path, dpi=300, bbox_inches=None, pad_inches=0.0)\n",
    "    fig.savefig(extended_pdf_path, bbox_inches=None, pad_inches=0.0)\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\n",
    "    {\n",
    "        \"extended_metrics_figure\": str(extended_fig_path),\n",
    "        \"extended_metrics_pdf\": str(extended_pdf_path),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Calibration metrics plot (ECE and Brier score)\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "\n",
    "def _expected_calibration_error(y_true, y_proba, n_bins=10):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    indices = np.digitize(y_proba, bins) - 1\n",
    "    ece = 0.0\n",
    "    n = len(y_true)\n",
    "    for b in range(n_bins):\n",
    "        mask = indices == b\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        p_bin = y_proba[mask].mean()\n",
    "        y_bin = y_true[mask].mean()\n",
    "        weight = mask.sum() / n\n",
    "        ece += weight * abs(p_bin - y_bin)\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "if pred_path.exists():\n",
    "    if \"preds_df\" not in locals():\n",
    "        preds_df = pd.read_csv(pred_path)\n",
    "        y_true = preds_df[\"y_true\"].astype(int).to_numpy()\n",
    "\n",
    "    eces = [\n",
    "        _expected_calibration_error(\n",
    "            y_true,\n",
    "            preds_df[f\"{model_key}_proba\"].astype(float).to_numpy(),\n",
    "            n_bins=10,\n",
    "        )\n",
    "        for model_key in model_order\n",
    "    ]\n",
    "    briers = [\n",
    "        float(brier_score_loss(y_true, preds_df[f\"{model_key}_proba\"].astype(float).to_numpy()))\n",
    "        for model_key in model_order\n",
    "    ]\n",
    "\n",
    "    def _bootstrap_calibration_stds(model_key, n_boot=1000, seed=0):\n",
    "        proba = preds_df[f\"{model_key}_proba\"].astype(float).to_numpy()\n",
    "        idx0 = np.flatnonzero(y_true == 0)\n",
    "        idx1 = np.flatnonzero(y_true == 1)\n",
    "        n0, n1 = len(idx0), len(idx1)\n",
    "        if n0 == 0 or n1 == 0:\n",
    "            raise ValueError(\"Bootstrap requires both classes in y_true.\")\n",
    "\n",
    "        rng = np.random.default_rng(seed)\n",
    "        vals = np.empty((n_boot, 2), dtype=float)\n",
    "        for bootstrap_index in range(n_boot):\n",
    "            sample_idx = np.concatenate(\n",
    "                [\n",
    "                    rng.choice(idx0, size=n0, replace=True),\n",
    "                    rng.choice(idx1, size=n1, replace=True),\n",
    "                ]\n",
    "            )\n",
    "            yb = y_true[sample_idx]\n",
    "            probab = proba[sample_idx]\n",
    "            vals[bootstrap_index, 0] = _expected_calibration_error(yb, probab, n_bins=10)\n",
    "            vals[bootstrap_index, 1] = brier_score_loss(yb, probab)\n",
    "\n",
    "        stds = vals.std(axis=0, ddof=1)\n",
    "        return float(stds[0]), float(stds[1])\n",
    "\n",
    "    ece_stds, brier_stds = [], []\n",
    "    for model_index, model_key in enumerate(model_order):\n",
    "        s_ece, s_brier = _bootstrap_calibration_stds(model_key, n_boot=1000, seed=29 + model_index)\n",
    "        ece_stds.append(s_ece)\n",
    "        brier_stds.append(s_brier)\n",
    "\n",
    "    with plt.rc_context(\n",
    "        {\n",
    "            \"font.size\": 9,\n",
    "            \"axes.labelsize\": 9,\n",
    "            \"xtick.labelsize\": 9,\n",
    "            \"ytick.labelsize\": 9,\n",
    "            \"legend.fontsize\": 9,\n",
    "            \"pdf.fonttype\": 42,  # TrueType\n",
    "            \"ps.fonttype\": 42,  # TrueType\n",
    "        }\n",
    "    ):\n",
    "        fig, ax = plt.subplots(figsize=(3.3, 2.0), dpi=300)\n",
    "        fig.subplots_adjust(left=0.16, right=0.995, bottom=0.22, top=0.80)\n",
    "\n",
    "        metric_specs = [\n",
    "            (\"ECE\", eces, ece_stds, \"o\", \"white\"),\n",
    "            (\"Brier\", briers, brier_stds, \"s\", \"0.75\"),\n",
    "        ]\n",
    "        offsets = np.array([-0.10, 0.10])\n",
    "\n",
    "        for metric_index, (metric_label, metric_values, metric_errors, marker, face) in enumerate(metric_specs):\n",
    "            ax.errorbar(\n",
    "                x + offsets[metric_index],\n",
    "                metric_values,\n",
    "                yerr=metric_errors,\n",
    "                fmt=marker,\n",
    "                linestyle=\"none\",\n",
    "                color=\"black\",\n",
    "                ecolor=\"black\",\n",
    "                elinewidth=0.6,\n",
    "                capsize=2,\n",
    "                capthick=0.6,\n",
    "                markerfacecolor=face,\n",
    "                markeredgecolor=\"black\",\n",
    "                markeredgewidth=0.7,\n",
    "                markersize=5.2,\n",
    "                zorder=3,\n",
    "            )\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([\"TF-IDF\", \"Style\", \"TF-IDF\\n+Style\"])\n",
    "        ax.set_ylabel(\"Error\")\n",
    "\n",
    "        all_vals = np.array([eces, briers], dtype=float).ravel()\n",
    "        y_min = max(0.0, np.floor((all_vals.min() - 0.01) * 100) / 100)\n",
    "        y_max = min(1.0, np.ceil((all_vals.max() + 0.01) * 100) / 100)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.grid(axis=\"y\", which=\"major\", lw=0.5, alpha=0.2)\n",
    "\n",
    "        legend_handles = [\n",
    "            Line2D(\n",
    "                [0],\n",
    "                [0],\n",
    "                marker=marker,\n",
    "                linestyle=\"none\",\n",
    "                color=\"black\",\n",
    "                markerfacecolor=face,\n",
    "                markeredgecolor=\"black\",\n",
    "                markeredgewidth=0.7,\n",
    "                markersize=5.2,\n",
    "                label=metric_label,\n",
    "            )\n",
    "            for (metric_label, _, _, marker, face) in metric_specs\n",
    "        ]\n",
    "        fig.legend(\n",
    "            handles=legend_handles,\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, 0.995),\n",
    "            ncol=2,\n",
    "            frameon=False,\n",
    "            handlelength=1.0,\n",
    "            columnspacing=1.0,\n",
    "            handletextpad=0.4,\n",
    "        )\n",
    "\n",
    "        cal_fig_path = results_dir / \"mbfc_logreg_calibration_metrics.png\"\n",
    "        cal_pdf_path = results_dir / \"mbfc_logreg_calibration_metrics.pdf\"\n",
    "        fig.savefig(cal_fig_path, dpi=300, bbox_inches=None, pad_inches=0.0)\n",
    "        fig.savefig(cal_pdf_path, bbox_inches=None, pad_inches=0.0)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(\n",
    "        {\n",
    "            \"calibration_metrics_figure\": str(cal_fig_path),\n",
    "            \"calibration_metrics_pdf\": str(cal_pdf_path),\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heatmap-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Heatmap of key metrics per model\n",
    "import numpy as np\n",
    "\n",
    "heatmap_metrics = [\"Accuracy\", \"ROC AUC\", \"Macro F1\", \"Macro recall\"]\n",
    "heatmap_values = np.array([accs, aucs, macro_f1s, macro_recalls])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), dpi=200)\n",
    "im = ax.imshow(heatmap_values, vmin=0.0, vmax=1.0, cmap=\"viridis\")\n",
    "\n",
    "ax.set_xticks(np.arange(len(model_labels)))\n",
    "ax.set_xticklabels(model_labels, rotation=15)\n",
    "ax.set_yticks(np.arange(len(heatmap_metrics)))\n",
    "ax.set_yticklabels(heatmap_metrics)\n",
    "\n",
    "for i in range(heatmap_values.shape[0]):\n",
    "    for j in range(heatmap_values.shape[1]):\n",
    "        val = heatmap_values[i, j]\n",
    "        text_color = \"white\" if val < 0.5 else \"black\"\n",
    "        ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=text_color, fontsize=8)\n",
    "\n",
    "ax.set_title(\"MBFC URL-masked logistic regression – metric heatmap\")\n",
    "cbar = fig.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Score\")\n",
    "fig.tight_layout()\n",
    "\n",
    "heatmap_path = results_dir / \"mbfc_logreg_metrics_heatmap.png\"\n",
    "fig.savefig(heatmap_path, dpi=200)\n",
    "plt.close(fig)\n",
    "print({\"metrics_heatmap\": str(heatmap_path)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799747d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper utilities for validation-tuned URL-masked evaluation\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import f1_score, recall_score, brier_score_loss\n",
    "\n",
    "RANDOM_STATES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "LR_GRID = [0.001, 0.003, 0.01]\n",
    "THRESH_GRID = [round(t, 2) for t in np.linspace(0.05, 0.95, 19)]\n",
    "PRIMARY_METRIC = 'macro_f1'\n",
    "\n",
    "# Simple URL pattern, aligned with the cleaning pipeline: strip http(s), www, and t.me links.\n",
    "URL_PATTERN = re.compile(r'(?:https?://|http://|www\\.[^\\s]*|t\\.me/[^\\s]*)', flags=re.IGNORECASE)\n",
    "\n",
    "def strip_urls(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace URLs with a space so they don't appear in the text features.\n",
    "    return URL_PATTERN.sub(' ', text)\n",
    "\n",
    "def _stack_features(a, b):\n",
    "    if sparse.issparse(a):\n",
    "        return sparse.vstack([a, b])\n",
    "    return np.vstack([a, b])\n",
    "\n",
    "def build_text_features(train_df, val_df, test_df):\n",
    "    # URL-masked text: remove URLs from the message field before vectorization.\n",
    "    def _preprocess(df):\n",
    "        return df['message'].astype(str).apply(strip_urls)\n",
    "\n",
    "    vec = TfidfVectorizer(ngram_range=(1, 2), max_features=20000, min_df=2, strip_accents='unicode')\n",
    "    X_train = vec.fit_transform(_preprocess(train_df))\n",
    "    X_val = vec.transform(_preprocess(val_df))\n",
    "    X_test = vec.transform(_preprocess(test_df))\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "def build_style_features(train_df, val_df, test_df):\n",
    "    train_tokens = train_df.apply(build_style_tokens, axis=1)\n",
    "    val_tokens = val_df.apply(build_style_tokens, axis=1)\n",
    "    test_tokens = test_df.apply(build_style_tokens, axis=1)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    X_train = mlb.fit_transform(train_tokens)\n",
    "    X_val = mlb.transform(val_tokens)\n",
    "    X_test = mlb.transform(test_tokens)\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "def sweep_thresholds(y_true, proba):\n",
    "    rows = []\n",
    "    for t in THRESH_GRID:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        rows.append({\n",
    "            'threshold': float(t),\n",
    "            'macro_f1': f1_score(y_true, pred, average='macro'),\n",
    "            'macro_recall': recall_score(y_true, pred, average='macro'),\n",
    "            'recall_pos': recall_score(y_true, pred, pos_label=1),\n",
    "        })\n",
    "    best = max(rows, key=lambda r: r[PRIMARY_METRIC])\n",
    "    return best, rows\n",
    "\n",
    "def expected_calibration_error(y_true, y_proba, n_bins=10):\n",
    "    \"\"\"Compute Expected Calibration Error (ECE) with equal-width bins.\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    indices = np.digitize(y_proba, bins) - 1\n",
    "    ece = 0.0\n",
    "    n = len(y_true)\n",
    "    for b in range(n_bins):\n",
    "        mask = indices == b\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        p_bin = y_proba[mask].mean()\n",
    "        y_bin = y_true[mask].mean()\n",
    "        weight = mask.sum() / n\n",
    "        ece += weight * abs(p_bin - y_bin)\n",
    "    return float(ece)\n",
    "\n",
    "def fit_with_val_search(X_train, y_train, X_val, y_val, X_test, y_test, lr_grid=None):\n",
    "    lr_grid = lr_grid or LR_GRID\n",
    "    best = None\n",
    "    for lr in lr_grid:\n",
    "        clf = ManualLogisticRegression(\n",
    "            max_iter=1000, lr=lr, C=1.0, class_weight=\"balanced\", tol=None, n_jobs=-1, verbose=False\n",
    "        )\n",
    "        clf.fit(X_train, y_train)\n",
    "        val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "        best_thr, _ = sweep_thresholds(y_val, val_proba)\n",
    "        candidate = {\n",
    "            'lr': lr,\n",
    "            'val_threshold': best_thr['threshold'],\n",
    "            'val_macro_f1': best_thr['macro_f1'],\n",
    "            'val_macro_recall': best_thr['macro_recall'],\n",
    "            'val_recall_pos': best_thr['recall_pos'],\n",
    "            'primary_score': best_thr[PRIMARY_METRIC],\n",
    "        }\n",
    "        if best is None or candidate['primary_score'] > best['primary_score']:\n",
    "            best = candidate\n",
    "    # Retrain on train+val with best lr\n",
    "    X_trainval = _stack_features(X_train, X_val)\n",
    "    y_trainval = np.concatenate([y_train, y_val])\n",
    "    final_clf = ManualLogisticRegression(\n",
    "        max_iter=1000, lr=best['lr'], C=1.0, class_weight=\"balanced\", tol=None, n_jobs=-1, verbose=False\n",
    "    )\n",
    "    final_clf.fit(X_trainval, y_trainval)\n",
    "    test_proba = final_clf.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_proba >= best['val_threshold']).astype(int)\n",
    "    brier = brier_score_loss(y_test, test_proba)\n",
    "    ece = expected_calibration_error(y_test, test_proba, n_bins=10)\n",
    "    return {\n",
    "        'best_lr': best['lr'],\n",
    "        'threshold': best['val_threshold'],\n",
    "        'test_macro_f1': f1_score(y_test, test_pred, average='macro'),\n",
    "        'test_macro_recall': recall_score(y_test, test_pred, average='macro'),\n",
    "        'test_recall_pos': recall_score(y_test, test_pred, pos_label=1),\n",
    "        'test_roc_auc': roc_auc_score(y_test, test_proba),\n",
    "        'test_accuracy': accuracy_score(y_test, test_pred),\n",
    "        'test_brier': float(brier),\n",
    "        'test_ece': float(ece),\n",
    "    }\n",
    "\n",
    "def train_base_for_stacking(X_train, y_train, X_val, y_val, X_test, lr_grid=None):\n",
    "    \"\"\"Train base ManualLogisticRegression and return val/test probabilities.\n",
    "\n",
    "    This mirrors fit_with_val_search's hyperparameter search but is used for\n",
    "    building stacked ensembles rather than for direct evaluation.\n",
    "    \"\"\"\n",
    "    lr_grid = lr_grid or LR_GRID\n",
    "    best_lr = None\n",
    "    best_thr = None\n",
    "    best_score = None\n",
    "    best_val_proba = None\n",
    "    for lr in lr_grid:\n",
    "        clf = ManualLogisticRegression(\n",
    "            max_iter=1000, lr=lr, C=1.0, class_weight=\"balanced\", tol=None, n_jobs=-1, verbose=False\n",
    "        )\n",
    "        clf.fit(X_train, y_train)\n",
    "        val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "        thr, _ = sweep_thresholds(y_val, val_proba)\n",
    "        score = thr[PRIMARY_METRIC]\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_lr = lr\n",
    "            best_thr = thr\n",
    "            best_val_proba = val_proba\n",
    "    X_trainval = _stack_features(X_train, X_val)\n",
    "    y_trainval = np.concatenate([y_train, y_val])\n",
    "    final_clf = ManualLogisticRegression(\n",
    "        max_iter=1000, lr=best_lr, C=1.0, class_weight=\"balanced\", tol=None, n_jobs=-1, verbose=False\n",
    "    )\n",
    "    final_clf.fit(X_trainval, y_trainval)\n",
    "    test_proba = final_clf.predict_proba(X_test)[:, 1]\n",
    "    return {\n",
    "        'lr': best_lr,\n",
    "        'val_threshold': best_thr['threshold'],\n",
    "        'val_macro_f1': best_thr['macro_f1'],\n",
    "        'val_macro_recall': best_thr['macro_recall'],\n",
    "        'val_recall_pos': best_thr['recall_pos'],\n",
    "        'val_proba': best_val_proba,\n",
    "        'test_proba': test_proba,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808895e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL-masked, validation-tuned evaluation (no tuning on test)\n",
    "results_dir = Path('mbfc_url_masked_logreg_results_v6')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# For URL-masked evaluation we only keep rows\n",
    "# that have a resolved normalized_domain (no NaNs in groups).\n",
    "df_eval = df.dropna(subset=['normalized_domain']).copy()\n",
    "print({'eval_rows': len(df_eval), 'unique_domains': int(df_eval['normalized_domain'].nunique())})\n",
    "url_masked_rows = []\n",
    "for split_id, seed in enumerate(RANDOM_STATES):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "    trainval_idx, test_idx = next(gss.split(df_eval, df_eval['y'], df_eval['normalized_domain']))\n",
    "    df_trainval = df_eval.iloc[trainval_idx].copy()\n",
    "    df_test_split = df_eval.iloc[test_idx].copy()\n",
    "\n",
    "    df_train_split, df_val_split = train_test_split(\n",
    "        df_trainval, test_size=0.125, random_state=100 + seed, stratify=df_trainval['y']\n",
    "    )\n",
    "    y_train = df_train_split['y'].values\n",
    "    y_val = df_val_split['y'].values\n",
    "    y_test = df_test_split['y'].values\n",
    "\n",
    "    # TF-IDF\n",
    "    X_train_text, X_val_text, X_test_text = build_text_features(df_train_split, df_val_split, df_test_split)\n",
    "    text_metrics = fit_with_val_search(X_train_text, y_train, X_val_text, y_val, X_test_text, y_test)\n",
    "    text_metrics.update({'model': 'tfidf', 'split_seed': int(seed)})\n",
    "    url_masked_rows.append(text_metrics)\n",
    "\n",
    "    # Style\n",
    "    X_train_style, X_val_style, X_test_style = build_style_features(df_train_split, df_val_split, df_test_split)\n",
    "    style_metrics = fit_with_val_search(X_train_style, y_train, X_val_style, y_val, X_test_style, y_test)\n",
    "    style_metrics.update({'model': 'style', 'split_seed': int(seed)})\n",
    "    url_masked_rows.append(style_metrics)\n",
    "\n",
    "    # Combined via stacked ensemble on probabilities\n",
    "    base_text = train_base_for_stacking(X_train_text, y_train, X_val_text, y_val, X_test_text)\n",
    "    base_style = train_base_for_stacking(X_train_style, y_train, X_val_style, y_val, X_test_style)\n",
    "\n",
    "    meta_X_val = np.stack([\n",
    "        base_text['val_proba'],\n",
    "        base_style['val_proba'],\n",
    "        base_text['val_proba'] * base_style['val_proba'],\n",
    "    ], axis=1)\n",
    "    meta_X_test = np.stack([\n",
    "        base_text['test_proba'],\n",
    "        base_style['test_proba'],\n",
    "        base_text['test_proba'] * base_style['test_proba'],\n",
    "    ], axis=1)\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    meta_clf = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=1000)\n",
    "    meta_clf.fit(meta_X_val, y_val)\n",
    "    meta_val_proba = meta_clf.predict_proba(meta_X_val)[:, 1]\n",
    "    meta_best_thr, _ = sweep_thresholds(y_val, meta_val_proba)\n",
    "\n",
    "    test_proba_combined = meta_clf.predict_proba(meta_X_test)[:, 1]\n",
    "    test_pred_combined = (test_proba_combined >= meta_best_thr['threshold']).astype(int)\n",
    "\n",
    "    brier_combined = brier_score_loss(y_test, test_proba_combined)\n",
    "    ece_combined = expected_calibration_error(y_test, test_proba_combined, n_bins=10)\n",
    "    combined_metrics = {\n",
    "        'best_lr': float(base_text['lr']),  # text lr for reference\n",
    "        'threshold': float(meta_best_thr['threshold']),\n",
    "        'test_macro_f1': f1_score(y_test, test_pred_combined, average='macro'),\n",
    "        'test_macro_recall': recall_score(y_test, test_pred_combined, average='macro'),\n",
    "        'test_recall_pos': recall_score(y_test, test_pred_combined, pos_label=1),\n",
    "        'test_roc_auc': roc_auc_score(y_test, test_proba_combined),\n",
    "        'test_accuracy': accuracy_score(y_test, test_pred_combined),\n",
    "        'test_brier': float(brier_combined),\n",
    "        'test_ece': float(ece_combined),\n",
    "        'model': 'combined',\n",
    "        'split_seed': int(seed),\n",
    "    }\n",
    "    url_masked_rows.append(combined_metrics)\n",
    "\n",
    "url_masked_df = pd.DataFrame(url_masked_rows)\n",
    "val_tuned_path = results_dir / 'url_masked_val_tuned_metrics.csv'\n",
    "url_masked_df.to_csv(val_tuned_path, index=False)\n",
    "print({'val_tuned_metrics_csv': str(val_tuned_path)})\n",
    "\n",
    "summary = url_masked_df.groupby('model')[\n",
    "    ['test_accuracy', 'test_roc_auc', 'test_macro_f1', 'test_macro_recall', 'test_recall_pos', 'test_brier', 'test_ece']\n",
    "].agg(['mean', 'std']).round(4)\n",
    "print('URL-masked (val-tuned) summary:')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a1339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save URL-masked robustness summary as grouped bars per model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Only plot the core metrics: accuracy, ROC AUC, macro F1\n",
    "metrics_to_plot = [\n",
    "    (\"test_accuracy\", \"Accuracy\"),\n",
    "    (\"test_roc_auc\", \"ROC AUC\"),\n",
    "    (\"test_macro_f1\", \"Macro F1\"),\n",
    "]\n",
    "\n",
    "models = [\"tfidf\", \"style\", \"combined\"]\n",
    "model_labels = [\"TF-IDF\", \"Style\", \"TF-IDF+Style\"]\n",
    "metric_colors = [\"tab:blue\", \"tab:orange\", \"tab:green\"]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.22\n",
    "fig, ax = plt.subplots(figsize=(7, 4), dpi=200)\n",
    "\n",
    "for j, ((metric_key, metric_label), color) in enumerate(zip(metrics_to_plot, metric_colors)):\n",
    "    means = [float(summary.loc[m][metric_key][\"mean\"]) for m in models]\n",
    "    offsets = x + (j - (len(metrics_to_plot) - 1) / 2) * width\n",
    "    ax.bar(offsets, means, width, label=metric_label, color=color, alpha=0.85)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels)\n",
    "# Zoom the y-axis to highlight differences between models\n",
    "ax.set_ylim(0.6, 0.9)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"URL-masked logistic regression – robustness across seeds\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "fig.tight_layout()\n",
    "\n",
    "robust_plot_path = results_dir / \"url_masked_val_tuned_robustness.png\"\n",
    "fig.savefig(robust_plot_path, dpi=200)\n",
    "plt.close(fig)\n",
    "print({\"url_masked_robustness_figure\": str(robust_plot_path)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0697f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: ManualLogisticRegression vs sklearn on a small text subset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "subset_n = min(len(df), 7000)\n",
    "df_subset = df.sample(n=subset_n, random_state=123, replace=False) if len(df) > subset_n else df.copy()\n",
    "train_sub, test_sub = train_test_split(\n",
    "    df_subset, test_size=0.3, random_state=321, stratify=df_subset['y']\n",
    ")\n",
    "vec_check = TfidfVectorizer(ngram_range=(1, 2), max_features=20000, min_df=2, strip_accents='unicode')\n",
    "X_tr = vec_check.fit_transform(train_sub['message'].astype(str))\n",
    "X_te = vec_check.transform(test_sub['message'].astype(str))\n",
    "y_tr = train_sub['y'].values\n",
    "y_te = test_sub['y'].values\n",
    "\n",
    "manual = ManualLogisticRegression(lr=0.01, max_iter=4000, C=1.0, class_weight=\"balanced\", tol=None)\n",
    "manual.fit(X_tr, y_tr)\n",
    "proba_manual = manual.predict_proba(X_te)[:, 1]\n",
    "\n",
    "sk = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', max_iter=4000)\n",
    "sk.fit(X_tr, y_tr)\n",
    "proba_sk = sk.predict_proba(X_te)[:, 1]\n",
    "\n",
    "mae = float(np.mean(np.abs(proba_manual - proba_sk)))\n",
    "print({'proba_mae_manual_vs_sklearn': mae})\n",
    "print({'roc_auc_manual': float(roc_auc_score(y_te, proba_manual)),\n",
    "       'roc_auc_sklearn': float(roc_auc_score(y_te, proba_sk)),\n",
    "       'acc_manual@0.5': float(accuracy_score(y_te, (proba_manual >= 0.5).astype(int))),\n",
    "       'acc_sklearn@0.5': float(accuracy_score(y_te, (proba_sk >= 0.5).astype(int)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c853ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: ManualLogisticRegression vs sklearn on a small text subset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "subset_n = min(len(df), 7000)\n",
    "df_subset = df.sample(n=subset_n, random_state=123, replace=False) if len(df) > subset_n else df.copy()\n",
    "train_sub, test_sub = train_test_split(\n",
    "    df_subset, test_size=0.3, random_state=321, stratify=df_subset['y']\n",
    ")\n",
    "vec_check = TfidfVectorizer(ngram_range=(1, 2), max_features=20000, min_df=2, strip_accents='unicode')\n",
    "X_tr = vec_check.fit_transform(train_sub['message'].astype(str))\n",
    "X_te = vec_check.transform(test_sub['message'].astype(str))\n",
    "y_tr = train_sub['y'].values\n",
    "y_te = test_sub['y'].values\n",
    "\n",
    "manual = ManualLogisticRegression(lr=0.01, max_iter=4000, C=1.0, class_weight=\"balanced\", tol=1e-4)\n",
    "manual.fit(X_tr, y_tr)\n",
    "proba_manual = manual.predict_proba(X_te)[:, 1]\n",
    "\n",
    "sk = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', max_iter=4000)\n",
    "sk.fit(X_tr, y_tr)\n",
    "proba_sk = sk.predict_proba(X_te)[:, 1]\n",
    "\n",
    "mae = float(np.mean(np.abs(proba_manual - proba_sk)))\n",
    "print({'proba_mae_manual_vs_sklearn': mae})\n",
    "print({'roc_auc_manual': float(roc_auc_score(y_te, proba_manual)),\n",
    "       'roc_auc_sklearn': float(roc_auc_score(y_te, proba_sk)),\n",
    "       'acc_manual@0.5': float(accuracy_score(y_te, (proba_manual >= 0.5).astype(int))),\n",
    "       'acc_sklearn@0.5': float(accuracy_score(y_te, (proba_sk >= 0.5).astype(int))),\n",
    "       'manual_loss_steps': len(manual.loss_history_),\n",
    "       'manual_final_loss': float(manual.loss_history_[-1])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f667e2",
   "metadata": {},
   "source": [
    "## discussion\n",
    "The results show that the compact and interpretable style representation can be used as a strong URL independent risk signal on the telegram. On the channel shielding 7164 message benchmark, the style only model is significantly better than the TF-IDF baseline in accuracy (0.852 vs.0.796) and macro F1 (0.846 vs.0.796), and the combined style+TF-IDF model has the best overall performance (0.892 accuracy, 0.890 macro F1). This shows that the lexical features themselves cannot capture the information related to the credibility of the \"how messages talk\" encoding, and when the explicit source identifier is deleted, these style patterns will be generalized in the invisible channel. At the same time, there are obvious sources of error and uncertainty: tags inherit from MBFC domain ratings rather than message level fact checking, so the model learns the agent of MBFC reputation judgment rather than the accuracy of basic facts; Style markers come from LLM, which may wrongly mark some themes, languages or rhetorical forms; The evaluation subset prefers channels linked to the MBFC rating domain, and the test set is mainly controlled by a small number of large high-risk or low-risk channels. The ROC ‑ AUC of TF ‑ IDF model is high, but the threshold performance is poor, which indicates that measurement selection and calibration are very important: the risk level of vocabulary model is high, but the calibration is not good, and the mainstream content is over marked, while the style based model produces a more reasonable positive rate, but may be conservative in some high-risk sources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5807b0a2",
   "metadata": {},
   "source": [
    "## conclusion\n",
    "In a word, this method is effective and has practical appeal, but it is not decisive. It provides a calibrated and interpretable message level risk score. It complements the URL based pipeline, and can work even when the domain is lost or confused. However, it inherits the bias of MBFC, ignores the content outside the message fragments linked with MBFC, and evaluates on a single channel partition rather than multiple random partitions. In terms of calculation, the cost of logistic regression stage is low, but the scale of LLM based marking pipeline is not insignificant, and it is assumed that the style strategy is relatively stable over time and community changes. Reasonable follow-up steps include: performing channel level cross validation or multiple groupshufflesplit seeds to quantify variance; Adjust the threshold of each model on the validation set to make the TF-IDF comparison more stringent; Expand the corpus beyond the MBFC link channel, and merge message level fact check tags where possible; Stress test the style label under the confrontation or evolution tactics; Explore richer but still interpretable features (for example, simple interaction patterns or forwarding structures) that keep URLs agnostic but capture more of the behavior specific to the telegraph platform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}