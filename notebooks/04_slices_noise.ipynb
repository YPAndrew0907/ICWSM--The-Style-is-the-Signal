{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01f1fa2",
   "metadata": {},
   "source": [
    "# MBFC URL-masked: slices + robustness (v6)\n",
    "\n",
    "This notebook adds reviewer-facing analyses on top of the **v6 URL-masked** experiment outputs:\n",
    "\n",
    "1. **Head/torso/tail domain analysis** (macro-F1 + calibration by domain-frequency bin).\n",
    "2. **Slice calibration** (reliability diagrams + ECE by theme + message-length bins).\n",
    "3. **Tagger-noise robustness** for **style-only (no Theme)** tags.\n",
    "\n",
    "It reuses the v6 **main URL-masked split** (the one that produced `test_predictions_all_models.csv`) so we can compute slices without re-running the full expensive training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    brier_score_loss,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "def _resolve_upward(start: Path, rel: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        cand = p / rel\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    raise FileNotFoundError(f\"Could not resolve path upward: {rel}\")\n",
    "\n",
    "\n",
    "HERE = Path.cwd().resolve()\n",
    "DATA_PATH = _resolve_upward(\n",
    "    HERE,\n",
    "    Path(\"mbfc_channel_masked_logreg_fullpackage_v2_MBFC_C\") / \"MBFC \" / \"mega_samples_dedup_qwen_mbfc.csv\",\n",
    ")\n",
    "V6_PREDS_PATH = _resolve_upward(\n",
    "    HERE,\n",
    "    Path(\"mbfc_channel_masked_logreg_fullpackage\")\n",
    "    / \"Pipelines\"\n",
    "    / \"mbfc_url_masked_logreg_results_v6\"\n",
    "    / \"test_predictions_all_models.csv\",\n",
    ")\n",
    "\n",
    "print({\"data_path\": str(DATA_PATH)})\n",
    "print({\"v6_test_predictions\": str(V6_PREDS_PATH)})\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "if \"source\" not in df.columns:\n",
    "    df = df.rename(columns={df.columns[0]: \"source\"})\n",
    "\n",
    "# Match v6 preprocessing: strip URLs from text.\n",
    "df[\"message\"] = (\n",
    "    df[\"message\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"(https?://|http://|www\\.[^\\s]*|t\\.me/[^\\s]*)\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "df = df[df[\"message\"] != \"\"].copy()\n",
    "\n",
    "# Use MBFC-derived binary risk label as y.\n",
    "df = df.dropna(subset=[\"risk_label\"]).copy()\n",
    "df[\"y\"] = df[\"risk_label\"].astype(int)\n",
    "\n",
    "print(\n",
    "    {\n",
    "        \"rows\": int(len(df)),\n",
    "        \"unique_domains\": int(df[\"normalized_domain\"].nunique()),\n",
    "        \"pos_rate\": float(df[\"y\"].mean()),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Reconstruct the exact v6 main URL-masked split used for test_predictions_all_models.csv.\n",
    "groups = df[\"normalized_domain\"].astype(str).values\n",
    "y = df[\"y\"].values\n",
    "p_global = float(y.mean())\n",
    "\n",
    "gss_outer = GroupShuffleSplit(n_splits=50, test_size=0.2, random_state=42)\n",
    "best_score_outer = None\n",
    "best_outer = None\n",
    "for split_id, (trainval_idx, test_idx) in enumerate(gss_outer.split(df, y, groups)):\n",
    "    y_trainval = y[trainval_idx]\n",
    "    y_test = y[test_idx]\n",
    "    p_trainval = y_trainval.mean()\n",
    "    p_test = y_test.mean()\n",
    "    score = max(abs(p_trainval - p_global), abs(p_test - p_global))\n",
    "    if best_score_outer is None or score < best_score_outer:\n",
    "        best_score_outer = score\n",
    "        best_outer = (trainval_idx, test_idx, split_id)\n",
    "\n",
    "trainval_idx, test_idx, outer_id = best_outer\n",
    "df_trainval = df.iloc[trainval_idx].copy()\n",
    "df_test = df.iloc[test_idx].copy().reset_index(drop=True)\n",
    "\n",
    "df_train, df_val = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size=0.125,\n",
    "    random_state=43,\n",
    "    stratify=df_trainval[\"y\"],\n",
    ")\n",
    "\n",
    "print(\n",
    "    {\n",
    "        \"outer_split_id\": int(outer_id),\n",
    "        \"outer_balance_score\": float(best_score_outer),\n",
    "        \"train_rows\": int(len(df_train)),\n",
    "        \"val_rows\": int(len(df_val)),\n",
    "        \"test_rows\": int(len(df_test)),\n",
    "        \"test_pos_rate\": float(df_test[\"y\"].mean()),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Load v6 predictions and align row-by-row (v6 wrote them in df_test order).\n",
    "preds = pd.read_csv(V6_PREDS_PATH).reset_index(drop=True)\n",
    "assert len(preds) == len(df_test), (len(preds), len(df_test))\n",
    "for col in [\"source\", \"msg_id\", \"channel\"]:\n",
    "    assert (df_test[col].astype(str) == preds[col].astype(str)).all(), f\"mismatch in {col}\"\n",
    "assert (df_test[\"y\"].astype(int) == preds[\"y_true\"].astype(int)).all(), \"y mismatch\"\n",
    "\n",
    "df_test_pred = pd.concat(\n",
    "    [\n",
    "        df_test,\n",
    "        preds[[\n",
    "            \"tfidf_pred\",\n",
    "            \"tfidf_proba\",\n",
    "            \"style_pred\",\n",
    "            \"style_proba\",\n",
    "            \"combined_pred\",\n",
    "            \"combined_proba\",\n",
    "        ]],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "print({\"df_test_pred_rows\": int(len(df_test_pred))})\n",
    "\n",
    "\n",
    "RESULTS_DIR = Path(\"mbfc_url_masked_logreg_generalization_analyses_results_v1\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "print({\"results_dir\": str(RESULTS_DIR.resolve())})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4c0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_calibration_error(y_true, y_proba, n_bins=10) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    idx = np.digitize(y_proba, bins) - 1\n",
    "    ece = 0.0\n",
    "    n = len(y_true)\n",
    "    for b in range(n_bins):\n",
    "        mask = idx == b\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        p_bin = float(y_proba[mask].mean())\n",
    "        y_bin = float(y_true[mask].mean())\n",
    "        weight = float(mask.sum() / n)\n",
    "        ece += weight * abs(p_bin - y_bin)\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "def calibration_bins(y_true, y_proba, n_bins=10):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    idx = np.digitize(y_proba, bins) - 1\n",
    "    rows = []\n",
    "    for b in range(n_bins):\n",
    "        mask = idx == b\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        rows.append(\n",
    "            {\n",
    "                \"bin\": int(b),\n",
    "                \"count\": int(mask.sum()),\n",
    "                \"p_mean\": float(y_proba[mask].mean()),\n",
    "                \"y_mean\": float(y_true[mask].mean()),\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "MODELS = [\n",
    "    (\"tfidf\", \"tfidf_pred\", \"tfidf_proba\"),\n",
    "    (\"style\", \"style_pred\", \"style_proba\"),\n",
    "    (\"combined\", \"combined_pred\", \"combined_proba\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6143ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Head/torso/tail domain analysis\n",
    "\n",
    "domain_counts = df[\"normalized_domain\"].astype(str).value_counts()\n",
    "df_test_pred[\"domain_count\"] = df_test_pred[\"normalized_domain\"].astype(str).map(domain_counts).astype(int)\n",
    "\n",
    "\n",
    "def domain_bin(count: int) -> str:\n",
    "    # Bins chosen from global count quantiles (roughly: <=4 tail, 5â€“49 torso, >=50 head).\n",
    "    if count >= 50:\n",
    "        return \"head (>=50)\"\n",
    "    if count >= 5:\n",
    "        return \"torso (5-49)\"\n",
    "    return \"tail (<=4)\"\n",
    "\n",
    "\n",
    "df_test_pred[\"domain_bin\"] = df_test_pred[\"domain_count\"].apply(domain_bin)\n",
    "print(df_test_pred[\"domain_bin\"].value_counts().to_dict())\n",
    "\n",
    "rows = []\n",
    "for bin_name, g in df_test_pred.groupby(\"domain_bin\"):\n",
    "    y_true = g[\"y\"].astype(int).to_numpy()\n",
    "    for model_name, pred_col, proba_col in MODELS:\n",
    "        y_pred = g[pred_col].astype(int).to_numpy()\n",
    "        y_proba = g[proba_col].astype(float).to_numpy()\n",
    "        rows.append(\n",
    "            {\n",
    "                \"domain_bin\": bin_name,\n",
    "                \"model\": model_name,\n",
    "                \"n\": int(len(g)),\n",
    "                \"pos_rate\": float(y_true.mean()),\n",
    "                \"macro_f1\": float(f1_score(y_true, y_pred, average=\"macro\")),\n",
    "                \"macro_recall\": float(recall_score(y_true, y_pred, average=\"macro\")),\n",
    "                \"roc_auc\": float(roc_auc_score(y_true, y_proba)),\n",
    "                \"brier\": float(brier_score_loss(y_true, y_proba)),\n",
    "                \"ece\": float(expected_calibration_error(y_true, y_proba, n_bins=10)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "domain_slice_df = pd.DataFrame(rows)\n",
    "domain_slice_csv = RESULTS_DIR / \"domain_head_torso_tail_metrics.csv\"\n",
    "domain_slice_df.to_csv(domain_slice_csv, index=False)\n",
    "print({\"domain_slice_csv\": str(domain_slice_csv)})\n",
    "display(domain_slice_df.sort_values([\"domain_bin\", \"model\"]))\n",
    "\n",
    "\n",
    "# Plot: macro-F1 and ECE by domain bin\n",
    "bin_order = [\"head (>=50)\", \"torso (5-49)\", \"tail (<=4)\"]\n",
    "model_order = [\"tfidf\", \"style\", \"combined\"]\n",
    "colors = {\"tfidf\": \"#666666\", \"style\": \"#1f77b4\", \"combined\": \"#2ca02c\"}\n",
    "\n",
    "x = np.arange(len(bin_order))\n",
    "width = 0.25\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9.0, 3.0), dpi=300)\n",
    "\n",
    "for mi, model in enumerate(model_order):\n",
    "    sub = domain_slice_df[domain_slice_df[\"model\"] == model].set_index(\"domain_bin\")\n",
    "    y_f1 = [float(sub.loc[b, \"macro_f1\"]) if b in sub.index else np.nan for b in bin_order]\n",
    "    y_ece = [float(sub.loc[b, \"ece\"]) if b in sub.index else np.nan for b in bin_order]\n",
    "    axes[0].bar(x + (mi - 1) * width, y_f1, width, label=model, color=colors[model])\n",
    "    axes[1].bar(x + (mi - 1) * width, y_ece, width, label=model, color=colors[model])\n",
    "\n",
    "axes[0].set_title(\"Macro-F1 by domain frequency\")\n",
    "axes[0].set_ylabel(\"Macro-F1\")\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(bin_order, rotation=15, ha=\"right\")\n",
    "axes[0].set_ylim(0.0, 1.0)\n",
    "axes[0].grid(axis=\"y\", alpha=0.25)\n",
    "\n",
    "axes[1].set_title(\"ECE by domain frequency\")\n",
    "axes[1].set_ylabel(\"ECE\")\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(bin_order, rotation=15, ha=\"right\")\n",
    "axes[1].grid(axis=\"y\", alpha=0.25)\n",
    "\n",
    "axes[0].legend(loc=\"lower left\", frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig_path = RESULTS_DIR / \"domain_head_torso_tail_macro_f1_ece.png\"\n",
    "pdf_path = RESULTS_DIR / \"domain_head_torso_tail_macro_f1_ece.pdf\"\n",
    "fig.savefig(fig_path, dpi=300)\n",
    "fig.savefig(pdf_path)\n",
    "plt.close(fig)\n",
    "print({\"domain_head_torso_tail_plot_png\": str(fig_path)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Slice calibration: reliability diagram + ECE by theme + message-length bins\n",
    "\n",
    "# Theme normalization from v6 (topic bucket only; Link/URL removed elsewhere).\n",
    "THEME_BUCKETS = [\n",
    "    \"Finance/Crypto\",\n",
    "    \"Public health & medicine\",\n",
    "    \"Politics\",\n",
    "    \"Lifestyle & well-being\",\n",
    "    \"Crime & public safety\",\n",
    "    \"Gaming/Gambling\",\n",
    "    \"News/Information\",\n",
    "    \"Sports\",\n",
    "    \"Technology\",\n",
    "    \"Conversation/Chat/Other\",\n",
    "    \"Other theme\",\n",
    "]\n",
    "\n",
    "\n",
    "def _norm_theme(raw: object) -> Optional[str]:\n",
    "    if not isinstance(raw, str):\n",
    "        return None\n",
    "    t = raw.strip()\n",
    "    if not t:\n",
    "        return None\n",
    "    # Normalize a couple of common unicode dashes to ASCII.\n",
    "    t = t.replace(\"\\u2011\", \"-\").replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n",
    "    tl = t.lower()\n",
    "\n",
    "    if t in THEME_BUCKETS:\n",
    "        return t\n",
    "\n",
    "    if any(\n",
    "        k in tl\n",
    "        for k in [\n",
    "            \"crypto\",\n",
    "            \"token\",\n",
    "            \"coin\",\n",
    "            \"airdrop\",\n",
    "            \"ido\",\n",
    "            \"staking\",\n",
    "            \"defi\",\n",
    "            \"exchange\",\n",
    "            \"market\",\n",
    "            \"finance\",\n",
    "            \"econom\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Finance/Crypto\"\n",
    "\n",
    "    if any(\n",
    "        k in tl\n",
    "        for k in [\n",
    "            \"health\",\n",
    "            \"covid\",\n",
    "            \"vaccine\",\n",
    "            \"vaccination\",\n",
    "            \"medicine\",\n",
    "            \"medical\",\n",
    "            \"clinical\",\n",
    "            \"disease\",\n",
    "            \"pandemic\",\n",
    "            \"public health\",\n",
    "            \"hospital\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Public health & medicine\"\n",
    "\n",
    "    if any(\n",
    "        k in tl\n",
    "        for k in [\n",
    "            \"politic\",\n",
    "            \"election\",\n",
    "            \"parliament\",\n",
    "            \"congress\",\n",
    "            \"senate\",\n",
    "            \"government\",\n",
    "            \"president\",\n",
    "            \"minister\",\n",
    "            \"policy\",\n",
    "            \"war\",\n",
    "            \"conflict\",\n",
    "            \"ukraine\",\n",
    "            \"russia\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Politics\"\n",
    "\n",
    "    if any(\n",
    "        k in tl\n",
    "        for k in [\n",
    "            \"crime\",\n",
    "            \"criminal\",\n",
    "            \"terror\",\n",
    "            \"shooting\",\n",
    "            \"police\",\n",
    "            \"public safety\",\n",
    "            \"fraud\",\n",
    "            \"scam\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Crime & public safety\"\n",
    "\n",
    "    if any(k in tl for k in [\"gaming\", \"gambling\", \"casino\", \"betting\", \"lottery\", \"poker\"]):\n",
    "        return \"Gaming/Gambling\"\n",
    "\n",
    "    if any(k in tl for k in [\"sport\", \"football\", \"soccer\", \"basketball\", \"tennis\", \"nba\", \"nfl\"]):\n",
    "        return \"Sports\"\n",
    "\n",
    "    if any(\n",
    "        k in tl\n",
    "        for k in [\n",
    "            \"technology\",\n",
    "            \"tech\",\n",
    "            \"software\",\n",
    "            \"app \",\n",
    "            \"platform\",\n",
    "            \"ai \",\n",
    "            \" a.i.\",\n",
    "            \"machine learning\",\n",
    "            \"blockchain\",\n",
    "            \"internet\",\n",
    "            \"social media\",\n",
    "            \"algorithm\",\n",
    "            \"science\",\n",
    "            \"research\",\n",
    "            \"study\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Technology\"\n",
    "\n",
    "    if any(\n",
    "        k in tl\n",
    "        for k in [\n",
    "            \"lifestyle\",\n",
    "            \"well-being\",\n",
    "            \"wellbeing\",\n",
    "            \"culture\",\n",
    "            \"entertainment\",\n",
    "            \"media\",\n",
    "            \"celebrity\",\n",
    "            \"social issues\",\n",
    "            \"society\",\n",
    "            \"family\",\n",
    "            \"community\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Lifestyle & well-being\"\n",
    "\n",
    "    if any(k in tl for k in [\"news\", \"headline\", \"breaking\", \"coverage\", \"roundup\", \"update\"]):\n",
    "        return \"News/Information\"\n",
    "\n",
    "    if any(k in tl for k in [\"comment\", \"conversation\", \"chat\", \"q&a\", \"ama\", \"ask me anything\"]):\n",
    "        return \"Conversation/Chat/Other\"\n",
    "\n",
    "    return \"Other theme\"\n",
    "\n",
    "\n",
    "df_test_pred[\"theme_norm\"] = df_test_pred[\"theme\"].apply(_norm_theme)\n",
    "\n",
    "\n",
    "# Reliability diagram\n",
    "y_true = df_test_pred[\"y\"].astype(int).to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4.2, 4.0), dpi=300)\n",
    "ax.plot([0, 1], [0, 1], \"--\", color=\"black\", linewidth=1, alpha=0.6)\n",
    "\n",
    "for model_name, _, proba_col in MODELS:\n",
    "    y_proba = df_test_pred[proba_col].astype(float).to_numpy()\n",
    "    bins_df = calibration_bins(y_true, y_proba, n_bins=10)\n",
    "    ax.plot(bins_df[\"p_mean\"], bins_df[\"y_mean\"], marker=\"o\", label=model_name)\n",
    "\n",
    "ax.set_xlabel(\"Predicted probability\")\n",
    "ax.set_ylabel(\"Observed frequency\")\n",
    "ax.set_title(\"Reliability diagram (test)\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(frameon=False, loc=\"upper left\")\n",
    "fig.tight_layout()\n",
    "\n",
    "rel_png = RESULTS_DIR / \"reliability_diagram_tfidf_style_combined.png\"\n",
    "rel_pdf = RESULTS_DIR / \"reliability_diagram_tfidf_style_combined.pdf\"\n",
    "fig.savefig(rel_png, dpi=300)\n",
    "fig.savefig(rel_pdf)\n",
    "plt.close(fig)\n",
    "print({\"reliability_png\": str(rel_png)})\n",
    "\n",
    "\n",
    "# ECE by theme slice\n",
    "theme_rows = []\n",
    "for theme, g in df_test_pred.groupby(\"theme_norm\"):\n",
    "    if theme is None:\n",
    "        continue\n",
    "    y_t = g[\"y\"].astype(int).to_numpy()\n",
    "    if len(y_t) < 50:\n",
    "        continue\n",
    "    for model_name, _, proba_col in MODELS:\n",
    "        y_p = g[proba_col].astype(float).to_numpy()\n",
    "        theme_rows.append(\n",
    "            {\n",
    "                \"theme\": theme,\n",
    "                \"model\": model_name,\n",
    "                \"n\": int(len(g)),\n",
    "                \"pos_rate\": float(y_t.mean()),\n",
    "                \"ece\": float(expected_calibration_error(y_t, y_p, n_bins=10)),\n",
    "                \"brier\": float(brier_score_loss(y_t, y_p)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "theme_slice_df = pd.DataFrame(theme_rows)\n",
    "theme_slice_csv = RESULTS_DIR / \"ece_brier_by_theme.csv\"\n",
    "theme_slice_df.to_csv(theme_slice_csv, index=False)\n",
    "print({\"ece_by_theme_csv\": str(theme_slice_csv)})\n",
    "display(theme_slice_df.sort_values([\"theme\", \"model\"]))\n",
    "\n",
    "\n",
    "# Message-length bins (quartiles) for ECE\n",
    "df_test_pred[\"tok_len\"] = df_test_pred[\"message\"].astype(str).str.split().apply(len)\n",
    "df_test_pred[\"len_bin\"] = pd.qcut(df_test_pred[\"tok_len\"], q=4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"]).astype(str)\n",
    "\n",
    "len_rows = []\n",
    "for lb, g in df_test_pred.groupby(\"len_bin\"):\n",
    "    y_t = g[\"y\"].astype(int).to_numpy()\n",
    "    for model_name, _, proba_col in MODELS:\n",
    "        y_p = g[proba_col].astype(float).to_numpy()\n",
    "        len_rows.append(\n",
    "            {\n",
    "                \"len_bin\": lb,\n",
    "                \"model\": model_name,\n",
    "                \"n\": int(len(g)),\n",
    "                \"ece\": float(expected_calibration_error(y_t, y_p, n_bins=10)),\n",
    "                \"brier\": float(brier_score_loss(y_t, y_p)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "len_slice_df = pd.DataFrame(len_rows)\n",
    "len_slice_csv = RESULTS_DIR / \"ece_brier_by_length_quartile.csv\"\n",
    "len_slice_df.to_csv(len_slice_csv, index=False)\n",
    "print({\"ece_by_length_csv\": str(len_slice_csv)})\n",
    "display(len_slice_df.sort_values([\"len_bin\", \"model\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Tagger-noise robustness for style-only (no Theme)\n",
    "# We simulate LLM tagging noise by flipping entries in the multi-hot tag vector.\n",
    "\n",
    "# v6 behavior: remove Link/URL evidence tags\n",
    "DROP_LINK_URL_LABEL = True\n",
    "_LINK_URL_LABEL_NORM = \"link/url\"\n",
    "\n",
    "\n",
    "def tokenize_multi(value: object) -> List[str]:\n",
    "    if not isinstance(value, str):\n",
    "        return []\n",
    "    value = value.replace(\"+\", \",\")\n",
    "    parts = [part.strip() for part in value.split(\",\") if part.strip()]\n",
    "    if not DROP_LINK_URL_LABEL:\n",
    "        return parts\n",
    "    return [p for p in parts if \"\".join(p.lower().split()) != _LINK_URL_LABEL_NORM]\n",
    "\n",
    "\n",
    "CLAIM_BUCKETS = [\n",
    "    \"Verifiable factual statement\",\n",
    "    \"Rumour / unverified report\",\n",
    "    \"Announcement\",\n",
    "    \"Opinion / subjective statement\",\n",
    "    \"Misleading context / cherry-picking\",\n",
    "    \"Promotional hype / exaggerated profit guarantee\",\n",
    "    \"Emotional appeal / fear-mongering\",\n",
    "    \"Scarcity/FOMO tactic\",\n",
    "    \"Statistics\",\n",
    "    \"Other claim type\",\n",
    "    \"No substantive claim\",\n",
    "    \"Fake content\",\n",
    "    \"Speculative forecast / prediction\",\n",
    "    \"None / assertion only\",\n",
    "]\n",
    "\n",
    "CTA_BUCKETS = [\n",
    "    \"Visit external link / watch video\",\n",
    "    \"Engage/Ask questions\",\n",
    "    \"Join/Subscribe\",\n",
    "    \"Buy / invest / donate\",\n",
    "    \"Attend event / livestream\",\n",
    "    \"Share / repost / like\",\n",
    "    \"No CTA\",\n",
    "    \"Other CTA\",\n",
    "]\n",
    "\n",
    "EVID_BUCKETS = [\n",
    "    \"Link/URL\",\n",
    "    \"Statistics\",\n",
    "    \"Quotes/Testimony\",\n",
    "    \"Chart / price graph / TA diagram\",\n",
    "    \"Other (Evidence)\",\n",
    "    \"None / assertion only\",\n",
    "]\n",
    "\n",
    "\n",
    "def _norm_claim_labels(raw: object) -> List[str]:\n",
    "    labels = tokenize_multi(raw)\n",
    "    out: List[str] = []\n",
    "    for lbl in labels:\n",
    "        base = lbl.strip()\n",
    "        if not base:\n",
    "            continue\n",
    "        low = base.lower()\n",
    "        if base in CLAIM_BUCKETS:\n",
    "            out.append(base)\n",
    "            continue\n",
    "        if \"verifiable\" in low or \"factual\" in low:\n",
    "            out.append(\"Verifiable factual statement\")\n",
    "        elif \"rumour\" in low or \"unverified\" in low:\n",
    "            out.append(\"Rumour / unverified report\")\n",
    "        elif \"misleading context\" in low or \"cherry\" in low:\n",
    "            out.append(\"Misleading context / cherry-picking\")\n",
    "        elif \"promotional hype\" in low or \"exaggerated profit\" in low:\n",
    "            out.append(\"Promotional hype / exaggerated profit guarantee\")\n",
    "        elif \"emotional appeal\" in low or \"fear-mongering\" in low or \"fear mongering\" in low:\n",
    "            out.append(\"Emotional appeal / fear-mongering\")\n",
    "        elif \"scarcity\" in low or \"fomo\" in low:\n",
    "            out.append(\"Scarcity/FOMO tactic\")\n",
    "        elif \"statistic\" in low:\n",
    "            out.append(\"Statistics\")\n",
    "        elif \"fake content\" in low or \"fabricated\" in low:\n",
    "            out.append(\"Fake content\")\n",
    "        elif \"predict\" in low or \"forecast\" in low:\n",
    "            out.append(\"Speculative forecast / prediction\")\n",
    "        elif \"announcement\" in low:\n",
    "            out.append(\"Announcement\")\n",
    "        elif \"opinion\" in low or \"interpretive\" in low or \"analysis\" in low or \"review\" in low:\n",
    "            out.append(\"Opinion / subjective statement\")\n",
    "        elif \"none / assertion only\" in low or \"assertion only\" in low:\n",
    "            out.append(\"None / assertion only\")\n",
    "        else:\n",
    "            out.append(\"Other claim type\")\n",
    "\n",
    "    seen = set()\n",
    "    result: List[str] = []\n",
    "    for v in out:\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            result.append(v)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _norm_cta_labels(raw: object) -> List[str]:\n",
    "    labels = tokenize_multi(raw)\n",
    "    out: List[str] = []\n",
    "    for lbl in labels:\n",
    "        base = lbl.strip()\n",
    "        if not base:\n",
    "            continue\n",
    "        low = base.lower()\n",
    "\n",
    "        if base in CTA_BUCKETS:\n",
    "            out.append(base)\n",
    "            continue\n",
    "\n",
    "        if base in {\"None\", \"No CTA\"} or \"no cta\" in low:\n",
    "            out.append(\"No CTA\")\n",
    "        elif \"engage\" in low or \"ask\" in low or \"anything\" in low:\n",
    "            out.append(\"Engage/Ask questions\")\n",
    "        elif \"attend\" in low or \"event\" in low or \"livestream\" in low or \"live stream\" in low:\n",
    "            out.append(\"Attend event / livestream\")\n",
    "        elif \"join\" in low or \"subscribe\" in low or \"follow\" in low or \"whitelist\" in low:\n",
    "            out.append(\"Join/Subscribe\")\n",
    "        elif \"buy\" in low or \"invest\" in low or \"donate\" in low or \"stake\" in low or \"swap\" in low:\n",
    "            out.append(\"Buy / invest / donate\")\n",
    "        elif \"share\" in low or \"repost\" in low or \"like\" in low:\n",
    "            out.append(\"Share / repost / like\")\n",
    "        elif (\n",
    "            \"visit\" in low\n",
    "            or \"read\" in low\n",
    "            or \"watch\" in low\n",
    "            or \"link\" in low\n",
    "            or \"website\" in low\n",
    "            or \"check\" in low\n",
    "            or \"view charts\" in low\n",
    "        ):\n",
    "            out.append(\"Visit external link / watch video\")\n",
    "        else:\n",
    "            out.append(\"Other CTA\")\n",
    "\n",
    "    seen = set()\n",
    "    result: List[str] = []\n",
    "    for v in out:\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            result.append(v)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _norm_evidence_labels(raw: object) -> List[str]:\n",
    "    labels = tokenize_multi(raw)\n",
    "    out: List[str] = []\n",
    "    for lbl in labels:\n",
    "        base = lbl.strip()\n",
    "        if not base:\n",
    "            continue\n",
    "        low = base.lower()\n",
    "\n",
    "        if base in EVID_BUCKETS:\n",
    "            if base != \"Link/URL\":\n",
    "                out.append(base)\n",
    "            continue\n",
    "\n",
    "        if \"link/url\" in low or \"link\" in low or \"url\" in low:\n",
    "            continue\n",
    "        elif \"statistic\" in low:\n",
    "            out.append(\"Statistics\")\n",
    "        elif \"quote\" in low or \"testimony\" in low:\n",
    "            out.append(\"Quotes/Testimony\")\n",
    "        elif \"chart\" in low or \"graph\" in low or \"diagram\" in low:\n",
    "            out.append(\"Chart / price graph / TA diagram\")\n",
    "        elif \"none / assertion only\" in low or \"assertion only\" in low:\n",
    "            out.append(\"None / assertion only\")\n",
    "        else:\n",
    "            out.append(\"Other (Evidence)\")\n",
    "\n",
    "    seen = set()\n",
    "    result: List[str] = []\n",
    "    for v in out:\n",
    "        if v not in seen:\n",
    "            seen.add(v)\n",
    "            result.append(v)\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_style_tokens_no_theme(row: pd.Series) -> List[str]:\n",
    "    tokens: List[str] = []\n",
    "    for label in _norm_claim_labels(row.get(\"claim_types\")):\n",
    "        tokens.append(f\"claim={label}\")\n",
    "    for label in _norm_cta_labels(row.get(\"ctas\")):\n",
    "        tokens.append(f\"cta={label}\")\n",
    "    for label in _norm_evidence_labels(row.get(\"evidence\")):\n",
    "        tokens.append(f\"evid={label}\")\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def sweep_thresholds(y_true, proba, grid=None):\n",
    "    grid = grid or [round(t, 2) for t in np.linspace(0.05, 0.95, 19)]\n",
    "    best = None\n",
    "    for t in grid:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        macro_f1 = f1_score(y_true, pred, average=\"macro\")\n",
    "        if best is None or macro_f1 > best[\"macro_f1\"]:\n",
    "            best = {\"threshold\": float(t), \"macro_f1\": float(macro_f1)}\n",
    "    return best\n",
    "\n",
    "\n",
    "def fit_logreg_val_tuned(X_train, y_train, X_val, y_val, X_test, y_test, seed=0):\n",
    "    clf = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=seed,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "    thr = sweep_thresholds(y_val, val_proba)\n",
    "\n",
    "    # retrain on train+val\n",
    "    X_trainval = np.vstack([X_train, X_val])\n",
    "    y_trainval = np.concatenate([y_train, y_val])\n",
    "    clf2 = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=seed,\n",
    "    )\n",
    "    clf2.fit(X_trainval, y_trainval)\n",
    "    test_proba = clf2.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_proba >= thr[\"threshold\"]).astype(int)\n",
    "    return {\n",
    "        \"threshold\": thr[\"threshold\"],\n",
    "        \"macro_f1\": float(f1_score(y_test, test_pred, average=\"macro\")),\n",
    "        \"roc_auc\": float(roc_auc_score(y_test, test_proba)),\n",
    "        \"accuracy\": float(accuracy_score(y_test, test_pred)),\n",
    "        \"brier\": float(brier_score_loss(y_test, test_proba)),\n",
    "        \"ece\": float(expected_calibration_error(y_test, test_proba, n_bins=10)),\n",
    "    }\n",
    "\n",
    "\n",
    "# Build clean style-only matrices for the v6 main split\n",
    "train_tokens = df_train.apply(build_style_tokens_no_theme, axis=1).tolist()\n",
    "val_tokens = df_val.apply(build_style_tokens_no_theme, axis=1).tolist()\n",
    "test_tokens = df_test.apply(build_style_tokens_no_theme, axis=1).tolist()\n",
    "\n",
    "vocab = sorted(set(t for toks in (train_tokens + val_tokens) for t in toks))\n",
    "mlb = MultiLabelBinarizer(classes=vocab)\n",
    "X_train_clean = mlb.fit_transform(train_tokens).astype(np.int8)\n",
    "X_val_clean = mlb.transform(val_tokens).astype(np.int8)\n",
    "X_test_clean = mlb.transform(test_tokens).astype(np.int8)\n",
    "\n",
    "y_train = df_train[\"y\"].astype(int).to_numpy()\n",
    "y_val = df_val[\"y\"].astype(int).to_numpy()\n",
    "y_test = df_test[\"y\"].astype(int).to_numpy()\n",
    "\n",
    "print({\"style_only_no_theme_vocab\": int(len(vocab))})\n",
    "\n",
    "\n",
    "def flip_noise(X: np.ndarray, noise_rate: float, rng: np.random.Generator) -> np.ndarray:\n",
    "    if noise_rate <= 0:\n",
    "        return X\n",
    "    mask = rng.random(X.shape) < noise_rate\n",
    "    return np.logical_xor(X.astype(bool), mask).astype(np.int8)\n",
    "\n",
    "\n",
    "noise_levels = [0.0, 0.05, 0.10, 0.20]\n",
    "n_repeats = 5\n",
    "\n",
    "robust_rows = []\n",
    "for p in noise_levels:\n",
    "    for rep in range(n_repeats):\n",
    "        rng = np.random.default_rng(1000 + rep)\n",
    "        Xtr = flip_noise(X_train_clean, p, rng)\n",
    "        Xva = flip_noise(X_val_clean, p, rng)\n",
    "        Xte = flip_noise(X_test_clean, p, rng)\n",
    "        metrics = fit_logreg_val_tuned(Xtr, y_train, Xva, y_val, Xte, y_test, seed=rep)\n",
    "        metrics.update({\"noise_rate\": float(p), \"rep\": int(rep)})\n",
    "        robust_rows.append(metrics)\n",
    "\n",
    "robust_df = pd.DataFrame(robust_rows)\n",
    "robust_csv = RESULTS_DIR / \"tagger_noise_robustness_style_only_no_theme.csv\"\n",
    "robust_df.to_csv(robust_csv, index=False)\n",
    "print({\"noise_robustness_csv\": str(robust_csv)})\n",
    "\n",
    "robust_summary = robust_df.groupby(\"noise_rate\")[[\"macro_f1\", \"roc_auc\", \"ece\", \"brier\", \"accuracy\"]].agg([\n",
    "    \"mean\",\n",
    "    \"std\",\n",
    "]).round(4)\n",
    "display(robust_summary)\n",
    "\n",
    "\n",
    "# Plot robustness curves\n",
    "means = robust_df.groupby(\"noise_rate\")[[\"macro_f1\", \"roc_auc\", \"ece\"]].mean()\n",
    "stds = robust_df.groupby(\"noise_rate\")[[\"macro_f1\", \"roc_auc\", \"ece\"]].std()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4.8, 3.0), dpi=300)\n",
    "x = means.index.to_numpy(dtype=float)\n",
    "ax.errorbar(x, means[\"macro_f1\"], yerr=stds[\"macro_f1\"], marker=\"^\", label=\"Macro-F1\", color=\"black\")\n",
    "ax.errorbar(x, means[\"roc_auc\"], yerr=stds[\"roc_auc\"], marker=\"s\", label=\"ROC-AUC\", color=\"#1f77b4\")\n",
    "ax.errorbar(x, means[\"ece\"], yerr=stds[\"ece\"], marker=\"o\", label=\"ECE\", color=\"#d62728\")\n",
    "ax.set_xlabel(\"Noise rate (bit flips)\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Tagger-noise robustness (style-only, no Theme)\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(frameon=False)\n",
    "fig.tight_layout()\n",
    "\n",
    "noise_png = RESULTS_DIR / \"tagger_noise_robustness_curve.png\"\n",
    "noise_pdf = RESULTS_DIR / \"tagger_noise_robustness_curve.pdf\"\n",
    "fig.savefig(noise_png, dpi=300)\n",
    "fig.savefig(noise_pdf)\n",
    "plt.close(fig)\n",
    "print({\"noise_robustness_plot_png\": str(noise_png)})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
