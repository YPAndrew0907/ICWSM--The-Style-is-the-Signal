{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "id": "intro",
      "source": [
        "# Dynamic Tag Analysis (Qwen codebook labels)\n",
        "\n",
        "This notebook computes **temporal and channel-level dynamics** over Qwen-assigned, codebook-constrained rhetorical tags:\n",
        "- `theme_cb`\n",
        "- `claim_types_cb`\n",
        "- `ctas_cb`\n",
        "- `evidence_cb`\n",
        "\n",
        "**Important scope note:** `combined_proba` is treated as a continuous MBFC-informed credibility-risk proxy used only for **risk-weighted descriptive summaries**. This notebook does **not** implement monitoring/alerting, nor does it interpret the score as a probability of misinformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "helpers",
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import ast\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Iterable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "\n",
        "URL_PAT = re.compile(r\"(http://|https://|www\\.|t\\.me/)\", re.IGNORECASE)\n",
        "\n",
        "\n",
        "def ensure_dirs(out_dir: Path) -> None:\n",
        "    (out_dir / \"artifacts\").mkdir(parents=True, exist_ok=True)\n",
        "    (out_dir / \"tables\").mkdir(parents=True, exist_ok=True)\n",
        "    (out_dir / \"figures\").mkdir(parents=True, exist_ok=True)\n",
        "    (out_dir / \"report\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def split_multi(s: object) -> list[str]:\n",
        "    if s is None:\n",
        "        return []\n",
        "    s = str(s).strip()\n",
        "    if s == \"\" or s.lower() == \"nan\" or s.upper() == \"NA\":\n",
        "        return []\n",
        "    parts = [p.strip() for p in s.split(\",\") if p.strip()]\n",
        "    parts = [p for p in parts if p.upper() != \"NA\"]\n",
        "    return parts\n",
        "\n",
        "\n",
        "def sanitize_token(s: object) -> str:\n",
        "    s = str(s)\n",
        "    s = s.replace(\"‑\", \"-\").replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
        "    s = re.sub(r\"\\s+\", \"_\", s.strip())\n",
        "    s = re.sub(r\"[^\\w\\-/]+\", \"_\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def build_tag_doc(theme: object, claim: object, cta: object, evid: object) -> str:\n",
        "    toks: list[str] = []\n",
        "    toks.append(\"THEME=\" + sanitize_token(theme))\n",
        "    for t in split_multi(claim):\n",
        "        toks.append(\"CLAIM=\" + sanitize_token(t))\n",
        "    for t in split_multi(cta):\n",
        "        toks.append(\"CTA=\" + sanitize_token(t))\n",
        "    for t in split_multi(evid):\n",
        "        toks.append(\"EVID=\" + sanitize_token(t))\n",
        "    return \" \".join(toks)\n",
        "\n",
        "\n",
        "def shannon_entropy(p: Iterable[float]) -> float:\n",
        "    p = np.asarray(list(p), dtype=float)\n",
        "    p = p[p > 0]\n",
        "    if p.size == 0:\n",
        "        return 0.0\n",
        "    return float(-(p * np.log(p)).sum())\n",
        "\n",
        "\n",
        "def coverage_curve(counts: pd.Series, ks: Iterable[int] = (1, 5, 10, 20, 50, 100, 200, 500, 1000)) -> pd.DataFrame:\n",
        "    total = float(counts.sum())\n",
        "    out = []\n",
        "    for k in ks:\n",
        "        out.append({\"k\": int(k), \"coverage\": float(counts.head(int(k)).sum() / total) if total > 0 else 0.0})\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "\n",
        "def keep_top_k(df_counts: pd.DataFrame, tag_col: str, k: int) -> pd.DataFrame:\n",
        "    df_counts = df_counts.copy()\n",
        "    top = (\n",
        "        df_counts.groupby(tag_col, observed=True)[\"n\"]\n",
        "        .sum()\n",
        "        .sort_values(ascending=False)\n",
        "        .head(int(k))\n",
        "        .index\n",
        "    )\n",
        "    col = df_counts[tag_col]\n",
        "    if pd.api.types.is_categorical_dtype(col):\n",
        "        if \"Other\" not in col.cat.categories:\n",
        "            df_counts[tag_col] = col.cat.add_categories([\"Other\"])\n",
        "    df_counts[tag_col] = df_counts[tag_col].where(df_counts[tag_col].isin(top), other=\"Other\")\n",
        "    return df_counts\n",
        "\n",
        "\n",
        "def js_divergence(p: np.ndarray, q: np.ndarray, eps: float = 1e-12) -> float:\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    q = np.asarray(q, dtype=float)\n",
        "    p = p / p.sum() if p.sum() > 0 else p\n",
        "    q = q / q.sum() if q.sum() > 0 else q\n",
        "    p = np.clip(p, eps, 1)\n",
        "    q = np.clip(q, eps, 1)\n",
        "    return float(jensenshannon(p, q) ** 2)  # divergence (not distance)\n",
        "\n",
        "\n",
        "def herfindahl(p: np.ndarray) -> float:\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    s = p.sum()\n",
        "    if s <= 0:\n",
        "        return 0.0\n",
        "    p = p / s\n",
        "    return float((p * p).sum())\n",
        "\n",
        "\n",
        "def df_to_markdown(df: pd.DataFrame, floatfmt: str = \"{:.4f}\") -> str:\n",
        "    cols = [str(c) for c in df.columns]\n",
        "    lines = []\n",
        "    lines.append(\"| \" + \" | \".join(cols) + \" |\")\n",
        "    lines.append(\"| \" + \" | \".join([\"---\"] * len(cols)) + \" |\")\n",
        "    for _, row in df.iterrows():\n",
        "        vals = []\n",
        "        for v in row.tolist():\n",
        "            if v is None or (isinstance(v, float) and np.isnan(v)):\n",
        "                vals.append(\"\")\n",
        "            elif isinstance(v, (float, np.floating)):\n",
        "                vals.append(floatfmt.format(float(v)))\n",
        "            else:\n",
        "                vals.append(str(v))\n",
        "        lines.append(\"| \" + \" | \".join(vals) + \" |\")\n",
        "    return \"\\n\".join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "config",
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "# Note: this notebook is designed to work whether you run it from DataAppend/ OR from tag_dynamics/.\n",
        "\n",
        "ANALYSIS_START = \"2024-01-01\"\n",
        "ANALYSIS_END: str | None = None\n",
        "\n",
        "HIGH_TAIL_QUANTILE = 0.95  # descriptive tail definition\n",
        "\n",
        "# Prototype clustering parameters\n",
        "K_MIN, K_MAX, K_STEP = 10, 40, 5\n",
        "K_FINAL: int | None = 25  # set None to auto-pick best silhouette\n",
        "SILHOUETTE_SAMPLE = 5000  # avoid O(n^2) on very large prototype vocabularies\n",
        "\n",
        "# Channel profiling\n",
        "TOPN_CHANNELS = 80\n",
        "CHANNEL_K_MIN, CHANNEL_K_MAX = 5, 20\n",
        "CHANNEL_K_FINAL: int | None = 12\n",
        "\n",
        "RANDOM_SEED = 0\n",
        "\n",
        "# Recompute knobs\n",
        "FORCE_REBUILD_BASE = False\n",
        "FORCE_RECLUSTER = False\n",
        "\n",
        "\n",
        "def resolve_paths() -> tuple[Path, Path, Path]:\n",
        "    import os\n",
        "\n",
        "    cwd = Path.cwd()\n",
        "\n",
        "    override = os.environ.get(\"TAGDYN_INPUT_CSV\")\n",
        "    if override:\n",
        "        override_path = Path(override).expanduser()\n",
        "        if override_path.exists():\n",
        "            base_dir = cwd if cwd.name == \"tag_dynamics\" else cwd / \"tag_dynamics\"\n",
        "            input_csv = override_path\n",
        "            out_dir = base_dir / \"outputs\"\n",
        "            return base_dir, input_csv, out_dir\n",
        "    if (cwd / \"full_risk_v2_core_final_closed.csv\").exists():\n",
        "        base_dir = cwd / \"tag_dynamics\"\n",
        "        input_csv = cwd / \"full_risk_v2_core_final_closed.csv\"\n",
        "        out_dir = base_dir / \"outputs\"\n",
        "        return base_dir, input_csv, out_dir\n",
        "    parent_csv = (cwd / \"../full_risk_v2_core_final_closed.csv\").resolve()\n",
        "    if parent_csv.exists():\n",
        "        base_dir = cwd\n",
        "        input_csv = parent_csv\n",
        "        out_dir = base_dir / \"outputs\"\n",
        "        return base_dir, input_csv, out_dir\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not find full_risk_v2_core_final_closed.csv in current directory or parent. \"\n",
        "        \"Run from DataAppend/ or tag_dynamics/.\"\n",
        "    )\n",
        "\n",
        "\n",
        "BASE_DIR, INPUT_CSV, OUT_DIR = resolve_paths()\n",
        "ensure_dirs(OUT_DIR)\n",
        "\n",
        "BASE_DIR, INPUT_CSV, OUT_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "base-table",
      "outputs": [],
      "source": [
        "# --- Stage 0: Build/load base analysis table ---\n",
        "\n",
        "USECOLS = [\n",
        "    \"M#\",\n",
        "    \"msg_id\",\n",
        "    \"channel\",\n",
        "    \"date\",\n",
        "    \"message_original\",\n",
        "    \"combined_proba\",\n",
        "    \"filled_channel\",\n",
        "    \"filled_date\",\n",
        "    \"fuzzy_filled_channel\",\n",
        "    \"fuzzy_filled_date\",\n",
        "    \"theme_cb\",\n",
        "    \"claim_types_cb\",\n",
        "    \"ctas_cb\",\n",
        "    \"evidence_cb\",\n",
        "]\n",
        "\n",
        "\n",
        "def build_base_table(force: bool = False) -> tuple[pd.DataFrame, dict]:\n",
        "    base_path = OUT_DIR / \"artifacts\" / \"base_table.pkl.gz\"\n",
        "    meta_path = OUT_DIR / \"artifacts\" / \"base_table_meta.json\"\n",
        "\n",
        "    if base_path.exists() and not force:\n",
        "        df = pd.read_pickle(base_path)\n",
        "        meta = json.loads(meta_path.read_text(encoding=\"utf-8\")) if meta_path.exists() else {}\n",
        "        return df, meta\n",
        "\n",
        "    df = pd.read_csv(INPUT_CSV, usecols=USECOLS, low_memory=False)\n",
        "\n",
        "    df[\"channel_final\"] = df[\"filled_channel\"].combine_first(df[\"fuzzy_filled_channel\"]).combine_first(df[\"channel\"])\n",
        "    df[\"date_final_raw\"] = df[\"filled_date\"].combine_first(df[\"fuzzy_filled_date\"]).combine_first(df[\"date\"])\n",
        "    df[\"date_final\"] = pd.to_datetime(df[\"date_final_raw\"], utc=True, errors=\"coerce\", format=\"mixed\")\n",
        "\n",
        "    df = df[df[\"channel_final\"].notna() & df[\"date_final\"].notna()].copy()\n",
        "    df = df[df[\"combined_proba\"].notna()].copy()\n",
        "    df[\"combined_proba\"] = df[\"combined_proba\"].astype(\"float32\")\n",
        "\n",
        "    # Codebook tags (closed vocab)\n",
        "    for c in [\"theme_cb\", \"claim_types_cb\", \"ctas_cb\", \"evidence_cb\"]:\n",
        "        df[c] = df[c].fillna(\"NA\").astype(str)\n",
        "\n",
        "    # Time grains\n",
        "    df[\"day\"] = df[\"date_final\"].dt.floor(\"D\")\n",
        "    df[\"week\"] = df[\"date_final\"].dt.to_period(\"W\").astype(str)\n",
        "    df[\"month\"] = df[\"date_final\"].dt.to_period(\"M\").astype(str)\n",
        "    df[\"year\"] = df[\"date_final\"].dt.year.astype(\"int32\")\n",
        "\n",
        "    # Prototype (canonical tag-combo)\n",
        "    df[\"prototype\"] = (\n",
        "        df[\"theme_cb\"]\n",
        "        + \" || \"\n",
        "        + df[\"claim_types_cb\"]\n",
        "        + \" || \"\n",
        "        + df[\"ctas_cb\"]\n",
        "        + \" || \"\n",
        "        + df[\"evidence_cb\"]\n",
        "    )\n",
        "\n",
        "    # Structural metadata\n",
        "    msg_text = df[\"message_original\"].fillna(\"\").astype(str)\n",
        "    df[\"msg_len\"] = msg_text.str.len().astype(\"int32\")\n",
        "    df[\"has_url\"] = msg_text.str.contains(URL_PAT).astype(\"int8\")\n",
        "\n",
        "    # Keep only what we need for downstream (avoid carrying full text)\n",
        "    keep_cols = [\n",
        "        \"M#\",\n",
        "        \"msg_id\",\n",
        "        \"channel_final\",\n",
        "        \"date_final\",\n",
        "        \"day\",\n",
        "        \"week\",\n",
        "        \"month\",\n",
        "        \"year\",\n",
        "        \"combined_proba\",\n",
        "        \"theme_cb\",\n",
        "        \"claim_types_cb\",\n",
        "        \"ctas_cb\",\n",
        "        \"evidence_cb\",\n",
        "        \"prototype\",\n",
        "        \"msg_len\",\n",
        "        \"has_url\",\n",
        "    ]\n",
        "    df = df[keep_cols].copy()\n",
        "\n",
        "    # Reduce memory\n",
        "    for c in [\"channel_final\", \"theme_cb\", \"claim_types_cb\", \"ctas_cb\", \"evidence_cb\", \"prototype\", \"week\", \"month\"]:\n",
        "        df[c] = df[c].astype(\"category\")\n",
        "\n",
        "    df.to_pickle(base_path, compression=\"gzip\", protocol=5)\n",
        "\n",
        "    meta = {\n",
        "        \"input_csv\": str(INPUT_CSV),\n",
        "        \"rows\": int(len(df)),\n",
        "        \"channels\": int(df[\"channel_final\"].nunique()),\n",
        "        \"day_min\": str(df[\"day\"].min()),\n",
        "        \"day_max\": str(df[\"day\"].max()),\n",
        "        \"cols\": list(df.columns),\n",
        "        \"pandas_version\": pd.__version__,\n",
        "    }\n",
        "    meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    return df, meta\n",
        "\n",
        "\n",
        "df_base, base_meta = build_base_table(force=FORCE_REBUILD_BASE)\n",
        "base_meta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "window-tail",
      "outputs": [],
      "source": [
        "# --- Apply analysis window + define high-tail ---\n",
        "\n",
        "def apply_window(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    start = pd.Timestamp(ANALYSIS_START, tz=\"UTC\")\n",
        "    out = df[df[\"date_final\"] >= start].copy()\n",
        "    if ANALYSIS_END is not None:\n",
        "        end = pd.Timestamp(ANALYSIS_END, tz=\"UTC\")\n",
        "        out = out[out[\"date_final\"] < end].copy()\n",
        "    return out\n",
        "\n",
        "\n",
        "df = apply_window(df_base)\n",
        "thr = float(df[\"combined_proba\"].quantile(HIGH_TAIL_QUANTILE))\n",
        "df[\"is_high_tail\"] = (df[\"combined_proba\"] >= thr).astype(\"int8\")\n",
        "\n",
        "thr, df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "tag-audit",
      "outputs": [],
      "source": [
        "# --- Stage 1: Tag audit + high-tail lift (descriptive) ---\n",
        "\n",
        "def make_tag_table(df: pd.DataFrame, field: str, out_csv: Path, high_tail_col: str = \"is_high_tail\") -> pd.DataFrame:\n",
        "    if field == \"theme_cb\":\n",
        "        g = (\n",
        "            df.groupby(field, observed=True)\n",
        "            .agg(\n",
        "                n=(\"combined_proba\", \"size\"),\n",
        "                mean_risk=(\"combined_proba\", \"mean\"),\n",
        "                median_risk=(\"combined_proba\", \"median\"),\n",
        "                risk_mass=(\"combined_proba\", \"sum\"),\n",
        "                high_tail_rate=(high_tail_col, \"mean\"),\n",
        "            )\n",
        "            .reset_index()\n",
        "            .rename(columns={field: \"tag\"})\n",
        "        )\n",
        "    else:\n",
        "        tmp = df[[field, \"combined_proba\", high_tail_col]].copy()\n",
        "        tmp[field] = tmp[field].astype(str).apply(split_multi)\n",
        "        tmp = tmp.explode(field)\n",
        "        tmp = tmp[tmp[field].notna() & (tmp[field] != \"\")]\n",
        "        tmp[field] = tmp[field].astype(str)\n",
        "        g = (\n",
        "            tmp.groupby(field, observed=True)\n",
        "            .agg(\n",
        "                n=(\"combined_proba\", \"size\"),\n",
        "                mean_risk=(\"combined_proba\", \"mean\"),\n",
        "                median_risk=(\"combined_proba\", \"median\"),\n",
        "                risk_mass=(\"combined_proba\", \"sum\"),\n",
        "                high_tail_rate=(high_tail_col, \"mean\"),\n",
        "            )\n",
        "            .reset_index()\n",
        "            .rename(columns={field: \"tag\"})\n",
        "        )\n",
        "\n",
        "    g[\"msg_share\"] = g[\"n\"] / g[\"n\"].sum()\n",
        "    g[\"risk_mass_share\"] = g[\"risk_mass\"] / g[\"risk_mass\"].sum()\n",
        "\n",
        "    base_tail = float(df[high_tail_col].mean())\n",
        "    g[\"high_tail_lift\"] = g[\"high_tail_rate\"] / (base_tail if base_tail > 0 else np.nan)\n",
        "\n",
        "    g = g.sort_values(\"high_tail_lift\", ascending=False)\n",
        "    g.to_csv(out_csv, index=False)\n",
        "    return g\n",
        "\n",
        "\n",
        "audit = {\n",
        "    \"analysis_start\": ANALYSIS_START,\n",
        "    \"analysis_end\": ANALYSIS_END,\n",
        "    \"rows\": int(len(df)),\n",
        "    \"channels\": int(df[\"channel_final\"].nunique()),\n",
        "    \"days\": int(df[\"day\"].nunique()),\n",
        "    \"score_quantiles\": {\n",
        "        \"q50\": float(df[\"combined_proba\"].quantile(0.50)),\n",
        "        \"q90\": float(df[\"combined_proba\"].quantile(0.90)),\n",
        "        \"q95\": float(df[\"combined_proba\"].quantile(0.95)),\n",
        "        \"q99\": float(df[\"combined_proba\"].quantile(0.99)),\n",
        "        \"max\": float(df[\"combined_proba\"].max()),\n",
        "    },\n",
        "    \"high_tail_quantile\": float(HIGH_TAIL_QUANTILE),\n",
        "    \"high_tail_threshold\": float(thr),\n",
        "    \"high_tail_rate\": float(df[\"is_high_tail\"].mean()),\n",
        "}\n",
        "(OUT_DIR / \"artifacts\" / \"audit.json\").write_text(json.dumps(audit, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "tags_theme = make_tag_table(df, \"theme_cb\", OUT_DIR / \"tables\" / \"tags_theme.csv\")\n",
        "tags_claim = make_tag_table(df, \"claim_types_cb\", OUT_DIR / \"tables\" / \"tags_claim.csv\")\n",
        "tags_cta = make_tag_table(df, \"ctas_cb\", OUT_DIR / \"tables\" / \"tags_cta.csv\")\n",
        "tags_evid = make_tag_table(df, \"evidence_cb\", OUT_DIR / \"tables\" / \"tags_evidence.csv\")\n",
        "\n",
        "# Diversity over time: theme entropy per week\n",
        "theme_week = df.groupby([\"week\", \"theme_cb\"], observed=True).size().rename(\"n\").reset_index()\n",
        "total_week = df.groupby(\"week\", observed=True).size().rename(\"N\").reset_index()\n",
        "theme_week = theme_week.merge(total_week, on=\"week\")\n",
        "theme_week[\"p\"] = theme_week[\"n\"] / theme_week[\"N\"]\n",
        "ent = (\n",
        "    theme_week.groupby(\"week\", observed=True)[\"p\"]\n",
        "    .apply(lambda s: shannon_entropy(s.values))\n",
        "    .rename(\"theme_entropy\")\n",
        "    .reset_index()\n",
        ")\n",
        "ent.to_csv(OUT_DIR / \"tables\" / \"theme_entropy_by_week.csv\", index=False)\n",
        "\n",
        "tags_theme.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "tag-logodds",
      "outputs": [],
      "source": [
        "# --- Stage 1b: Tag association with high-tail via log-odds z (Dirichlet prior) ---\n",
        "\n",
        "def log_odds_dirichlet(counts_a: pd.Series, counts_b: pd.Series, alpha: pd.Series) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Monroe et al.-style informative Dirichlet prior log-odds with z-scores.\n",
        "    counts_a: counts in group A (e.g., high tail)\n",
        "    counts_b: counts in group B (rest)\n",
        "    alpha: prior pseudo-count per tag (e.g., global counts)\n",
        "    \"\"\"\n",
        "    a = counts_a.copy()\n",
        "    b = counts_b.copy()\n",
        "    al = alpha.copy()\n",
        "\n",
        "    a.index = a.index.map(str)\n",
        "    b.index = b.index.map(str)\n",
        "    al.index = al.index.map(str)\n",
        "\n",
        "    vocab = sorted(set(a.index) | set(b.index) | set(al.index))\n",
        "    a = a.reindex(vocab, fill_value=0).astype(float)\n",
        "    b = b.reindex(vocab, fill_value=0).astype(float)\n",
        "    al = al.reindex(vocab, fill_value=0.0).astype(float)\n",
        "\n",
        "    a0 = float(a.sum())\n",
        "    b0 = float(b.sum())\n",
        "    al0 = float(al.sum())\n",
        "\n",
        "    denom_a = (a0 + al0) - (a + al)\n",
        "    denom_b = (b0 + al0) - (b + al)\n",
        "\n",
        "    logit_a = np.log((a + al) / (denom_a + 1e-12))\n",
        "    logit_b = np.log((b + al) / (denom_b + 1e-12))\n",
        "    delta = logit_a - logit_b\n",
        "\n",
        "    var = 1.0 / (a + al) + 1.0 / (b + al)\n",
        "    z = delta / np.sqrt(var)\n",
        "\n",
        "    out = (\n",
        "        pd.DataFrame(\n",
        "            {\n",
        "                \"tag\": vocab,\n",
        "                \"count_high\": a.values.astype(int),\n",
        "                \"count_rest\": b.values.astype(int),\n",
        "                \"log_odds\": delta.values,\n",
        "                \"z\": z.values,\n",
        "            }\n",
        "        )\n",
        "        .sort_values(\"z\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    return out\n",
        "\n",
        "\n",
        "def tag_instance_counts(df: pd.DataFrame, field: str, group_col: str = \"is_high_tail\") -> tuple[pd.Series, pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Returns (counts_high, counts_rest, prior_alpha) for a tag field.\n",
        "    For multi-label fields, counts are over tag instances (exploded).\n",
        "    \"\"\"\n",
        "    if field == \"theme_cb\":\n",
        "        high = df[df[group_col] == 1].groupby(field, observed=True).size()\n",
        "        rest = df[df[group_col] == 0].groupby(field, observed=True).size()\n",
        "        prior = high.add(rest, fill_value=0).astype(float)\n",
        "        return high, rest, prior\n",
        "\n",
        "    tmp = df[[field, group_col]].copy()\n",
        "    tmp[field] = tmp[field].astype(str).apply(split_multi)\n",
        "    tmp = tmp.explode(field)\n",
        "    tmp = tmp[tmp[field].notna() & (tmp[field] != \"\")]\n",
        "    tmp[field] = tmp[field].astype(str)\n",
        "\n",
        "    high = tmp[tmp[group_col] == 1].groupby(field, observed=True).size()\n",
        "    rest = tmp[tmp[group_col] == 0].groupby(field, observed=True).size()\n",
        "    prior = high.add(rest, fill_value=0).astype(float)\n",
        "    return high, rest, prior\n",
        "\n",
        "\n",
        "TAG_FIELDS = [\"theme_cb\", \"claim_types_cb\", \"ctas_cb\", \"evidence_cb\"]\n",
        "logodds_base: dict[str, pd.DataFrame] = {}\n",
        "\n",
        "for field in TAG_FIELDS:\n",
        "    c_hi, c_lo, prior = tag_instance_counts(df, field)\n",
        "    lod = log_odds_dirichlet(c_hi, c_lo, alpha=prior + 1.0)  # +1 smoothing\n",
        "    lod.to_csv(OUT_DIR / \"tables\" / f\"logodds_{field}.csv\", index=False)\n",
        "    logodds_base[field] = lod\n",
        "\n",
        "print(\"Wrote log-odds tables ->\", OUT_DIR / \"tables\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "tag-sensitivity",
      "outputs": [],
      "source": [
        "# --- Stage 1c: Sensitivity to tail definition (quantiles) ---\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "TAILS = [0.90, 0.95, 0.97, 0.99]\n",
        "TOP_N = 30\n",
        "TOP_STAB = 10\n",
        "TAG_FIELDS = [\"theme_cb\", \"claim_types_cb\", \"ctas_cb\", \"evidence_cb\"]\n",
        "\n",
        "if \"logodds_base\" not in globals():\n",
        "    logodds_base = {f: pd.read_csv(OUT_DIR / \"tables\" / f\"logodds_{f}.csv\") for f in TAG_FIELDS}\n",
        "\n",
        "results = []\n",
        "stability = []\n",
        "\n",
        "for q in TAILS:\n",
        "    thr_q = float(df[\"combined_proba\"].quantile(q))\n",
        "    is_hi = (df[\"combined_proba\"] >= thr_q).astype(\"int8\")\n",
        "    n_high = int(is_hi.sum())\n",
        "    tail_rate = float(is_hi.mean())\n",
        "\n",
        "    for field in TAG_FIELDS:\n",
        "        tmp = df[[field]].copy()\n",
        "        tmp[\"is_high_tail\"] = is_hi.values\n",
        "\n",
        "        c_hi, c_lo, prior = tag_instance_counts(tmp, field, group_col=\"is_high_tail\")\n",
        "        lod = log_odds_dirichlet(c_hi, c_lo, alpha=prior + 1.0)\n",
        "        lod[\"tail_q\"] = float(q)\n",
        "        lod[\"thr\"] = float(thr_q)\n",
        "\n",
        "        top = lod.head(TOP_N).copy()\n",
        "        top[\"field\"] = field\n",
        "        top[\"n_high_msgs\"] = n_high\n",
        "        top[\"tail_rate\"] = tail_rate\n",
        "        results.append(top)\n",
        "\n",
        "        base = logodds_base[field]\n",
        "        base_topN = set(base.head(TOP_N)[\"tag\"].astype(str))\n",
        "        cur_topN = set(top[\"tag\"].astype(str))\n",
        "        unionN = base_topN | cur_topN\n",
        "        jaccN = (len(base_topN & cur_topN) / len(unionN)) if unionN else np.nan\n",
        "\n",
        "        base_top10 = set(base.head(TOP_STAB)[\"tag\"].astype(str))\n",
        "        cur_top10 = set(lod.head(TOP_STAB)[\"tag\"].astype(str))\n",
        "        union10 = base_top10 | cur_top10\n",
        "        jacc10 = (len(base_top10 & cur_top10) / len(union10)) if union10 else np.nan\n",
        "\n",
        "        z_base = base.set_index(\"tag\")[\"z\"]\n",
        "        z_cur = lod.set_index(\"tag\")[\"z\"]\n",
        "        common = z_base.index.intersection(z_cur.index)\n",
        "        if len(common) >= 2:\n",
        "            rho = spearmanr(z_base.loc[common].values, z_cur.loc[common].values).correlation\n",
        "            rho = float(rho) if rho is not None else np.nan\n",
        "        else:\n",
        "            rho = np.nan\n",
        "\n",
        "        stability.append(\n",
        "            {\n",
        "                \"field\": field,\n",
        "                \"tail_q\": float(q),\n",
        "                \"thr\": float(thr_q),\n",
        "                \"topN\": int(TOP_N),\n",
        "                \"n_high_msgs\": n_high,\n",
        "                \"tail_rate\": tail_rate,\n",
        "                \"jaccard_topN_vs_base\": float(jaccN) if not (isinstance(jaccN, float) and np.isnan(jaccN)) else np.nan,\n",
        "                \"jaccard_top10_vs_base\": float(jacc10) if not (isinstance(jacc10, float) and np.isnan(jacc10)) else np.nan,\n",
        "                \"spearman_z_vs_base\": rho,\n",
        "            }\n",
        "        )\n",
        "\n",
        "sens = pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
        "sens.to_csv(OUT_DIR / \"tables\" / \"sensitivity_logodds_top_tags.csv\", index=False)\n",
        "\n",
        "stab = pd.DataFrame(stability)\n",
        "stab.to_csv(OUT_DIR / \"tables\" / \"sensitivity_logodds_stability.csv\", index=False)\n",
        "\n",
        "print(\"Wrote sensitivity tables ->\", OUT_DIR / \"tables\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "temporal-dynamics",
      "outputs": [],
      "source": [
        "# --- Stage 2: Temporal dynamics (shares + risk-weighted shares) ---\n",
        "\n",
        "def dynamics_single(df: pd.DataFrame, time_col: str, tag_col: str, top_k: int, out_prefix: str) -> None:\n",
        "    c = df.groupby([time_col, tag_col], observed=True).size().rename(\"n\").reset_index()\n",
        "    c = keep_top_k(c, tag_col, top_k)\n",
        "    c = c.groupby([time_col, tag_col], observed=True)[\"n\"].sum().reset_index()\n",
        "\n",
        "    total = df.groupby(time_col, observed=True).size().rename(\"N\").reset_index()\n",
        "    c = c.merge(total, on=time_col)\n",
        "    c[\"share\"] = c[\"n\"] / c[\"N\"]\n",
        "    c.to_csv(OUT_DIR / \"tables\" / f\"{out_prefix}_{time_col}_share.csv\", index=False)\n",
        "\n",
        "    r = df.groupby([time_col, tag_col], observed=True)[\"combined_proba\"].sum().rename(\"risk_mass\").reset_index()\n",
        "    r = keep_top_k(r.rename(columns={\"risk_mass\": \"n\"}), tag_col, top_k).rename(columns={\"n\": \"risk_mass\"})\n",
        "    r = r.groupby([time_col, tag_col], observed=True)[\"risk_mass\"].sum().reset_index()\n",
        "    rt = df.groupby(time_col, observed=True)[\"combined_proba\"].sum().rename(\"R\").reset_index()\n",
        "    r = r.merge(rt, on=time_col)\n",
        "    r[\"risk_share\"] = r[\"risk_mass\"] / r[\"R\"]\n",
        "    r.to_csv(OUT_DIR / \"tables\" / f\"{out_prefix}_{time_col}_risk_share.csv\", index=False)\n",
        "\n",
        "\n",
        "def dynamics_multi(df: pd.DataFrame, time_col: str, field: str, top_k: int, out_prefix: str) -> None:\n",
        "    tmp = df[[time_col, field, \"combined_proba\"]].copy()\n",
        "    tmp[field] = tmp[field].astype(str).apply(split_multi)\n",
        "    tmp = tmp.explode(field)\n",
        "    tmp = tmp[tmp[field].notna() & (tmp[field] != \"\")]\n",
        "    tmp[field] = tmp[field].astype(str)\n",
        "\n",
        "    c = tmp.groupby([time_col, field], observed=True).size().rename(\"n\").reset_index().rename(columns={field: \"tag\"})\n",
        "    c = keep_top_k(c, \"tag\", top_k)\n",
        "    c = c.groupby([time_col, \"tag\"], observed=True)[\"n\"].sum().reset_index()\n",
        "    total = df.groupby(time_col, observed=True).size().rename(\"N\").reset_index()\n",
        "    c = c.merge(total, on=time_col)\n",
        "    c[\"share\"] = c[\"n\"] / c[\"N\"]\n",
        "    c.to_csv(OUT_DIR / \"tables\" / f\"{out_prefix}_{field}_{time_col}_share.csv\", index=False)\n",
        "\n",
        "    r = (\n",
        "        tmp.groupby([time_col, field], observed=True)[\"combined_proba\"]\n",
        "        .sum()\n",
        "        .rename(\"risk_mass\")\n",
        "        .reset_index()\n",
        "        .rename(columns={field: \"tag\"})\n",
        "    )\n",
        "    r = keep_top_k(r.rename(columns={\"risk_mass\": \"n\"}), \"tag\", top_k).rename(columns={\"n\": \"risk_mass\"})\n",
        "    r = r.groupby([time_col, \"tag\"], observed=True)[\"risk_mass\"].sum().reset_index()\n",
        "    rt = df.groupby(time_col, observed=True)[\"combined_proba\"].sum().rename(\"R\").reset_index()\n",
        "    r = r.merge(rt, on=time_col)\n",
        "    r[\"risk_share\"] = r[\"risk_mass\"] / r[\"R\"]\n",
        "    r.to_csv(OUT_DIR / \"tables\" / f\"{out_prefix}_{field}_{time_col}_risk_share.csv\", index=False)\n",
        "\n",
        "\n",
        "# theme dynamics\n",
        "dynamics_single(df, \"week\", \"theme_cb\", top_k=10, out_prefix=\"dyn\")\n",
        "dynamics_single(df, \"month\", \"theme_cb\", top_k=10, out_prefix=\"dyn\")\n",
        "\n",
        "# multi-label fields\n",
        "for f in [\"claim_types_cb\", \"ctas_cb\", \"evidence_cb\"]:\n",
        "    dynamics_multi(df, \"week\", f, top_k=12, out_prefix=\"dyn\")\n",
        "    dynamics_multi(df, \"month\", f, top_k=12, out_prefix=\"dyn\")\n",
        "\n",
        "print(\"Wrote dynamics tables ->\", OUT_DIR / \"tables\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "prototype-mining",
      "outputs": [],
      "source": [
        "# --- Stage 3: Prototype mining ---\n",
        "\n",
        "def build_prototypes(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    proto = (\n",
        "        df.groupby([\"prototype\", \"theme_cb\", \"claim_types_cb\", \"ctas_cb\", \"evidence_cb\"], observed=True)\n",
        "        .agg(\n",
        "            n=(\"combined_proba\", \"size\"),\n",
        "            mean_risk=(\"combined_proba\", \"mean\"),\n",
        "            median_risk=(\"combined_proba\", \"median\"),\n",
        "            risk_mass=(\"combined_proba\", \"sum\"),\n",
        "            high_tail_rate=(\"is_high_tail\", \"mean\"),\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "    proto[\"risk_mass_share\"] = proto[\"risk_mass\"] / proto[\"risk_mass\"].sum()\n",
        "    proto[\"tag_doc\"] = [\n",
        "        build_tag_doc(t, c, a, e)\n",
        "        for t, c, a, e in zip(proto[\"theme_cb\"], proto[\"claim_types_cb\"], proto[\"ctas_cb\"], proto[\"evidence_cb\"])\n",
        "    ]\n",
        "    return proto\n",
        "\n",
        "\n",
        "proto_all = build_prototypes(df).sort_values(\"n\", ascending=False)\n",
        "proto_all.to_csv(OUT_DIR / \"tables\" / \"prototypes_overall.csv\", index=False)\n",
        "\n",
        "cov_all = coverage_curve(df[\"prototype\"].value_counts())\n",
        "cov_all.to_csv(OUT_DIR / \"tables\" / \"prototype_coverage_overall.csv\", index=False)\n",
        "\n",
        "hi = df[df[\"is_high_tail\"] == 1].copy()\n",
        "proto_hi = build_prototypes(hi).sort_values(\"n\", ascending=False)\n",
        "proto_hi.to_csv(OUT_DIR / \"tables\" / \"prototypes_high_tail.csv\", index=False)\n",
        "\n",
        "cov_hi = coverage_curve(hi[\"prototype\"].value_counts())\n",
        "cov_hi.to_csv(OUT_DIR / \"tables\" / \"prototype_coverage_high_tail.csv\", index=False)\n",
        "\n",
        "# per-theme top prototypes\n",
        "top_by_theme = []\n",
        "for theme, g in df.groupby(\"theme_cb\", observed=True):\n",
        "    vc = g[\"prototype\"].value_counts().head(30)\n",
        "    for p, n in vc.items():\n",
        "        top_by_theme.append(\n",
        "            {\n",
        "                \"theme_cb\": str(theme),\n",
        "                \"prototype\": str(p),\n",
        "                \"n\": int(n),\n",
        "                \"share_within_theme\": float(n / len(g)) if len(g) else 0.0,\n",
        "            }\n",
        "        )\n",
        "pd.DataFrame(top_by_theme).to_csv(OUT_DIR / \"tables\" / \"prototypes_top_by_theme.csv\", index=False)\n",
        "\n",
        "print(\"Wrote prototype tables ->\", OUT_DIR / \"tables\")\n",
        "proto_all.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "cluster-prototypes",
      "outputs": [],
      "source": [
        "# --- Stage 4: Cluster prototypes into strategy clusters ---\n",
        "\n",
        "def pick_k(proto_count: int) -> list[int]:\n",
        "    ks = list(range(int(K_MIN), int(K_MAX) + 1, int(K_STEP)))\n",
        "    ks = [k for k in ks if 2 <= k < proto_count]\n",
        "    return ks\n",
        "\n",
        "\n",
        "def cluster_prototypes(df: pd.DataFrame, force: bool = False) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    mapping_path = OUT_DIR / \"artifacts\" / \"prototype_to_cluster.csv\"\n",
        "    base_with_clusters_path = OUT_DIR / \"artifacts\" / \"base_with_clusters.pkl.gz\"\n",
        "\n",
        "    if mapping_path.exists() and base_with_clusters_path.exists() and not force:\n",
        "        mapping = pd.read_csv(mapping_path)\n",
        "        dfc = pd.read_pickle(base_with_clusters_path)\n",
        "        return dfc, mapping\n",
        "\n",
        "    proto = (\n",
        "        df.groupby([\"prototype\", \"theme_cb\", \"claim_types_cb\", \"ctas_cb\", \"evidence_cb\"], observed=True)\n",
        "        .agg(n=(\"combined_proba\", \"size\"), mean_risk=(\"combined_proba\", \"mean\"), risk_mass=(\"combined_proba\", \"sum\"))\n",
        "        .reset_index()\n",
        "        .sort_values(\"n\", ascending=False)\n",
        "    )\n",
        "\n",
        "    proto[\"tag_doc\"] = [\n",
        "        build_tag_doc(t, c, a, e)\n",
        "        for t, c, a, e in zip(proto[\"theme_cb\"], proto[\"claim_types_cb\"], proto[\"ctas_cb\"], proto[\"evidence_cb\"])\n",
        "    ]\n",
        "    proto.to_csv(OUT_DIR / \"tables\" / \"prototype_universe.csv\", index=False)\n",
        "\n",
        "    vec = TfidfVectorizer(token_pattern=r\"(?u)\\b[\\w\\-/=]+\\b\", min_df=1)\n",
        "    X = vec.fit_transform(proto[\"tag_doc\"].values)\n",
        "\n",
        "    ks = pick_k(proto_count=X.shape[0])\n",
        "    rows = []\n",
        "    for k in ks:\n",
        "        km = KMeans(n_clusters=k, random_state=RANDOM_SEED, n_init=20)\n",
        "        lab = km.fit_predict(X)\n",
        "        sil = float(\n",
        "            silhouette_score(\n",
        "                X,\n",
        "                lab,\n",
        "                metric=\"cosine\",\n",
        "                sample_size=min(int(SILHOUETTE_SAMPLE), int(X.shape[0] - 1)) if X.shape[0] > 2 else None,\n",
        "                random_state=RANDOM_SEED,\n",
        "            )\n",
        "        )\n",
        "        rows.append({\"k\": int(k), \"silhouette_cosine\": sil, \"inertia\": float(km.inertia_)})\n",
        "    ksel = pd.DataFrame(rows)\n",
        "    ksel.to_csv(OUT_DIR / \"tables\" / \"k_selection_strategy_clusters.csv\", index=False)\n",
        "\n",
        "    if ksel.empty:\n",
        "        raise RuntimeError(f\"Not enough unique prototypes ({X.shape[0]}) to cluster.\")\n",
        "\n",
        "    if K_FINAL is None:\n",
        "        k_final = int(ksel.sort_values(\"silhouette_cosine\", ascending=False).iloc[0][\"k\"])\n",
        "    else:\n",
        "        k_final = int(K_FINAL)\n",
        "        if k_final >= X.shape[0]:\n",
        "            k_final = int(min(ksel[\"k\"].max(), X.shape[0] - 1))\n",
        "        if k_final < 2:\n",
        "            k_final = int(ksel[\"k\"].min())\n",
        "\n",
        "    km = KMeans(n_clusters=k_final, random_state=RANDOM_SEED, n_init=50)\n",
        "    proto[\"cluster\"] = km.fit_predict(X)\n",
        "\n",
        "    # Cluster signatures (top centroid tokens)\n",
        "    feats = np.array(vec.get_feature_names_out())\n",
        "    centroids = km.cluster_centers_\n",
        "    sig = []\n",
        "    for c in range(k_final):\n",
        "        top_idx = np.argsort(centroids[c])[::-1][:15]\n",
        "        sig.append({\"cluster\": int(c), \"top_tokens\": \"; \".join(feats[top_idx].tolist())})\n",
        "    pd.DataFrame(sig).to_csv(OUT_DIR / \"tables\" / \"strategy_cluster_signatures.csv\", index=False)\n",
        "\n",
        "    mapping = proto[[\"prototype\", \"cluster\"]].copy()\n",
        "    mapping.to_csv(mapping_path, index=False)\n",
        "\n",
        "    dfc = df.merge(mapping, on=\"prototype\", how=\"left\")\n",
        "    dfc.to_pickle(base_with_clusters_path, compression=\"gzip\", protocol=5)\n",
        "\n",
        "    # Cluster summary (descriptive)\n",
        "    base_tail = float(dfc[\"is_high_tail\"].mean())\n",
        "    cl = (\n",
        "        dfc.groupby(\"cluster\", observed=True)\n",
        "        .agg(\n",
        "            n_msgs=(\"combined_proba\", \"size\"),\n",
        "            mean_risk=(\"combined_proba\", \"mean\"),\n",
        "            risk_mass=(\"combined_proba\", \"sum\"),\n",
        "            high_tail_rate=(\"is_high_tail\", \"mean\"),\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "    cl[\"risk_mass_share\"] = cl[\"risk_mass\"] / cl[\"risk_mass\"].sum()\n",
        "    cl[\"high_tail_lift\"] = cl[\"high_tail_rate\"] / (base_tail if base_tail > 0 else np.nan)\n",
        "    cl = cl.sort_values(\"high_tail_lift\", ascending=False)\n",
        "    cl.to_csv(OUT_DIR / \"tables\" / \"strategy_cluster_summary.csv\", index=False)\n",
        "\n",
        "    meta = {\n",
        "        \"k_final\": int(k_final),\n",
        "        \"high_tail_quantile\": float(HIGH_TAIL_QUANTILE),\n",
        "        \"high_tail_threshold\": float(thr),\n",
        "        \"base_high_tail_rate\": float(base_tail),\n",
        "        \"silhouette_sample\": int(SILHOUETTE_SAMPLE),\n",
        "    }\n",
        "    (OUT_DIR / \"artifacts\" / \"strategy_cluster_meta.json\").write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    return dfc, mapping\n",
        "\n",
        "\n",
        "dfc, proto_cluster_map = cluster_prototypes(df, force=FORCE_RECLUSTER)\n",
        "dfc[[\"prototype\", \"cluster\"]].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "cluster-dynamics",
      "outputs": [],
      "source": [
        "# --- Stage 5: Cluster dynamics + drift ---\n",
        "\n",
        "wk = dfc.groupby([\"week\", \"cluster\"], observed=True).size().rename(\"n\").reset_index()\n",
        "wN = dfc.groupby(\"week\", observed=True).size().rename(\"N\").reset_index()\n",
        "wk = wk.merge(wN, on=\"week\")\n",
        "wk[\"share\"] = wk[\"n\"] / wk[\"N\"]\n",
        "wk.to_csv(OUT_DIR / \"tables\" / \"cluster_share_by_week.csv\", index=False)\n",
        "\n",
        "wr = (\n",
        "    dfc.groupby([\"week\", \"cluster\"], observed=True)[\"combined_proba\"]\n",
        "    .sum()\n",
        "    .rename(\"risk_mass\")\n",
        "    .reset_index()\n",
        ")\n",
        "wR = dfc.groupby(\"week\", observed=True)[\"combined_proba\"].sum().rename(\"R\").reset_index()\n",
        "wr = wr.merge(wR, on=\"week\")\n",
        "wr[\"risk_share\"] = wr[\"risk_mass\"] / wr[\"R\"]\n",
        "wr.to_csv(OUT_DIR / \"tables\" / \"cluster_risk_share_by_week.csv\", index=False)\n",
        "\n",
        "burst = wk.groupby(\"cluster\", observed=True)[\"share\"].agg([\"median\", \"max\", \"mean\", \"std\"]).reset_index()\n",
        "burst[\"max_over_median\"] = burst[\"max\"] / burst[\"median\"].replace(0, np.nan)\n",
        "burst = burst.sort_values(\"max_over_median\", ascending=False)\n",
        "burst.to_csv(OUT_DIR / \"tables\" / \"cluster_burstiness.csv\", index=False)\n",
        "\n",
        "# JS drift between consecutive weeks\n",
        "clusters = sorted([int(c) for c in dfc[\"cluster\"].dropna().unique()])\n",
        "idx = {c: i for i, c in enumerate(clusters)}\n",
        "\n",
        "week_list = sorted([str(w) for w in dfc[\"week\"].dropna().unique()])\n",
        "week_vec: dict[str, np.ndarray] = {}\n",
        "for w, g in wk.groupby(\"week\", observed=True):\n",
        "    v = np.zeros(len(clusters))\n",
        "    for c, n in zip(g[\"cluster\"].values, g[\"n\"].values):\n",
        "        v[idx[int(c)]] = n\n",
        "    week_vec[str(w)] = v\n",
        "\n",
        "drift_rows = []\n",
        "for i in range(1, len(week_list)):\n",
        "    w0, w1 = week_list[i - 1], week_list[i]\n",
        "    js = js_divergence(week_vec.get(w0, np.zeros(len(clusters))), week_vec.get(w1, np.zeros(len(clusters))))\n",
        "    drift_rows.append({\"week_prev\": w0, \"week\": w1, \"js_cluster\": js})\n",
        "pd.DataFrame(drift_rows).to_csv(OUT_DIR / \"tables\" / \"cluster_js_drift_by_week.csv\", index=False)\n",
        "\n",
        "print(\"Wrote cluster dynamics tables ->\", OUT_DIR / \"tables\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "tag-js-drift",
      "outputs": [],
      "source": [
        "# --- Stage 5b: JS drift for tag distributions (weekly) ---\n",
        "\n",
        "def weekly_dist_single(df: pd.DataFrame, time_col: str, field: str) -> dict[str, pd.Series]:\n",
        "    out: dict[str, pd.Series] = {}\n",
        "    for t, g in df.groupby(time_col, observed=True):\n",
        "        vc = g[field].astype(str).value_counts()\n",
        "        out[str(t)] = vc\n",
        "    return out\n",
        "\n",
        "\n",
        "def weekly_dist_multi(df: pd.DataFrame, time_col: str, field: str) -> dict[str, pd.Series]:\n",
        "    out: dict[str, pd.Series] = {}\n",
        "    for t, g in df.groupby(time_col, observed=True):\n",
        "        tmp = g[field].astype(str).apply(split_multi).explode()\n",
        "        tmp = tmp[tmp.notna() & (tmp != \"\")]\n",
        "        vc = tmp.astype(str).value_counts()\n",
        "        out[str(t)] = vc\n",
        "    return out\n",
        "\n",
        "\n",
        "def js_drift_from_countdict(d: dict[str, pd.Series]) -> pd.DataFrame:\n",
        "    weeks = sorted(d.keys())\n",
        "    vocab = sorted(set().union(*[set(s.index) for s in d.values() if len(s) > 0]))\n",
        "    if not vocab:\n",
        "        return pd.DataFrame(columns=[\"week_prev\", \"week\", \"js\"])\n",
        "\n",
        "    idx = {str(k): i for i, k in enumerate(vocab)}\n",
        "\n",
        "    def vec(s: pd.Series) -> np.ndarray:\n",
        "        v = np.zeros(len(vocab))\n",
        "        for k, n in s.items():\n",
        "            v[idx[str(k)]] = float(n)\n",
        "        return v\n",
        "\n",
        "    rows = []\n",
        "    for i in range(1, len(weeks)):\n",
        "        w0, w1 = weeks[i - 1], weeks[i]\n",
        "        js = js_divergence(vec(d[w0]), vec(d[w1]))\n",
        "        rows.append({\"week_prev\": w0, \"week\": w1, \"js\": js})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "TAG_FIELDS = [\"theme_cb\", \"claim_types_cb\", \"ctas_cb\", \"evidence_cb\"]\n",
        "for field in TAG_FIELDS:\n",
        "    if field == \"theme_cb\":\n",
        "        d = weekly_dist_single(dfc, \"week\", field)\n",
        "    else:\n",
        "        d = weekly_dist_multi(dfc, \"week\", field)\n",
        "    drift = js_drift_from_countdict(d)\n",
        "    drift.to_csv(OUT_DIR / \"tables\" / f\"js_drift_{field}_by_week.csv\", index=False)\n",
        "\n",
        "print(\"Wrote tag JS drift tables ->\", OUT_DIR / \"tables\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "burst-case-studies",
      "outputs": [],
      "source": [
        "# --- Stage 5c: Burst case study extraction (cluster-week exemplars) ---\n",
        "\n",
        "wk_tbl = pd.read_csv(OUT_DIR / \"tables\" / \"cluster_share_by_week.csv\")\n",
        "burst_tbl = pd.read_csv(OUT_DIR / \"tables\" / \"cluster_burstiness.csv\")\n",
        "\n",
        "topC = burst_tbl.head(5)[\"cluster\"].tolist()\n",
        "peaks = (\n",
        "    wk_tbl[wk_tbl[\"cluster\"].isin(topC)]\n",
        "    .sort_values([\"cluster\", \"share\"], ascending=[True, False])\n",
        "    .groupby(\"cluster\", as_index=False)\n",
        "    .head(1)\n",
        ")\n",
        "\n",
        "peaks.to_csv(OUT_DIR / \"tables\" / \"burst_case_study_peaks.csv\", index=False)\n",
        "\n",
        "rows = []\n",
        "for _, r in peaks.iterrows():\n",
        "    c = int(r[\"cluster\"])\n",
        "    w = str(r[\"week\"])\n",
        "    sub = dfc[(dfc[\"cluster\"] == c) & (dfc[\"week\"].astype(str) == w)]\n",
        "    top_proto = sub[\"prototype\"].value_counts().head(10)\n",
        "    for p, n in top_proto.items():\n",
        "        rows.append({\"cluster\": c, \"week\": w, \"prototype\": str(p), \"n\": int(n)})\n",
        "\n",
        "pd.DataFrame(rows).to_csv(OUT_DIR / \"tables\" / \"burst_case_study_top_prototypes.csv\", index=False)\n",
        "print(\"Wrote burst case study tables ->\", OUT_DIR / \"tables\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "channel-profiles",
      "outputs": [],
      "source": [
        "# --- Stage 6: Channel profiles (strategy ecosystems) ---\n",
        "\n",
        "# Focus on top-N channels by volume\n",
        "top_channels = (\n",
        "    dfc.groupby(\"channel_final\", observed=True)\n",
        "    .size()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(int(TOPN_CHANNELS))\n",
        "    .index\n",
        "    .tolist()\n",
        ")\n",
        "df_top = dfc[dfc[\"channel_final\"].isin(top_channels)].copy()\n",
        "\n",
        "# Channel x cluster counts\n",
        "mat = pd.crosstab(df_top[\"channel_final\"], df_top[\"cluster\"]).astype(float)\n",
        "X = normalize(mat.values, norm=\"l1\", axis=1)\n",
        "\n",
        "# Choose channel community K\n",
        "ks = [k for k in range(int(CHANNEL_K_MIN), int(CHANNEL_K_MAX) + 1) if 2 <= k < X.shape[0]]\n",
        "eval_rows = []\n",
        "for k in ks:\n",
        "    km = KMeans(n_clusters=k, random_state=RANDOM_SEED, n_init=20)\n",
        "    lab = km.fit_predict(X)\n",
        "    sil = float(silhouette_score(X, lab, metric=\"cosine\"))\n",
        "    eval_rows.append({\"k\": int(k), \"silhouette_cosine\": sil, \"inertia\": float(km.inertia_)})\n",
        "eval_df = pd.DataFrame(eval_rows)\n",
        "eval_df.to_csv(OUT_DIR / \"tables\" / \"channel_k_selection.csv\", index=False)\n",
        "\n",
        "if eval_df.empty:\n",
        "    raise RuntimeError(f\"Not enough channels ({X.shape[0]}) to cluster into communities.\")\n",
        "\n",
        "if CHANNEL_K_FINAL is None:\n",
        "    k_final = int(eval_df.sort_values(\"silhouette_cosine\", ascending=False).iloc[0][\"k\"])\n",
        "else:\n",
        "    k_final = int(CHANNEL_K_FINAL)\n",
        "    if k_final >= X.shape[0]:\n",
        "        k_final = int(min(eval_df[\"k\"].max(), X.shape[0] - 1))\n",
        "    if k_final < 2:\n",
        "        k_final = int(eval_df[\"k\"].min())\n",
        "\n",
        "km = KMeans(n_clusters=k_final, random_state=RANDOM_SEED, n_init=50)\n",
        "chan_cluster = km.fit_predict(X)\n",
        "\n",
        "# PCA for plotting\n",
        "pca = PCA(n_components=2, random_state=RANDOM_SEED)\n",
        "X2 = pca.fit_transform(X)\n",
        "\n",
        "# Persist PCA explained variance for figure labels\n",
        "pca_meta = {\n",
        "    \"pc1_var\": float(pca.explained_variance_ratio_[0]),\n",
        "    \"pc2_var\": float(pca.explained_variance_ratio_[1]),\n",
        "}\n",
        "(OUT_DIR / \"artifacts\" / \"channel_pca_meta.json\").write_text(json.dumps(pca_meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "channel_profiles = pd.DataFrame(\n",
        "    {\n",
        "        \"channel_final\": mat.index.astype(str),\n",
        "        \"community\": chan_cluster.astype(int),\n",
        "        \"pca1\": X2[:, 0],\n",
        "        \"pca2\": X2[:, 1],\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add specialization metrics + volume + mean risk\n",
        "vol = df_top.groupby(\"channel_final\", observed=True).size().rename(\"n_msgs\")\n",
        "mean_r = df_top.groupby(\"channel_final\", observed=True)[\"combined_proba\"].mean().rename(\"mean_risk\")\n",
        "channel_profiles = (\n",
        "    channel_profiles.merge(vol.reset_index().rename(columns={\"channel_final\": \"channel_final\"}), on=\"channel_final\")\n",
        "    .merge(mean_r.reset_index().rename(columns={\"channel_final\": \"channel_final\"}), on=\"channel_final\")\n",
        ")\n",
        "\n",
        "channel_profiles[\"hhi_specialization\"] = [herfindahl(row) for row in X]\n",
        "channel_profiles.to_csv(OUT_DIR / \"tables\" / \"channel_profiles.csv\", index=False)\n",
        "\n",
        "# Community mean cluster shares\n",
        "mat_share = pd.DataFrame(X, index=mat.index.astype(str), columns=[str(c) for c in mat.columns])\n",
        "mat_share[\"community\"] = chan_cluster.astype(int)\n",
        "comm = mat_share.groupby(\"community\", observed=True).mean(numeric_only=True)\n",
        "comm.to_csv(OUT_DIR / \"tables\" / \"community_mean_cluster_share.csv\")\n",
        "\n",
        "# Community x theme counts\n",
        "df_top2 = df_top.merge(channel_profiles[[\"channel_final\", \"community\"]], on=\"channel_final\", how=\"left\")\n",
        "theme_comm = df_top2.groupby([\"community\", \"theme_cb\"], observed=True).size().rename(\"n\").reset_index()\n",
        "theme_comm.to_csv(OUT_DIR / \"tables\" / \"community_theme_counts.csv\", index=False)\n",
        "\n",
        "print(\"Wrote channel profile tables ->\", OUT_DIR / \"tables\")\n",
        "channel_profiles.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "plots",
      "outputs": [],
      "source": [
        "# --- Stage 7: ICWSM-style plots (PDF+PNG) ---\n",
        "\n",
        "def savefig(path_base: Path) -> None:\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(str(path_base) + \".pdf\", bbox_inches=\"tight\", pad_inches=0.05)\n",
        "    plt.savefig(str(path_base) + \".png\", dpi=300, bbox_inches=\"tight\", pad_inches=0.05)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def week_label_to_ts(w: object) -> pd.Timestamp:\n",
        "    s = str(w)\n",
        "    # expected form: YYYY-MM-DD/YYYY-MM-DD\n",
        "    try:\n",
        "        return pd.to_datetime(s.split(\"/\")[0], errors=\"coerce\")\n",
        "    except Exception:\n",
        "        return pd.NaT\n",
        "\n",
        "\n",
        "def plot_top_series(df_in: pd.DataFrame, time_col: str, tag_col: str, value_col: str, top_k: int, title: str, xlabel: str, ylabel: str, outpath: Path) -> None:\n",
        "    order = df_in.groupby(tag_col, observed=True)[value_col].mean().sort_values(ascending=False).head(int(top_k)).index.tolist()\n",
        "    pivot = df_in[df_in[tag_col].isin(order)].pivot_table(index=time_col, columns=tag_col, values=value_col, fill_value=0.0)\n",
        "    pivot = pivot.sort_index()\n",
        "\n",
        "    # nicer x-axis for weekly strings\n",
        "    if time_col == \"week\":\n",
        "        pivot.index = [week_label_to_ts(w) for w in pivot.index]\n",
        "        pivot = pivot.sort_index()\n",
        "\n",
        "    plt.figure(figsize=(9, 4))\n",
        "    for col in pivot.columns:\n",
        "        plt.plot(pivot.index, pivot[col], label=str(col))\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.legend(ncol=2, fontsize=8)\n",
        "    savefig(outpath)\n",
        "\n",
        "\n",
        "figdir = OUT_DIR / \"figures\"\n",
        "\n",
        "# Theme share + risk-weighted share\n",
        "t1 = pd.read_csv(OUT_DIR / \"tables\" / \"dyn_week_share.csv\")\n",
        "t2 = pd.read_csv(OUT_DIR / \"tables\" / \"dyn_week_risk_share.csv\")\n",
        "\n",
        "plot_top_series(\n",
        "    t1,\n",
        "    time_col=\"week\",\n",
        "    tag_col=\"theme_cb\",\n",
        "    value_col=\"share\",\n",
        "    top_k=8,\n",
        "    title=\"Theme share over time (weekly)\",\n",
        "    xlabel=\"Week\",\n",
        "    ylabel=\"Share of messages\",\n",
        "    outpath=figdir / \"fig1_theme_share_week\",\n",
        ")\n",
        "\n",
        "plot_top_series(\n",
        "    t2,\n",
        "    time_col=\"week\",\n",
        "    tag_col=\"theme_cb\",\n",
        "    value_col=\"risk_share\",\n",
        "    top_k=8,\n",
        "    title=\"Theme risk-weighted share over time (weekly)\",\n",
        "    xlabel=\"Week\",\n",
        "    ylabel=\"Share of risk mass (Σ score)\",\n",
        "    outpath=figdir / \"fig2_theme_risk_share_week\",\n",
        ")\n",
        "\n",
        "# Claim/CTA/evidence dynamics (share)\n",
        "for field in [\"claim_types_cb\", \"ctas_cb\", \"evidence_cb\"]:\n",
        "    df_share = pd.read_csv(OUT_DIR / \"tables\" / f\"dyn_{field}_week_share.csv\")\n",
        "    plot_top_series(\n",
        "        df_share,\n",
        "        time_col=\"week\",\n",
        "        tag_col=\"tag\",\n",
        "        value_col=\"share\",\n",
        "        top_k=10,\n",
        "        title=f\"{field} share over time (weekly)\",\n",
        "        xlabel=\"Week\",\n",
        "        ylabel=\"Share of messages\",\n",
        "        outpath=figdir / f\"fig_{field}_share_week\",\n",
        "    )\n",
        "\n",
        "# Prototype coverage\n",
        "cov_all = pd.read_csv(OUT_DIR / \"tables\" / \"prototype_coverage_overall.csv\")\n",
        "cov_hi = pd.read_csv(OUT_DIR / \"tables\" / \"prototype_coverage_high_tail.csv\")\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(cov_all[\"k\"], cov_all[\"coverage\"], marker=\"o\", label=\"All messages\")\n",
        "plt.plot(cov_hi[\"k\"], cov_hi[\"coverage\"], marker=\"o\", label=f\"High-tail (q≥{HIGH_TAIL_QUANTILE})\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Top-k prototypes (log scale)\")\n",
        "plt.ylabel(\"Cumulative coverage\")\n",
        "plt.legend()\n",
        "savefig(figdir / \"fig3_prototype_coverage\")\n",
        "\n",
        "# Cluster enrichment + burstiness\n",
        "cl = pd.read_csv(OUT_DIR / \"tables\" / \"strategy_cluster_summary.csv\").head(12)\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.barh(cl[\"cluster\"].astype(str), cl[\"high_tail_lift\"].astype(float))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"High-tail lift (P(tail|cluster)/P(tail))\")\n",
        "plt.ylabel(\"Strategy cluster\")\n",
        "savefig(figdir / \"fig4_cluster_high_tail_lift\")\n",
        "\n",
        "burst = pd.read_csv(OUT_DIR / \"tables\" / \"cluster_burstiness.csv\").head(12)\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.barh(burst[\"cluster\"].astype(str), burst[\"max_over_median\"].astype(float))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Burstiness (max weekly share / median weekly share)\")\n",
        "plt.ylabel(\"Strategy cluster\")\n",
        "savefig(figdir / \"fig5_cluster_burstiness\")\n",
        "\n",
        "# Drift time series\n",
        "drift = pd.read_csv(OUT_DIR / \"tables\" / \"cluster_js_drift_by_week.csv\").sort_values(\"week\")\n",
        "x = [week_label_to_ts(w) for w in drift[\"week\"].tolist()]\n",
        "plt.figure(figsize=(9, 4))\n",
        "plt.plot(x, drift[\"js_cluster\"].astype(float))\n",
        "plt.xlabel(\"Week\")\n",
        "plt.ylabel(\"JS divergence (cluster distribution)\")\n",
        "savefig(figdir / \"fig6_cluster_js_drift\")\n",
        "\n",
        "# Channel communities in PCA space (self-explanatory legend)\n",
        "ch = pd.read_csv(OUT_DIR / \"tables\" / \"channel_profiles.csv\")\n",
        "comm_share = pd.read_csv(OUT_DIR / \"tables\" / \"community_mean_cluster_share.csv\")\n",
        "sig = pd.read_csv(OUT_DIR / \"tables\" / \"strategy_cluster_signatures.csv\")\n",
        "\n",
        "import json\n",
        "\n",
        "def _pretty_token(s: object) -> str:\n",
        "    t = str(s)\n",
        "    t = t.replace(\"___\", \" & \").replace(\"_/_\", \" / \").replace(\"__\", \", \")\n",
        "    t = t.replace(\"_\", \" \")\n",
        "    t = \" \".join(t.split())\n",
        "    return (t[:1].upper() + t[1:]) if t else t\n",
        "\n",
        "\n",
        "def _tok(label: object) -> str:\n",
        "    return sanitize_token(label).lower()\n",
        "\n",
        "\n",
        "def _extract_codebook_lists(py_path: Path):\n",
        "    try:\n",
        "        tree = ast.parse(py_path.read_text(encoding=\"utf-8\"))\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "    out = {}\n",
        "    for node in tree.body:\n",
        "        if not isinstance(node, ast.Assign) or len(node.targets) != 1 or not isinstance(node.targets[0], ast.Name):\n",
        "            continue\n",
        "        name = node.targets[0].id\n",
        "        if name not in {\"THEMES\", \"CLAIMS\", \"CTAS\", \"EVID\"}:\n",
        "            continue\n",
        "        try:\n",
        "            out[name] = ast.literal_eval(node.value)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _mk_map_from_labels(labels):\n",
        "    return {_tok(l): str(l) for l in labels}\n",
        "\n",
        "\n",
        "# Canonical display names so plot labels match the codebook\n",
        "codebook_py = BASE_DIR.parent / \"codebook_normalize_and_fill.py\"\n",
        "cb_lists = _extract_codebook_lists(codebook_py) if codebook_py.exists() else {}\n",
        "\n",
        "THEME_MAP = _mk_map_from_labels(cb_lists.get(\"THEMES\", []))\n",
        "CLAIM_MAP = _mk_map_from_labels(cb_lists.get(\"CLAIMS\", []))\n",
        "CTA_MAP = _mk_map_from_labels(cb_lists.get(\"CTAS\", []))\n",
        "EVID_MAP = _mk_map_from_labels(cb_lists.get(\"EVID\", []))\n",
        "\n",
        "# Add observed multi-theme combos / missing tags when available\n",
        "summary_path = BASE_DIR.parent / \"full_risk_v2_core_codebook_summary.json\"\n",
        "if summary_path.exists():\n",
        "    cb = json.loads(summary_path.read_text(encoding=\"utf-8\"))\n",
        "    for row in cb.get(\"theme_cb\", []):\n",
        "        if row and row[0]:\n",
        "            THEME_MAP[_tok(row[0])] = str(row[0])\n",
        "\n",
        "for m in [THEME_MAP, CLAIM_MAP, CTA_MAP, EVID_MAP]:\n",
        "    m.setdefault(_tok(\"MISSING\"), \"MISSING\")\n",
        "\n",
        "\n",
        "def _display_from_map(map_, token: object) -> str:\n",
        "    key = str(token).lower()\n",
        "    return map_.get(key, _pretty_token(token))\n",
        "\n",
        "\n",
        "def _display_theme(token: object) -> str:\n",
        "    key = str(token).lower()\n",
        "    if key in THEME_MAP:\n",
        "        return THEME_MAP[key]\n",
        "    parts = re.split(r\"(?<!_)__(?!_)\", key)\n",
        "    if len(parts) > 1:\n",
        "        return \", \".join([THEME_MAP.get(p, _pretty_token(p)) for p in parts])\n",
        "    return _pretty_token(token)\n",
        "\n",
        "\n",
        "def _pick_first(tokens, prefix, avoid=None):\n",
        "    vals = [t[len(prefix) :] for t in tokens if t.startswith(prefix)]\n",
        "    if not vals:\n",
        "        return None\n",
        "    if avoid:\n",
        "        for v in vals:\n",
        "            if v not in avoid:\n",
        "                return v\n",
        "    return vals[0]\n",
        "\n",
        "\n",
        "def strategy_cluster_label(cluster_id: int) -> str:\n",
        "    row = sig[sig[\"cluster\"] == cluster_id]\n",
        "    if row.empty:\n",
        "        return f\"Strategy {cluster_id}\"\n",
        "    tokens = [t.strip() for t in str(row.iloc[0][\"top_tokens\"]).split(\";\") if t.strip()]\n",
        "\n",
        "    theme = _pick_first(tokens, \"theme=\") or \"other__theme_\"\n",
        "    claim = (\n",
        "        _pick_first(tokens, \"claim=\", avoid={\"no_substantive_claim\"})\n",
        "        or _pick_first(tokens, \"claim=\")\n",
        "        or \"no_substantive_claim\"\n",
        "    )\n",
        "    cta = (_pick_first(tokens, \"cta=\", avoid={\"no_cta\"}) or _pick_first(tokens, \"cta=\") or \"no_cta\")\n",
        "    evid = (\n",
        "        _pick_first(tokens, \"evid=\", avoid={\"none_/_assertion_only\"})\n",
        "        or _pick_first(tokens, \"evid=\")\n",
        "        or \"none_/_assertion_only\"\n",
        "    )\n",
        "\n",
        "    theme_s = _display_theme(theme)\n",
        "    claim_s = _display_from_map(CLAIM_MAP, claim)\n",
        "    cta_s = _display_from_map(CTA_MAP, cta)\n",
        "    evid_s = _display_from_map(EVID_MAP, evid)\n",
        "\n",
        "    extras = []\n",
        "    if cta in {\"buy_/_invest_/_donate\", \"no_cta\"}:\n",
        "        extras.append(cta_s)\n",
        "    if evid in {\"statistics\", \"quotes/testimony\", \"chart_/_price_graph_/_ta_diagram\"}:\n",
        "        extras.append(evid_s)\n",
        "    extra = f\" ({', '.join(extras)})\" if extras else \"\"\n",
        "    return f\"{theme_s}: {claim_s}{extra}\"\n",
        "\n",
        "\n",
        "cluster_labels = {int(c): strategy_cluster_label(int(c)) for c in sig[\"cluster\"].unique()}\n",
        "\n",
        "comm_share = comm_share.set_index(\"community\")\n",
        "comm_share.columns = [int(c) for c in comm_share.columns]\n",
        "top2 = {}\n",
        "for cid, row in comm_share.iterrows():\n",
        "    cols = row.sort_values(ascending=False).head(2).index.tolist()\n",
        "    top2[int(cid)] = cols\n",
        "\n",
        "# Plot IDs (C1..Ck) are deterministic: largest communities first\n",
        "comm_sizes = ch.groupby(\"community\", observed=True).size().sort_values(ascending=False)\n",
        "comm_order = comm_sizes.index.astype(int).tolist()\n",
        "plot_id = {cid: i + 1 for i, cid in enumerate(comm_order)}\n",
        "\n",
        "community_labels = {}\n",
        "for cid in comm_order:\n",
        "    cs = top2.get(cid, [])\n",
        "    if len(cs) >= 2:\n",
        "        p1 = float(comm_share.loc[cid, cs[0]])\n",
        "        p2 = float(comm_share.loc[cid, cs[1]])\n",
        "        l1 = cluster_labels.get(cs[0], f\"Strategy {cs[0]}\")\n",
        "        l2 = cluster_labels.get(cs[1], f\"Strategy {cs[1]}\")\n",
        "        community_labels[cid] = f\"{l1} [{p1:.0%}] + {l2} [{p2:.0%}]\"\n",
        "    elif len(cs) == 1:\n",
        "        p1 = float(comm_share.loc[cid, cs[0]])\n",
        "        l1 = cluster_labels.get(cs[0], f\"Strategy {cs[0]}\")\n",
        "        community_labels[cid] = f\"{l1} [{p1:.0%}]\"\n",
        "    else:\n",
        "        community_labels[cid] = \"Mixed strategies\"\n",
        "\n",
        "# Save mapping for reuse\n",
        "pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"community_raw\": int(cid),\n",
        "            \"community_plot\": f\"C{plot_id[cid]}\",\n",
        "            \"n_channels\": int(comm_sizes.loc[cid]),\n",
        "            \"top_strategy_1\": int(top2.get(cid, [None, None])[0]) if top2.get(cid) else None,\n",
        "            \"top_strategy_2\": int(top2.get(cid, [None, None])[1]) if len(top2.get(cid, [])) > 1 else None,\n",
        "            \"label\": community_labels[cid],\n",
        "        }\n",
        "        for cid in comm_order\n",
        "    ]\n",
        ").to_csv(OUT_DIR / \"tables\" / \"community_labels.csv\", index=False)\n",
        "\n",
        "# --- Fig 7: Channel communities by strategy mix (paper-ready) ---\n",
        "from itertools import combinations\n",
        "\n",
        "# Paper-safe rendering defaults (TrueType embedding + readable strokes)\n",
        "plt.rcParams.update({\"pdf.fonttype\": 42, \"ps.fonttype\": 42})\n",
        "\n",
        "\n",
        "def _abbr_theme(theme: str) -> str:\n",
        "    t = theme.strip()\n",
        "    t = t.replace(\"\\u2011\", \"-\")  # non-breaking hyphen\n",
        "    low = t.lower()\n",
        "    if low.startswith(\"conversation\") or low.startswith(\"chat\"):\n",
        "        return \"Chat\"\n",
        "    if low.startswith(\"public health\") or low.startswith(\"health\"):\n",
        "        return \"Health\"\n",
        "    if low.startswith(\"finance\") or \"crypto\" in low:\n",
        "        return \"Crypto\"\n",
        "    if low.startswith(\"news\"):\n",
        "        return \"News\"\n",
        "    if low.startswith(\"politics\"):\n",
        "        return \"Pol\"\n",
        "    if low.startswith(\"lifestyle\"):\n",
        "        return \"Life\"\n",
        "    if low.startswith(\"technology\"):\n",
        "        return \"Tech\"\n",
        "    if low.startswith(\"gaming\"):\n",
        "        return \"Gaming\"\n",
        "    if low.startswith(\"sports\"):\n",
        "        return \"Sports\"\n",
        "    if low.startswith(\"other\"):\n",
        "        return \"Other\"\n",
        "    return t.split(\"/\")[0].split(\",\")[0].split(\"(\")[0].strip()[:12] or \"Other\"\n",
        "\n",
        "\n",
        "def _abbr_claim(claim: str) -> str:\n",
        "    c = claim.strip()\n",
        "    low = c.lower()\n",
        "    if low.startswith(\"announcement\"):\n",
        "        return \"Ann\"\n",
        "    if low.startswith(\"verifiable factual\"):\n",
        "        return \"Fact\"\n",
        "    if low.startswith(\"speculative forecast\") or low.startswith(\"speculative\"):\n",
        "        return \"Forecast\"\n",
        "    if low.startswith(\"promotional hype\"):\n",
        "        return \"Hype\"\n",
        "    if low.startswith(\"opinion\"):\n",
        "        return \"Opinion\"\n",
        "    if low.startswith(\"no substantive\"):\n",
        "        return \"No-claim\"\n",
        "    if low.startswith(\"rumour\"):\n",
        "        return \"Rumour\"\n",
        "    if low.startswith(\"misleading\"):\n",
        "        return \"Mislead\"\n",
        "    if low.startswith(\"emotional\"):\n",
        "        return \"Fear\"\n",
        "    if low.startswith(\"scarcity\"):\n",
        "        return \"FOMO\"\n",
        "    if low.startswith(\"other\"):\n",
        "        return \"Other\"\n",
        "    return c.split(\"/\")[0].strip()[:12] or \"Other\"\n",
        "\n",
        "\n",
        "def _abbr_extras(extras: list[str]) -> list[str]:\n",
        "    out: list[str] = []\n",
        "    for e in extras:\n",
        "        low = e.lower().strip()\n",
        "        if \"no cta\" in low and \"NoCTA\" not in out:\n",
        "            out.append(\"NoCTA\")\n",
        "        elif \"buy\" in low and \"Buy\" not in out:\n",
        "            out.append(\"Buy\")\n",
        "        elif \"statistic\" in low and \"Stat\" not in out:\n",
        "            out.append(\"Stat\")\n",
        "        elif (\"quote\" in low or \"testimony\" in low) and \"Quote\" not in out:\n",
        "            out.append(\"Quote\")\n",
        "        elif (\"chart\" in low or \"price graph\" in low or \"ta\" in low) and \"Chart\" not in out:\n",
        "            out.append(\"Chart\")\n",
        "    return out\n",
        "\n",
        "\n",
        "def strategy_label_parts(full_label: str) -> tuple[str, str, list[str]]:\n",
        "    if \":\" in full_label:\n",
        "        theme, rest = full_label.split(\":\", 1)\n",
        "    else:\n",
        "        return \"Other\", full_label.strip(), []\n",
        "    rest = rest.strip()\n",
        "    claim = rest.split(\"(\")[0].strip()\n",
        "    m = re.search(r\"\\((.*?)\\)\", rest)\n",
        "    extras = [x.strip() for x in m.group(1).split(\",\")] if m else []\n",
        "    return theme.strip(), claim.strip(), extras\n",
        "\n",
        "\n",
        "def strategy_label_short(full_label: str, include_extras: bool = True) -> str:\n",
        "    theme, claim, extras = strategy_label_parts(full_label)\n",
        "    t = _abbr_theme(theme)\n",
        "    c = _abbr_claim(claim)\n",
        "    s = f\"{t}-{c}\"\n",
        "    if include_extras:\n",
        "        ex = _abbr_extras(extras)\n",
        "        if ex:\n",
        "            s += \"[\" + \",\".join(ex) + \"]\"\n",
        "    return s\n",
        "\n",
        "\n",
        "def strategy_label_phrase(full_label: str, include_extras: bool = False) -> str:\n",
        "    theme, claim, extras = strategy_label_parts(full_label)\n",
        "    t = _abbr_theme(theme)\n",
        "    c = _abbr_claim(claim)\n",
        "    s = f\"{t} {c}\"\n",
        "    if include_extras:\n",
        "        ex = _abbr_extras(extras)\n",
        "        if ex:\n",
        "            s += \" (\" + \", \".join(ex) + \")\"\n",
        "    return s\n",
        "\n",
        "\n",
        "def community_short_label(cid: int) -> str:\n",
        "    cs = top2.get(int(cid), [])\n",
        "    if not cs:\n",
        "        return \"Mixed\"\n",
        "    t1 = int(cs[0])\n",
        "    l1_full = cluster_labels.get(t1, f\"Strategy {t1}\")\n",
        "    p1 = float(comm_share.loc[int(cid), t1])\n",
        "    s1 = strategy_label_phrase(l1_full, include_extras=False)\n",
        "    if len(cs) == 1:\n",
        "        return s1\n",
        "    t2 = int(cs[1])\n",
        "    l2_full = cluster_labels.get(t2, f\"Strategy {t2}\")\n",
        "    s2 = strategy_label_phrase(l2_full, include_extras=False)\n",
        "    if s2 == s1:\n",
        "        s2 = strategy_label_phrase(l2_full, include_extras=True)\n",
        "    # add a hint when one strategy dominates\n",
        "    if p1 >= 0.65:\n",
        "        s1 = s1 + \" (dom.)\"\n",
        "    return f\"{s1} + {s2}\"\n",
        "\n",
        "\n",
        "def _resolve_label_overlaps(fig, ax, annotations, max_iter: int = 250) -> bool:\n",
        "    fig.canvas.draw()\n",
        "    renderer = fig.canvas.get_renderer()\n",
        "    ax_bb = ax.get_window_extent(renderer)\n",
        "    px_per_pt = fig.dpi / 72.0\n",
        "    pad_px = 2.0\n",
        "    step = 0.35\n",
        "\n",
        "    for _ in range(int(max_iter)):\n",
        "        fig.canvas.draw()\n",
        "        renderer = fig.canvas.get_renderer()\n",
        "        ax_bb = ax.get_window_extent(renderer)\n",
        "        bbs = [ann.get_window_extent(renderer).expanded(1.05, 1.15) for ann in annotations]\n",
        "\n",
        "        shifts = np.zeros((len(annotations), 2), dtype=float)\n",
        "        n_ov = 0\n",
        "        for i, j in combinations(range(len(annotations)), 2):\n",
        "            if not bbs[i].overlaps(bbs[j]):\n",
        "                continue\n",
        "            n_ov += 1\n",
        "            ci = np.array([(bbs[i].x0 + bbs[i].x1) / 2.0, (bbs[i].y0 + bbs[i].y1) / 2.0])\n",
        "            cj = np.array([(bbs[j].x0 + bbs[j].x1) / 2.0, (bbs[j].y0 + bbs[j].y1) / 2.0])\n",
        "            v = ci - cj\n",
        "            if float(np.hypot(v[0], v[1])) < 1e-6:\n",
        "                v = np.array([1.0, 0.0]) if i < j else np.array([-1.0, 0.0])\n",
        "            v = v / float(np.hypot(v[0], v[1]))\n",
        "            ox = min(bbs[i].x1, bbs[j].x1) - max(bbs[i].x0, bbs[j].x0)\n",
        "            oy = min(bbs[i].y1, bbs[j].y1) - max(bbs[i].y0, bbs[j].y0)\n",
        "            mag = max(float(ox), float(oy), 1.0) + pad_px\n",
        "            shifts[i] += v * (mag / 2.0)\n",
        "            shifts[j] -= v * (mag / 2.0)\n",
        "\n",
        "        # Keep labels inside axes\n",
        "        for idx, bb in enumerate(bbs):\n",
        "            dx = 0.0\n",
        "            dy = 0.0\n",
        "            if bb.x0 < ax_bb.x0 + pad_px:\n",
        "                dx += (ax_bb.x0 + pad_px) - bb.x0\n",
        "            if bb.x1 > ax_bb.x1 - pad_px:\n",
        "                dx -= bb.x1 - (ax_bb.x1 - pad_px)\n",
        "            if bb.y0 < ax_bb.y0 + pad_px:\n",
        "                dy += (ax_bb.y0 + pad_px) - bb.y0\n",
        "            if bb.y1 > ax_bb.y1 - pad_px:\n",
        "                dy -= bb.y1 - (ax_bb.y1 - pad_px)\n",
        "            shifts[idx] += np.array([dx, dy])\n",
        "\n",
        "        if n_ov == 0:\n",
        "            return True\n",
        "\n",
        "        # Apply shifts in offset-points coordinates\n",
        "        for ann, (sx, sy) in zip(annotations, shifts):\n",
        "            if sx == 0 and sy == 0:\n",
        "                continue\n",
        "            dx_pt, dy_pt = ann.get_position()\n",
        "            ndx = float(dx_pt) + (float(sx) * step) / px_per_pt\n",
        "            ndy = float(dy_pt) + (float(sy) * step) / px_per_pt\n",
        "            # clamp so labels can't drift off\n",
        "            ndx = float(np.clip(ndx, -90.0, 90.0))\n",
        "            ndy = float(np.clip(ndy, -70.0, 70.0))\n",
        "            ann.set_position((ndx, ndy))\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# Signature table for paper (separate from the plot)\n",
        "rows = []\n",
        "for cid in comm_order:\n",
        "    cs = top2.get(cid, [])\n",
        "    if not cs:\n",
        "        continue\n",
        "    t1 = int(cs[0])\n",
        "    t2 = int(cs[1]) if len(cs) > 1 else None\n",
        "    p1 = float(comm_share.loc[cid, t1])\n",
        "    p2 = float(comm_share.loc[cid, t2]) if t2 is not None else None\n",
        "    l1_full = cluster_labels.get(t1, f\"Strategy {t1}\")\n",
        "    l2_full = cluster_labels.get(t2, f\"Strategy {t2}\") if t2 is not None else \"\"\n",
        "    rows.append(\n",
        "        {\n",
        "            \"community_id\": f\"C{plot_id[cid]}\",\n",
        "            \"community_raw\": int(cid),\n",
        "            \"n_channels\": int(comm_sizes.loc[cid]),\n",
        "            \"short_label\": community_short_label(int(cid)),\n",
        "            \"top1_cluster\": int(t1),\n",
        "            \"top1_label_full\": l1_full,\n",
        "            \"top1_label_short\": strategy_label_short(l1_full, include_extras=True),\n",
        "            \"top1_share\": float(p1),\n",
        "            \"top2_cluster\": int(t2) if t2 is not None else None,\n",
        "            \"top2_label_full\": l2_full if t2 is not None else None,\n",
        "            \"top2_label_short\": strategy_label_short(l2_full, include_extras=True) if t2 is not None else None,\n",
        "            \"top2_share\": float(p2) if p2 is not None else None,\n",
        "            \"top_strats\": (\n",
        "                f\"{strategy_label_short(l1_full, include_extras=True)} {p1:.0%}\"\n",
        "                + (f\"; {strategy_label_short(l2_full, include_extras=True)} {p2:.0%}\" if p2 is not None else \"\")\n",
        "            ),\n",
        "        }\n",
        "    )\n",
        "sig_table = pd.DataFrame(rows)\n",
        "sig_table[\"_comm_num\"] = sig_table[\"community_id\"].astype(str).str.replace(\"C\", \"\", regex=False).astype(int)\n",
        "sig_table = sig_table.sort_values(\"_comm_num\").drop(columns=[\"_comm_num\"])\n",
        "sig_table.to_csv(OUT_DIR / \"tables\" / \"community_signature_table.csv\", index=False)\n",
        "\n",
        "# Compact signature string for figure/table panels\n",
        "sig_table[\"sig_compact\"] = \"\"\n",
        "for r in sig_table.itertuples(index=False):\n",
        "    top1_full = r.top1_label_full if not pd.isna(r.top1_label_full) else \"Other: Other\"\n",
        "    s1 = strategy_label_phrase(str(top1_full), include_extras=False)\n",
        "\n",
        "    missing_top2 = pd.isna(r.top2_label_full) or pd.isna(r.top2_share)\n",
        "    if missing_top2:\n",
        "        sig_table.loc[sig_table[\"community_id\"] == r.community_id, \"sig_compact\"] = f\"{s1} {r.top1_share:.0%}\"\n",
        "        continue\n",
        "\n",
        "    top2_full = str(r.top2_label_full)\n",
        "    s2 = strategy_label_phrase(top2_full, include_extras=False)\n",
        "    if s1 == s2:\n",
        "        s1 = strategy_label_phrase(str(top1_full), include_extras=True)\n",
        "        s2 = strategy_label_phrase(top2_full, include_extras=True)\n",
        "\n",
        "    sig_table.loc[sig_table[\"community_id\"] == r.community_id, \"sig_compact\"] = (\n",
        "        f\"{s1} {r.top1_share:.0%} + {s2} {r.top2_share:.0%}\"\n",
        "    )\n",
        "\n",
        "# Read explained variance (or fall back to current PCA object if present)\n",
        "pc1_var = None\n",
        "pc2_var = None\n",
        "meta_path = OUT_DIR / \"artifacts\" / \"channel_pca_meta.json\"\n",
        "if meta_path.exists():\n",
        "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
        "    pc1_var = float(meta.get(\"pc1_var\", 0.0))\n",
        "    pc2_var = float(meta.get(\"pc2_var\", 0.0))\n",
        "elif \"pca\" in globals():\n",
        "    pc1_var = float(pca.explained_variance_ratio_[0])\n",
        "    pc2_var = float(pca.explained_variance_ratio_[1])\n",
        "\n",
        "# Grayscale-safe: marker shapes + black edges (color helps but isn't required)\n",
        "markers = [\"o\", \"s\", \"^\", \"v\", \"D\", \"P\", \"X\", \"<\", \">\", \"h\", \"p\", \"*\"]\n",
        "cmap = plt.get_cmap(\"tab20\")\n",
        "colors = [cmap(i) for i in range(len(comm_order))]\n",
        "\n",
        "# Layout switch: use \"double\" for a scatter + compact table panel (paper-ready).\n",
        "# Use \"single\" for a single-column variant (legend moved to external table in the paper).\n",
        "FIG7_MODE = \"double\"  # {\"single\", \"double\"}\n",
        "\n",
        "with plt.rc_context(\n",
        "    {\n",
        "        \"font.size\": 9,\n",
        "        \"axes.labelsize\": 9,\n",
        "        \"axes.titlesize\": 10,\n",
        "        \"xtick.labelsize\": 9,\n",
        "        \"ytick.labelsize\": 9,\n",
        "        \"pdf.fonttype\": 42,\n",
        "        \"ps.fonttype\": 42,\n",
        "    }\n",
        "):\n",
        "    if FIG7_MODE == \"single\":\n",
        "        fig = plt.figure(figsize=(3.35, 4.9))\n",
        "        gs = fig.add_gridspec(2, 1, height_ratios=[3.0, 2.0], hspace=0.15)\n",
        "        ax = fig.add_subplot(gs[0, 0])\n",
        "        ax_tbl = fig.add_subplot(gs[1, 0])\n",
        "    else:\n",
        "        fig = plt.figure(figsize=(7.0, 3.6))\n",
        "        gs = fig.add_gridspec(1, 2, width_ratios=[1.55, 1.0], wspace=0.06)\n",
        "        ax = fig.add_subplot(gs[0, 0])\n",
        "        ax_tbl = fig.add_subplot(gs[0, 1])\n",
        "\n",
        "    ax.grid(True, alpha=0.25, linewidth=0.6)\n",
        "\n",
        "    annotations = []\n",
        "    for i, cid in enumerate(comm_order):\n",
        "        sub = ch[ch[\"community\"] == cid]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        pid = plot_id[cid]\n",
        "        marker = markers[i % len(markers)]\n",
        "        color = colors[i]\n",
        "\n",
        "        ax.scatter(\n",
        "            sub[\"pca1\"],\n",
        "            sub[\"pca2\"],\n",
        "            s=44,\n",
        "            marker=marker,\n",
        "            alpha=0.88,\n",
        "            facecolor=color,\n",
        "            edgecolor=\"black\",\n",
        "            linewidth=0.6,\n",
        "            zorder=2,\n",
        "        )\n",
        "\n",
        "        cx, cy = float(sub[\"pca1\"].mean()), float(sub[\"pca2\"].mean())\n",
        "        ann = ax.annotate(\n",
        "            f\"C{pid}\",\n",
        "            xy=(cx, cy),\n",
        "            xycoords=\"data\",\n",
        "            xytext=(0, 0),\n",
        "            textcoords=\"offset points\",\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            fontsize=10,\n",
        "            weight=\"bold\",\n",
        "            bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"black\", lw=0.9, alpha=0.98),\n",
        "            arrowprops=dict(arrowstyle=\"-\", color=\"0.35\", lw=0.8, shrinkA=4, shrinkB=4),\n",
        "            zorder=5,\n",
        "        )\n",
        "        annotations.append(ann)\n",
        "\n",
        "    ax.set_xlabel(f\"PC1 ({pc1_var*100:.1f}% var.)\" if pc1_var is not None else \"PC1\")\n",
        "    ax.set_ylabel(f\"PC2 ({pc2_var*100:.1f}% var.)\" if pc2_var is not None else \"PC2\")\n",
        "    ax.set_title(\"Channel communities by strategy mix\", pad=3)\n",
        "\n",
        "    # Run overlap resolution after final layout so the bbox coordinates match export\n",
        "    fig.tight_layout()\n",
        "    ok = _resolve_label_overlaps(fig, ax, annotations, max_iter=500)\n",
        "    if not ok:\n",
        "        print(\"[fig7] Warning: label overlap solver reached max_iter; consider FIG7_MODE='double' or increasing figsize.\")\n",
        "\n",
        "    # Compact signature panel: marker + C# (n) + top-2 strategy mix\n",
        "    ax_tbl.axis(\"off\")\n",
        "    ax_tbl.set_xlim(0, 1)\n",
        "    ax_tbl.set_ylim(0, 1)\n",
        "    # Column anchors (axes coords)\n",
        "    X_MARK = 0.04\n",
        "    X_COMM = 0.20\n",
        "    X_N = 0.32\n",
        "    X_SIG = 0.38\n",
        "    ax_tbl.text(X_COMM, 0.98, \"Comm\", ha=\"right\", va=\"top\", fontsize=9, weight=\"bold\", transform=ax_tbl.transAxes)\n",
        "    ax_tbl.text(X_N, 0.98, \"n\", ha=\"right\", va=\"top\", fontsize=9, weight=\"bold\", transform=ax_tbl.transAxes)\n",
        "    ax_tbl.text(\n",
        "        X_SIG,\n",
        "        0.98,\n",
        "        \"Signature (top 2 strategies; mean share)\",\n",
        "        ha=\"left\",\n",
        "        va=\"top\",\n",
        "        fontsize=9,\n",
        "        weight=\"bold\",\n",
        "        transform=ax_tbl.transAxes,\n",
        "    )\n",
        "    ax_tbl.plot([0.02, 0.98], [0.94, 0.94], color=\"0.8\", lw=0.8, transform=ax_tbl.transAxes, clip_on=False)\n",
        "\n",
        "    # Ensure table rows are ordered C1..Ck\n",
        "    sig_tbl = sig_table.copy()\n",
        "    sig_tbl[\"_n\"] = sig_tbl[\"community_id\"].astype(str).str.replace(\"C\", \"\", regex=False).astype(int)\n",
        "    sig_tbl = sig_tbl.sort_values(\"_n\").drop(columns=[\"_n\"])\n",
        "\n",
        "    y = 0.90\n",
        "    dy = 0.072 if FIG7_MODE == \"double\" else 0.078\n",
        "    for r in sig_tbl.itertuples(index=False):\n",
        "        raw = int(r.community_raw)\n",
        "        idx = comm_order.index(raw)\n",
        "        marker = markers[idx % len(markers)]\n",
        "        color = colors[idx]\n",
        "\n",
        "        ax_tbl.scatter(\n",
        "            [X_MARK],\n",
        "            [y],\n",
        "            s=48,\n",
        "            marker=marker,\n",
        "            facecolor=color,\n",
        "            edgecolor=\"black\",\n",
        "            linewidth=0.6,\n",
        "            transform=ax_tbl.transAxes,\n",
        "            clip_on=False,\n",
        "            zorder=3,\n",
        "        )\n",
        "        ax_tbl.text(\n",
        "            X_COMM,\n",
        "            y,\n",
        "            f\"{r.community_id}\",\n",
        "            ha=\"right\",\n",
        "            va=\"center\",\n",
        "            fontsize=9,\n",
        "            weight=\"bold\",\n",
        "            transform=ax_tbl.transAxes,\n",
        "        )\n",
        "        ax_tbl.text(\n",
        "            X_N,\n",
        "            y,\n",
        "            f\"{int(r.n_channels)}\",\n",
        "            ha=\"right\",\n",
        "            va=\"center\",\n",
        "            fontsize=9,\n",
        "            transform=ax_tbl.transAxes,\n",
        "        )\n",
        "        ax_tbl.text(\n",
        "            X_SIG,\n",
        "            y,\n",
        "            str(r.sig_compact),\n",
        "            ha=\"left\",\n",
        "            va=\"center\",\n",
        "            fontsize=9,\n",
        "            transform=ax_tbl.transAxes,\n",
        "        )\n",
        "        y -= dy\n",
        "\n",
        "    out_base = figdir / \"fig7_channel_communities_pca\"\n",
        "    fig.savefig(str(out_base) + \".pdf\", bbox_inches=\"tight\", pad_inches=0.05)\n",
        "    fig.savefig(str(out_base) + \".png\", dpi=300, bbox_inches=\"tight\", pad_inches=0.05)\n",
        "    plt.close(fig)\n",
        "\n",
        "# Community x cluster heatmap\n",
        "comm = pd.read_csv(OUT_DIR / \"tables\" / \"community_mean_cluster_share.csv\", index_col=0)\n",
        "top_cols = comm.mean(axis=0).sort_values(ascending=False).head(15).index\n",
        "comm = comm[top_cols].copy()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(comm.values, aspect=\"auto\")\n",
        "plt.yticks(range(len(comm.index)), [str(i) for i in comm.index])\n",
        "plt.xticks(range(len(comm.columns)), [str(c) for c in comm.columns], rotation=90)\n",
        "plt.colorbar(label=\"Mean share\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Channel community\")\n",
        "savefig(figdir / \"fig8_community_cluster_heatmap\")\n",
        "\n",
        "print(\"Wrote figures ->\", figdir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "id": "report",
      "outputs": [],
      "source": [
        "# --- Stage 8: Report scaffold (Markdown, no tabulate dependency) ---\n",
        "\n",
        "audit = json.loads((OUT_DIR / \"artifacts\" / \"audit.json\").read_text(encoding=\"utf-8\"))\n",
        "base_meta = json.loads((OUT_DIR / \"artifacts\" / \"base_table_meta.json\").read_text(encoding=\"utf-8\"))\n",
        "clmeta = json.loads((OUT_DIR / \"artifacts\" / \"strategy_cluster_meta.json\").read_text(encoding=\"utf-8\"))\n",
        "\n",
        "lod_theme_path = OUT_DIR / \"tables\" / \"logodds_theme_cb.csv\"\n",
        "lod_claim_path = OUT_DIR / \"tables\" / \"logodds_claim_types_cb.csv\"\n",
        "lod_cta_path = OUT_DIR / \"tables\" / \"logodds_ctas_cb.csv\"\n",
        "lod_evid_path = OUT_DIR / \"tables\" / \"logodds_evidence_cb.csv\"\n",
        "stab_path = OUT_DIR / \"tables\" / \"sensitivity_logodds_stability.csv\"\n",
        "\n",
        "lod_theme = pd.read_csv(lod_theme_path).head(12) if lod_theme_path.exists() else pd.DataFrame()\n",
        "lod_claim = pd.read_csv(lod_claim_path).head(12) if lod_claim_path.exists() else pd.DataFrame()\n",
        "lod_cta = pd.read_csv(lod_cta_path).head(12) if lod_cta_path.exists() else pd.DataFrame()\n",
        "lod_evid = pd.read_csv(lod_evid_path).head(12) if lod_evid_path.exists() else pd.DataFrame()\n",
        "stab = pd.read_csv(stab_path) if stab_path.exists() else pd.DataFrame()\n",
        "\n",
        "clsum = pd.read_csv(OUT_DIR / \"tables\" / \"strategy_cluster_summary.csv\").head(12)\n",
        "sig = pd.read_csv(OUT_DIR / \"tables\" / \"strategy_cluster_signatures.csv\").head(20)\n",
        "\n",
        "md: list[str] = []\n",
        "md.append(\"# Dynamic Tag Analysis (Qwen codebook labels)\\n\")\n",
        "md.append(\"## Scope and disclaimers\\n\")\n",
        "md.append(\n",
        "    \"This section analyzes the *temporal and channel-level dynamics* of Qwen-assigned, codebook-constrained rhetorical tags \"\n",
        "    \"(theme, claim/framing, CTA, evidence). The continuous score `combined_proba` is treated as an MBFC-informed credibility-risk proxy \"\n",
        "    \"used only for *risk-weighted descriptive summaries*. We do **not** interpret the score as a probability of misinformation or factual falsity.\\n\"\n",
        ")\n",
        "\n",
        "md.append(\"## Data window\\n\")\n",
        "md.append(f\"- Analysis window start: **{audit['analysis_start']}**\\n\")\n",
        "md.append(f\"- Messages in window: **{audit['rows']}**\\n\")\n",
        "md.append(f\"- Channels in window: **{audit['channels']}**\\n\")\n",
        "md.append(f\"- Days in window: **{audit['days']}**\\n\")\n",
        "md.append(\n",
        "    \"- Score quantiles: \"\n",
        "    f\"q50={audit['score_quantiles']['q50']:.4f}, \"\n",
        "    f\"q95={audit['score_quantiles']['q95']:.4f}, \"\n",
        "    f\"q99={audit['score_quantiles']['q99']:.4f}\\n\"\n",
        ")\n",
        "md.append(\n",
        "    f\"- High-tail definition (descriptive): top **{audit['high_tail_quantile']:.2f}**, \"\n",
        "    f\"threshold={audit['high_tail_threshold']:.6f}\\n\"\n",
        ")\n",
        "\n",
        "md.append(\"\\n## High-tail association (descriptive)\\n\")\n",
        "md.append(\n",
        "    \"We quantify tag association with the high-score tail using Monroe et al.-style log-odds with an informative Dirichlet prior \"\n",
        "    \"(reported as log-odds and z-scores). This is descriptive (not causal).\\n\"\n",
        ")\n",
        "\n",
        "md.append(\"\\nTop themes by log-odds z (high tail vs rest):\\n\")\n",
        "if not lod_theme.empty:\n",
        "    md.append(df_to_markdown(lod_theme[[\"tag\", \"count_high\", \"count_rest\", \"log_odds\", \"z\"]]))\n",
        "else:\n",
        "    md.append(\"(Missing: outputs/tables/logodds_theme_cb.csv)\\n\")\n",
        "\n",
        "md.append(\"\\n\\nTop claim/framing tags by log-odds z:\\n\")\n",
        "if not lod_claim.empty:\n",
        "    md.append(df_to_markdown(lod_claim[[\"tag\", \"count_high\", \"count_rest\", \"log_odds\", \"z\"]]))\n",
        "else:\n",
        "    md.append(\"(Missing: outputs/tables/logodds_claim_types_cb.csv)\\n\")\n",
        "\n",
        "md.append(\"\\n\\nTop CTA tags by log-odds z:\\n\")\n",
        "if not lod_cta.empty:\n",
        "    md.append(df_to_markdown(lod_cta[[\"tag\", \"count_high\", \"count_rest\", \"log_odds\", \"z\"]]))\n",
        "else:\n",
        "    md.append(\"(Missing: outputs/tables/logodds_ctas_cb.csv)\\n\")\n",
        "\n",
        "md.append(\"\\n\\nTop evidence tags by log-odds z:\\n\")\n",
        "if not lod_evid.empty:\n",
        "    md.append(df_to_markdown(lod_evid[[\"tag\", \"count_high\", \"count_rest\", \"log_odds\", \"z\"]]))\n",
        "else:\n",
        "    md.append(\"(Missing: outputs/tables/logodds_evidence_cb.csv)\\n\")\n",
        "\n",
        "md.append(\"\\n## Sensitivity to tail definition\\n\")\n",
        "md.append(\n",
        "    \"We recompute log-odds at multiple tail quantiles (q ∈ {0.90, 0.95, 0.97, 0.99}). \"\n",
        "    \"The table reports stability vs the baseline tail (the notebook's `HIGH_TAIL_QUANTILE`) using Jaccard overlap of top-N tags and \"\n",
        "    \"Spearman correlation of z-scores.\\n\"\n",
        ")\n",
        "if not stab.empty:\n",
        "    md.append(\n",
        "        df_to_markdown(\n",
        "            stab[[\n",
        "                \"field\",\n",
        "                \"tail_q\",\n",
        "                \"thr\",\n",
        "                \"n_high_msgs\",\n",
        "                \"tail_rate\",\n",
        "                \"topN\",\n",
        "                \"jaccard_top10_vs_base\",\n",
        "                \"jaccard_topN_vs_base\",\n",
        "                \"spearman_z_vs_base\",\n",
        "            ]]\n",
        "        )\n",
        "    )\n",
        "else:\n",
        "    md.append(\"(Missing: outputs/tables/sensitivity_logodds_stability.csv)\\n\")\n",
        "\n",
        "md.append(\"\\n## Drift and burst case studies\\n\")\n",
        "md.append(\"- Cluster JS drift: outputs/tables/cluster_js_drift_by_week.csv\\n\")\n",
        "md.append(\"- Tag JS drift (weekly): outputs/tables/js_drift_{theme_cb,claim_types_cb,ctas_cb,evidence_cb}_by_week.csv\\n\")\n",
        "md.append(\"- Burst peaks: outputs/tables/burst_case_study_peaks.csv\\n\")\n",
        "md.append(\"- Burst prototypes: outputs/tables/burst_case_study_top_prototypes.csv\\n\")\n",
        "\n",
        "md.append(\"\\n## Strategy prototypes and clustering\\n\")\n",
        "md.append(f\"- Strategy clusters (k-means over prototype tag tokens): **k={clmeta['k_final']}**\\n\")\n",
        "md.append(\"Top clusters by high-tail lift:\\n\")\n",
        "md.append(df_to_markdown(clsum[[\"cluster\", \"n_msgs\", \"mean_risk\", \"high_tail_lift\", \"risk_mass_share\"]]))\n",
        "\n",
        "md.append(\"\\n\\nCluster signatures (top tokens per cluster):\\n\")\n",
        "md.append(df_to_markdown(sig[[\"cluster\", \"top_tokens\"]], floatfmt=\"{:.4f}\"))\n",
        "\n",
        "md.append(\"\\n## Figures produced\\n\")\n",
        "md.append(\"- outputs/figures/fig1_theme_share_week.{pdf,png}\\n\")\n",
        "md.append(\"- outputs/figures/fig2_theme_risk_share_week.{pdf,png}\\n\")\n",
        "md.append(\"- outputs/figures/fig3_prototype_coverage.{pdf,png}\\n\")\n",
        "md.append(\"- outputs/figures/fig4_cluster_high_tail_lift.{pdf,png}\\n\")\n",
        "md.append(\"- outputs/figures/fig5_cluster_burstiness.{pdf,png}\\n\")\n",
        "md.append(\"- outputs/figures/fig6_cluster_js_drift.{pdf,png}\\n\")\n",
        "md.append(\"- outputs/figures/fig7_channel_communities_pca.{pdf,png}\\n\")\n",
        "md.append(\"- outputs/figures/fig8_community_cluster_heatmap.{pdf,png}\\n\")\n",
        "\n",
        "out_md = OUT_DIR / \"report\" / \"dynamic_tag_analysis_section.md\"\n",
        "out_md.write_text(\"\\n\".join(md), encoding=\"utf-8\")\n",
        "\n",
        "out_md\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
