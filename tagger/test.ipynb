{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbda125",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source /etc/network_turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8565af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "cache_dir = \"/root/autodl-tmp/torch_cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True) # 确保目录存在\n",
    "os.environ[\"TORCH_INDUCTOR_CACHE_DIR\"] = cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fdfbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- 1. 设置和加载模型 ---\n",
    "\n",
    "\n",
    "# 测试数据集的路径\n",
    "test_data_path = \"/root/test_prompts.json\"\n",
    "\n",
    "# 加载gptoss模型和分词器\n",
    "# 必须从保存的目录加载，这样它会自动应用LoRA适配器\n",
    "print(\"Loading fine-tuned model and tokenizer...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载主流模型和分词器\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "model_name = \"/root/autodl-tmp\"\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, \"/root/autodl-tmp/qwen-qlora-split2\", torch_dtype=torch.bfloat16)\n",
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe895eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Loading test data from {test_data_path}...\")\n",
    "with open(test_data_path, 'r', encoding='utf-8') as f:\n",
    "    test_samples = json.load(f)\n",
    "print(f\"Found {len(test_samples)} samples in the test set.\")\n",
    "\n",
    "\n",
    "# --- 3. 辅助函数：用于解析标签 ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_labels(text_block: str) -> dict:\n",
    "    \"\"\"\n",
    "    从模型的生成文本或真实标签文本中解析出Q1-Q4的标签。\n",
    "    \"\"\"\n",
    "    parsed = {}\n",
    "    lines = text_block.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        # 跳过消息ID行，如 'M61'\n",
    "        if line.lower().startswith('m') and line[1:].isdigit():\n",
    "            continue\n",
    "        \n",
    "        # 分割键和值，例如 \"theme: Finance/Crypto\"\n",
    "        parts = line.split(':', 1)\n",
    "        if len(parts) == 2:\n",
    "            key = parts[0].strip()\n",
    "            value = parts[1].strip()\n",
    "            # 兼容 claim_types, ctas, evidence 等键\n",
    "            if key in [\"theme\", \"claim_types\", \"ctas\", \"evidence\"]:\n",
    "                parsed[key] = value\n",
    "    return parsed\n",
    "\n",
    "# --- 4. 执行评估循环 ---\n",
    "\n",
    "results = []\n",
    "correct_counts = {\"theme\": 0, \"claim_types\": 0, \"ctas\": 0, \"evidence\": 0, \"all\": 0}\n",
    "total_samples = len(test_samples)\n",
    "\n",
    "# 使用tqdm来显示进度条\n",
    "for sample in tqdm(test_samples, desc=\"Evaluating\"):\n",
    "    # 提取 prompt (system + user message)\n",
    "    prompt_messages = sample['message'][:-1] # 去掉 assistant 的部分\n",
    "    \n",
    "    # 使用 apply_chat_template 来格式化输入，确保与训练时一致\n",
    "    # add_generation_prompt=True 会在末尾添加 assistant 提示符，引导模型生成\n",
    "    inputs = tokenizer.apply_chat_template(prompt_messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    print(inputs.shape)\n",
    "    # 生成预测\n",
    "    # 使用 torch.no_grad() 来禁用梯度计算，加速推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=128,  # 设置一个合理的长度，防止生成过长\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False, # 使用确定性生成，关闭采样\n",
    "        )\n",
    "    \n",
    "    # 解码生成的 token IDs，并跳过输入的 prompt部分\n",
    "    response_ids = outputs[0][len(inputs[0]):]\n",
    "    generated_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 提取并解析真实标签\n",
    "    ground_truth_text = sample['message'][-1]['content']\n",
    "    print(ground_truth_text)\n",
    "    # 解析预测和真实标签\n",
    "    parsed_prediction = parse_labels(generated_text)\n",
    "    parsed_ground_truth = parse_labels(ground_truth_text)\n",
    "\n",
    "    # --- 5. 比较结果 ---\n",
    "    is_theme_correct = parsed_prediction.get(\"theme\") == parsed_ground_truth.get(\"theme\")\n",
    "    is_claim_types_correct = parsed_prediction.get(\"claim_types\") == parsed_ground_truth.get(\"claim_types\")\n",
    "    is_ctas_correct = parsed_prediction.get(\"ctas\") == parsed_ground_truth.get(\"ctas\")\n",
    "    is_evidence_correct = parsed_prediction.get(\"evidence\") == parsed_ground_truth.get(\"evidence\")\n",
    "\n",
    "    # 更新计数\n",
    "    if is_theme_correct: correct_counts[\"theme\"] += 1\n",
    "    if is_claim_types_correct: correct_counts[\"claim_types\"] += 1\n",
    "    if is_ctas_correct: correct_counts[\"ctas\"] += 1\n",
    "    if is_evidence_correct: correct_counts[\"evidence\"] += 1\n",
    "        \n",
    "    is_all_correct = is_theme_correct and is_claim_types_correct and is_ctas_correct and is_evidence_correct\n",
    "    if is_all_correct:\n",
    "        correct_counts[\"all\"] += 1\n",
    "\n",
    "    # 保存详细结果以便后续分析\n",
    "    results.append({\n",
    "        \"message_id\": sample['message_id'],\n",
    "        \"prompt\": tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True),\n",
    "        \"generated_text\": generated_text.strip(),\n",
    "        \"ground_truth_text\": ground_truth_text.strip(),\n",
    "        \"parsed_prediction\": parsed_prediction,\n",
    "        \"parsed_ground_truth\": parsed_ground_truth,\n",
    "        \"is_all_correct\": is_all_correct\n",
    "    })\n",
    "\n",
    "# --- 6. 计算并打印最终准确率 ---\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "print(f\"Total samples evaluated: {total_samples}\")\n",
    "\n",
    "if total_samples > 0:\n",
    "    accuracies = {key: (value / total_samples) * 100 for key, value in correct_counts.items()}\n",
    "    \n",
    "    print(\"\\nAccuracy Scores:\")\n",
    "    print(f\"  Q1 (Theme) Accuracy:         {accuracies['theme']:.2f}% ({correct_counts['theme']}/{total_samples})\")\n",
    "    print(f\"  Q2 (Claim Types) Accuracy:   {accuracies['claim_types']:.2f}% ({correct_counts['claim_types']}/{total_samples})\")\n",
    "    print(f\"  Q3 (CTAs) Accuracy:          {accuracies['ctas']:.2f}% ({correct_counts['ctas']}/{total_samples})\")\n",
    "    print(f\"  Q4 (Evidence) Accuracy:      {accuracies['evidence']:.2f}% ({correct_counts['evidence']}/{total_samples})\")\n",
    "    print(\"  ----------------------------------\")\n",
    "    print(f\"  Overall Exact Match Accuracy: {accuracies['all']:.2f}% ({correct_counts['all']}/{total_samples})\")\n",
    "\n",
    "# (可选) 将详细结果保存到文件中，方便检查错误案例\n",
    "output_results_file = \"evaluation_results.json\"\n",
    "with open(output_results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nDetailed results saved to {output_results_file}\")\n",
    "\n",
    "# (可选) 打印一个错误案例进行分析\n",
    "print(\"\\n--- Example of an Incorrect Prediction ---\")\n",
    "incorrect_examples = [r for r in results if not r['is_all_correct']]\n",
    "if incorrect_examples:\n",
    "    example = incorrect_examples[0]\n",
    "    print(f\"Message ID: {example['message_id']}\")\n",
    "    print(f\"\\n[Prompt Sent to Model]\\n{example['prompt']}\")\n",
    "    print(f\"\\n[Model Prediction]\\n{example['generated_text']}\")\n",
    "    print(f\"\\n[Ground Truth]\\n{example['ground_truth_text']}\")\n",
    "else:\n",
    "    print(\"Congratulations! No incorrect predictions found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}